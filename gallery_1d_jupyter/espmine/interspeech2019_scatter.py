# -*- coding: utf-8 -*-
"""Interspeech2019_scatter

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Q4L6KuenvT8nTxwjwQscuiKkOIYXt_F

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/interspeech2019_asr.ipynb)

# [T6] Advanced methods for neural end-to-end speech processing â€“ unification, integration, and <span style="color:red">implementation</span> -

**PART4: Building End-to-End ASR System**

Speaker: [Shigeki Karita](https://github.com/ShigekiKarita)

NTT Communication Science Laboratories

15, September, 2019

# Overview of tutorial

<span style="color:gray">1. Introduction to End-to-End Speech Processing </span> Watanabe

<span style="color:gray">2. End-to-End Integration of Multiple Speech Applications </span> Hori

**coffee break**

<span style="color:gray">3. Building End-to-End TTS System (45 min.)</span> Hayashi

<span style="color:red">4. Building End-to-End ASR System (45 min.)</span> Karita

<span style="color:gray">5. Conclusion and Future Research Directions </span> Watanabe

## Abstract

How to build end-to-end ASR systems using ESPnet.


### materials

- This slide https://github.com/espnet/interspeech2019-tutorial
- API documetation https://espnet.github.io/espnet/

## Colab

GPU available! https://colab.research.google.com/github/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/interspeech2019_asr.ipynb

![colab.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/colab.png?raw=1)

## TOC

1. Overview
1. Installation
1. Use ESPnet in Bash
1. Use ESPnet in Python
1. Extend ESPnet
1. Summary

# 1. Overview

ESPnet provides **bash recipes and python library** for speech processing.

This part demonstrates

1. bash recipes in ASR
2. internals of python library

## 1.1 Python library overview

- Python 3.6+
- Neural network libraries
  - PyTorch
  - Chainer
- Automatically tested
  - Coverage 78% https://codecov.io/gh/espnet/espnet
  - Linux distributions: Ubuntu/Debian/CentOS https://circleci.com/gh/espnet/espnet
  - Sphinx documentation https://espnet.github.io/espnet/

## 1.2 Bash recipe overview

ESPnet supports total 34+ ASR tasks including

- Multilingual ASR: en, zh, ja, etc
- Noise robust and far-field ASR
- Multi-ch ASR: joint training with speech enhancement
- Speech Translation: transfer learning from ASR and MT

For more detail:
https://github.com/espnet/espnet/tree/master/egs

## 1.3 ASR Performance

On free corpora, ESPnet achieved:

- Aishell (zh): CER test: 6.7%
- Common Voice (en): WER test: 2.3%
- LibriSpeech (en): WER test-clean: 2.6%, test-other 5.7%
- TED-LIUM2 (en): WER test: 8.1%

**Pretrained models are available**

https://github.com/espnet/espnet#asr-results

# 2. Installation

ESPnet depends on Kaldi ASR toolkit and Warp-CTC.

You can install them with source compilation:
```
$ cd espnet/tools; make
```

## 2.1 Installation (Google colab)

In Google colab, we can use pre-compiled binaries for faster startup (3 min):
"""

# OS setup
!cat /etc/os-release
!apt-get install -qq bc tree sox
!nvidia-smi

# espnet setup
!git clone --depth 5 https://github.com/espnet/espnet
!pip install -q torch==1.1
!cd espnet; pip install -q -e .

# download pre-compiled warp-ctc and kaldi tools
!espnet/utils/download_from_google_drive.sh \
    "https://drive.google.com/open?id=13Y4tSygc8WtqzvAVGK_vRV9GlV7TRC0w" espnet/tools tar.gz > /dev/null
!cd espnet/tools/warp-ctc/pytorch_binding && \
    pip install -U dist/warpctc_pytorch-0.1.1-cp36-cp36m-linux_x86_64.whl

# make dummy activate
!mkdir -p espnet/tools/venv/bin && touch espnet/tools/venv/bin/activate
!echo "setup done."
!cd espnet/egs/an4/asr1; ./run.sh 
!cat espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha1.0/train.log

!cat espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha1.0/train.log

"""# 3. Use ESPnet in Bash

- `espnet/egs/*/asr1/run.sh` is an out-of-the-box recipe
- It reproduces our reported results
- It consists of **common stages similar to our TTS recipe**:
![image.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/stages.png?raw=1)

## 3.1 Kaldi-style directory structure

Always we organize each recipe as `egs/xxx/asr1/run.sh`

The most important directories:

- `conf/`: configurations for stages and machines (e.g., Local, SLURM, SGE)
- `data/`: raw [data prepared by Kaldi](https://kaldi-asr.org/doc/data_prep.html) in Stage 0 - 1
- `dump/`: python-friendly dataset (e.g., json, hdf5) in Stage 2
- `exp/`: log files and saved model parameters in Stage 3 - 5
"""

!tree -L 1 espnet/egs/librispeech/asr1

"""## 3.2 Data preparation (Stage 0 - 2)

For example, if you add `--stop-stage 2`, you can stop the script before neural network training.

These stages perform FBANK speech feature extraction, normalization, and text formatting **as same as TTS recipe**.

![image.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/stages_prep.png?raw=1)
"""

!cd espnet/egs/an4/asr1; ./run.sh  --ngpu 1 --stop-stage 2

!cat exp/train_nodev_pytorch_train_mtlalpha1.0/train.log

"""## JSON dataset

Stage 2 dumps dataset of the speech feature and trascription pairs into JSON **as same as TTS recipe**
"""

!cat espnet/egs/an4/asr1/dump/train_dev/deltafalse/data.json

"""## 3.3 NN Training (Stage 3 - 4)

You can configure NN training with `conf/train_xxx.yaml`

![image.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/stages_nn.png?raw=1)
"""

!tree espnet/egs/voxforge/asr1/conf

"""## ESPnet training features

A lot of options at `espnet/espnet/bin/asr_train.py`

- NN backends: pytorch, chainer
- Predefined models
  - RNN (LSTM, GRU, VGG, etc) with Attention + CTC (dot, location, multi-head, etc)
  - Transformer + CTC
  - RNN Transducer with Attention
- Speech enhancement (joint training):
  - beamformer
  - dereverb (WPE, DNN-WPE)
  - speech separation
- Data augmentation: SpecAugment, speed perturbation, etc
- Minibatch strategy
  - sorting, category, counting (tensor elements, sequence frames, sequences)
- Multi GPU training
- Half/mixed precision training
- Regularizations: dropout, label smoothing, weight noise, weight decay, etc

## Training config: RNN with Attention + CTC

- mtlalpha: 0.0 (attention), 0.5 (CTC/attention), 1.0 (CTC)
- complete list of common options https://espnet.github.io/espnet/apis/espnet_bin.html#Named%20Arguments
- complete list of model-specific options https://espnet.github.io/espnet/_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E.add_arguments
"""

!cat espnet/egs/an4/asr1/conf/train_mtlalpha0.5.yaml

"""## Training config: Transformer + CTC

- `--model-module <module>:<class>` to select non-default model implementation
- differences from RNN: minibatch strategy, optimizer setting, etc
- complete list of model-specific options https://espnet.github.io/espnet/_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#E2E.add_arguments
"""

!cat espnet/egs/librispeech/asr1/conf/tuning/train_pytorch_transformer_large_ngpu4.yaml

"""## Training config: RNN Transducer (with Attention)

- `--model-module <module>:<class>` to select non-default model implementation
- differences from RNN: joint network, optimizer setting, etc
- complete list of model-specific options https://espnet.github.io/espnet/_modules/espnet/nets/pytorch_backend/e2e_asr_transducer.html#E2E.add_arguments
"""

!cat espnet/egs/voxforge/asr1/conf/tuning/train_transducer.yaml

"""### Run LM and ASR NN training

RNNLM and RNN with attention and CTC in AN4
"""

# WARNING: This code takes several minutes!
!cd espnet/egs/an4/asr1; ./run.sh  --ngpu 1 --stage 3 --stop-stage 4 --train-config ./conf/train_mtlalpha0.5.yaml

"""### TIPS 1/3: change_yaml.py

Tweak training config. for example:  **--train-config $(change_yaml.py train.yaml -a lr=10.0)**

- creates the config with new name: `train_lr10.0.yaml`
- exp results are stored in dir: `exp/train_lr10.0/results`
- useful hyperparameter search by array jobs
"""

# WARNING: This code takes several minutes!
!cd espnet/egs/an4/asr1; source path.sh; \
  ./run.sh  --ngpu 1 --stage 4 --stop-stage 4 \
  --train-config $(change_yaml.py ./conf/train_mtlalpha0.5.yaml -a eunits=100 -a epochs=5)

"""### TIPS 2/3: tensorboard

To find the best config, view tensorboard
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -q tf-nightly-2.0-preview
# Load the TensorBoard notebook extension
# %load_ext tensorboard 
# %tensorboard --logdir espnet/egs/an4/asr1/tensorboard

"""![image.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/tb.png?raw=1)

### TIPS 3/3 log files

To monitor training, `exp/train*/results/` contains useful files:

- `loss.png` train/valid loss values
- `acc.png` train/valid accuracy
- `cer.png` train/valid character error rate
- `att_ws/*.png` attention plots
"""

import glob
from IPython.display import Image, display_png
expdir = "espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha0.5/results/"
for name in ["loss.png", "acc.png", "cer.png"]:
    print(name)
    display_png(Image(expdir + name, width=500))

"""### Attention plots (RNN)

A diagonal attention is good measure to check training in ASR

<table>
<tr>
    <td><img src="https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/4k0c0302.ep.1.png?raw=1"></td>
    <td><img src="https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/4k0c0302.ep.15.png?raw=1"></td>
</tr>
</table>
RNN attentions at the first (left) and last (right) epoch

### Attention plots (Transformer)

Transformer has many attentions. We recommend to monitor the last decoder layer's attention to encoded features.

![trnf-attn](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/4k0c0302.decoder.decoders.5.src_attn.ep.100.png?raw=1)

Transformer multi (=4) head attention plot in the last decoder layer at the last epoch

### Attention plots (Transformer)

Transformer has many attentions. We recommend to monitor the last decoder layer's attention to encoded features.

![trnf-attn](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/4k0c0302.decoder.decoders.0.src_attn.ep.100.png?raw=1)

Transformer multi (=4) head attention plot in the first decoder layer at the last epoch

## 3.4 Decoding and evaluation (Stage 5)

The last stage of ASR recipe

![image.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/stages_eval.png?raw=1)

### decoding YAML config

very similar to training YAML

- decoding score: 

$\mathrm{argmax}_y (1 - \lambda) \log P_{dec}(y|x) + \lambda \log P_{ctc}(y|x) + \gamma \log P_{lm}(y) + b |y|$
"""

!cat espnet/egs/an4/asr1/conf/decode_ctcweight0.5.yaml

"""### Run ASR decoding"""

# WARNING: This code takes several minutes!
!cd espnet/egs/an4/asr1; source path.sh; \
  ./run.sh --stage 5 --decode-config $(change_yaml.py conf/decode_ctcweight0.5.yaml -a batchsize=0) --train-config conf/train_mtlalpha0.5.yaml

"""## 3.5 Check evaluation results

ESPnet uses `sclite` in SPTK to evaluate ASR errors

- token error rate: exp/(train dir)/(decode dir)/result.txt
- word error rate: exp/(train dir)/(decode dir)/result.wrd.txt
"""

!ls espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha0.5/*/result.txt | xargs -n1 grep -e Avg -e SPK -m 2

!head -n 36 espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha0.5/decode_test_decode_ctcweight0.5_batchsize0_lm_word100/result.txt

!tail -n 7 espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha0.5/decode_test_decode_ctcweight0.5_batchsize0_lm_word100/result.txt

"""### ASR result as `data.json`

Find detail results in `exp/xxx/decode_yyy/data.json`
"""

!head -n30 espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha0.5/decode_test_decode_ctcweight0.5_lm_word100/data.json

"""# 4. Use ESPnet in Python

1. Load the speech features
2. Load the pretrained model
3. Recognize the speech by the model
4. Visualizations (attention, ctc)

### 4.1 Load speech features
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import json
import matplotlib.pyplot as plt
import kaldiio

# ESPnet summarizes dataset to JSON
root = "espnet/egs/an4/asr1"
with open(root + "/dump/test/deltafalse/data.json", "r") as f:
    test_json = json.load(f)["utts"]

key, info = list(test_json.items())[10]
fbank = kaldiio.load_mat(info["input"][0]["feat"])

# plot the speech feature
plt.matshow(fbank.T[::-1])
plt.title(key + ": " + info["output"][0]["text"])

"""## 4.2 Load pretrained model

pretrained model configuration (JSON) and snapshots (pickle) are available in `exp/train_xxx/results`
"""

!ls espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha0.5/results

"""## 4.2 Load pretrained model

let's load it from python
"""

import json
import torch
from espnet.nets.pytorch_backend.e2e_asr import E2E

model_dir = "espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha0.5/results"

# load model
with open(model_dir + "/model.json", "r") as f:
    idim, odim, conf = json.load(f)
model = E2E.build(idim, odim, **conf)
model.load_state_dict(torch.load(model_dir + "/model.acc.best"))
model.cpu().eval()
vocab = conf["char_list"]
print(vocab)
model

"""## 4.3 Recognize the speech by the model

You can perform joint decoding with all the models (S2S, CTC, LM, etc)  in ESPnet
"""

import re
from espnet.nets.beam_search import BeamSearch

key, info = list(test_json.items())[10]
fbank = kaldiio.load_mat(info["input"][0]["feat"])

# setup beam search
bs = BeamSearch(
    scorers=model.scorers(), weights={"decoder": 0.5, "ctc": 0.5},
    sos=model.sos, eos=model.eos,
    beam_size=2, vocab_size=len(vocab))
# GPU decoding: model.cuda(), bs.cuda()
with torch.no_grad():
    encoded = model.encode(torch.as_tensor(fbank))
    result = bs(encoded)  # get N-best results

print("groundtruth:", info["output"][0]["text"])
print("N-best list:")
for n, hyp in enumerate(result, 1):
    text = "".join(vocab[y] for y in hyp.yseq).replace("<space>", " ").replace("<eos>", "")
    scores = {k: f"{float(v):0.3f}" for k, v in hyp.scores.items()}
    print(f"{n}: {text}, score: {scores}")

"""## 4.4 Visualizations

Let's see NN internals with python.

- Attention matrix between encoder and decoder
- CTC posterior probability
"""

# Attention plot
x = torch.as_tensor(fbank).unsqueeze(0)
y = result[0].yseq.unsqueeze(0)
attn = model.calculate_all_attentions(x, [len(fbank)], y)[0]

# plot
fig, ax = plt.subplots(2, figsize=(10, 7))
ax[0].set_title("Attention")
ax[0].matshow(attn, aspect="auto")
txt = [vocab[int(i)] for i in y[0]]
ax[0].set_yticks(range(len(txt)))
ax[0].set_yticklabels(txt)
ax[1].matshow(fbank.T[::-1], aspect="auto")
fig.tight_layout()

# CTC posterior plot
with torch.no_grad():
    logp = model.ctc.log_softmax(encoded.unsqueeze(0))[0]
    prob = logp.exp_().numpy()

fig, ax = plt.subplots(2, figsize=(10, 7))
ax[0].set_title("CTC posterior")
vs = set(int(y) for y in result[0].yseq)
vs.add(0)
for n, i in enumerate(vs):
    v = vocab[i]
    ax[0].plot(prob[:, i], label=v, linestyle="-" if len(v) == 1 else "--")
ax[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))
ax[0].set_xlim(0, len(encoded)-1)
ax[1].matshow(fbank.T[::-1], aspect="auto")
fig.tight_layout()

"""# 5 Extend ESPnet for your research

1. Python library structure
2. Define your new model (Transformer)
3. Use your new model via `run.sh --model-module ...`

## 5.1 Python library structure under `espnet`

- nets: neural networks
- bin: command line tools
- transform: pre/post processing (data augumentation, frontend, etc)
- asr, lm, mt, tts: task specific procedures (train, decode, etc)
- utils: others

Let's extend `nets` with your new ASR model
"""

!tree -L 1 espnet/espnet | grep -v pycache

"""## 5.2 Define your new ASR model

Each task defines its own interfaces for new models
"""

ls espnet/espnet/nets/*interface.py

!cat espnet/espnet/nets/asr_interface.py

"""## Custom ASR model

Create new class implementing the `ASRInterface` in `./custom.py`

For training, `forward` and `add_arguments` are needed
"""

!cat ./custom.py

"""## Custom train config

Create new config `./train_custom.yaml`
"""

!cat train_custom.yaml

"""## Excute run.sh

using previous `custom.py` and `train_custom.yaml`

ESPnet provides log files and tensorboard for your custom model
"""

!cwd=$(pwd); cd espnet/egs/an4/asr1; \
  PYTHONPATH=$cwd ./run.sh --ngpu 0 --stage 4 \
  --train-config $cwd/train_custom.yaml --verbose 0

"""## Summary

ESPnet supports your research

1. Many recipes and pretrained models
1. Extensible design: you can reuse many tools in ESPnet
1. Open community-driven development 

send us your pull requests!

https://github.com/espnet/espnet

## Thanks for your attention

## Appendix Custom model training (manual for loop)

Train the model with prepared datasets in the section 3.2
"""

# create minibatch data loader
import json
import kaldiio
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
from espnet.utils.training.batchfy import make_batchset

root = "espnet/egs/an4/asr1"
with open(root + "/dump/train_nodev/deltafalse/data.json", "r") as f:
    train_json = json.load(f)["utts"]
with open(root + "/dump/train_dev/deltafalse/data.json", "r") as f:
    dev_json = json.load(f)["utts"]

batch_size = 16
trainset = make_batchset(train_json, batch_size)
devset = make_batchset(dev_json, batch_size)

def collate(minibatch):
    fbanks = []
    tokens = []
    for key, info in minibatch[0]:
        fbanks.append(torch.tensor(kaldiio.load_mat(info["input"][0]["feat"])))
        tokens.append(torch.tensor([int(s) for s in info["output"][0]["tokenid"].split()]))
    ilens = torch.tensor([x.shape[0] for x in fbanks])
    return pad_sequence(fbanks, batch_first=True), ilens, pad_sequence(tokens, batch_first=True)

train_loader = DataLoader(trainset, collate_fn=collate, shuffle=True, pin_memory=True)
dev_loader = DataLoader(devset, collate_fn=collate, pin_memory=True)

# training iteration
import numpy
from torch.nn.utils.clip_grad import clip_grad_norm_

model = ASRTransformer(83, 30)
model.cuda()
optim = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98))
n_epoch = 30
for epoch in range(n_epoch):
    acc = []
    model.train()
    for data in train_loader:
        loss = model(*[d.cuda() for d in data])
        optim.zero_grad()
        loss.backward()
        acc.append(model.acc)
        norm = clip_grad_norm_(model.parameters(), 10.0)
        optim.step()
    train_acc = numpy.mean(acc)

    acc = []
    model.eval()
    for data in dev_loader:
        model(*[d.cuda() for d in data])
        acc.append(model.acc)
    valid_acc = numpy.mean(acc)
    print(f"epoch: {epoch}, train acc: {train_acc:.3f}, dev acc: {valid_acc:.3f}")

torch.save(model.state_dict(), "model.pt")

# Appendix audio example in AN4
import os
import kaldiio
from IPython.display import Audio


try:
  d = os.getcwd()
  os.chdir(root)
  sr, wav = kaldiio.load_scp("data/test/wav.scp")[key]
finally:
  os.chdir(d)
Audio(wav, rate=sr)