{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\vct}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\mtx}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\tr}{^\\mathrm{T}}\n",
    "\\newcommand{\\reals}{\\mathbb{R}}\n",
    "\\newcommand{\\lpa}{\\left(}\n",
    "\\newcommand{\\rpa}{\\right)}\n",
    "\\newcommand{\\lsb}{\\left[}\n",
    "\\newcommand{\\rsb}{\\right]}\n",
    "\\newcommand{\\lbr}{\\left\\lbrace}\n",
    "\\newcommand{\\rbr}{\\right\\rbrace}\n",
    "\\newcommand{\\fset}[1]{\\lbr #1 \\rbr}\n",
    "\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}$\n",
    "\n",
    "# Background\n",
    "\n",
    "A neural network is composed of a number of layers, each implementing some transformation. The typical high level implementation of a single such layer (of basic type -> linear transformation plus non-linearity) can be formulated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\vct{y} = \\vct{f}(\\mtx{W} \\vct{x} + \\vct{b})\n",
    "    \\qquad\n",
    "    \\Leftrightarrow\n",
    "    \\qquad\n",
    "    y_k = f\\lpa\\sum_{d=1}^D \\lpa W_{kd} x_d \\rpa + b_k \\rpa\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mtx{W}$ and $\\vct{b}$ parameterise an affine transformation, and $f$ is a function applied elementwise to the result of the affine transformation. For example a common choice for $f$ is the rectifier linear unit: \n",
    "\n",
    "\\begin{equation}\n",
    "  f(u) = max(0,u)\n",
    "\\end{equation}\n",
    "\n",
    "One can stack an arbitrary number of such transformations, followed by an output layer. This means we can combine implementations of `AffineLayer` class with any non-linear function applied to the outputs just by implementing a layer object for the relevant non-linearity and then stacking the two layers together. \n",
    "\n",
    "To give a concrete example, in the `mlp.layers` module there is a definition for a `ReluLayer` equivalent to the following (documentation strings have been removed here for brevity)\n",
    "\n",
    "```python\n",
    "class ReluLayer(Layer):\n",
    "    \n",
    "    def fprop(self, inputs):\n",
    "        return np.maximum(inputs, 0.)\n",
    "\n",
    "    def bprop(self, inputs, outputs, grads_wrt_outputs):\n",
    "        return (outputs > 0) * grads_wrt_outputs\n",
    "```\n",
    "\n",
    "As you can see this `ReluLayer` class has a very lightweight definition, defining just two key methods:\n",
    "\n",
    "  * `fprop` which takes a batch of activations at the input to the layer and forward propagates them to produce activations at the outputs (directly equivalently to the `fprop` method of `AffineLayer`),\n",
    "  * `brop` which takes a batch of gradients with respect to the outputs of the layer and back-propagates them to calculate gradients with respect to the inputs of the layer (explained in more detail below).\n",
    "  \n",
    "This `ReluLayer` class only implements the rectified linear transformation and so does not have any parameters. Therefore unlike `AffineLayer` it is derived directly from the base `Layer` class rather than `LayerWithParameters` and does not need to implement `grads_wrt_params` or `params` methods. \n",
    "\n",
    "To create a model consisting of an affine transformation followed by applying an elementwise rectified linear transformation you can first create a list of the two layer objects (in the order they are applied from inputs to outputs) and then use this to instantiate a new `MultipleLayerModel` object:\n",
    "\n",
    "```python\n",
    "from mlp.layers import AffineLayer, ReluLayer\n",
    "from mlp.models import MultipleLayerModel\n",
    "\n",
    "layers = [AffineLayer(input_dim, output_dim), ReluLayer()]\n",
    "model = MultipleLayerModel(layers)\n",
    "```\n",
    "\n",
    "You can also stack an arbitrarily long sequence of layers together to produce deeper models. For instance the following would define a model consisting of three pairs of affine and rectified linear transformations.\n",
    "\n",
    "```python\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim), ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim), ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim), ReluLayer(),\n",
    "])\n",
    "```\n",
    "\n",
    "## Back-propagation of gradients\n",
    " The `bprop` method takes gradients of an error function with respect to the *outputs* of a layer and uses these gradients to calculate gradients of the error function with respect to the *inputs* of a layer. As the inputs to a non-input layer in a multiple-layer model consist of the outputs of the previous layer, this means we can calculate the gradients of the error function with respect to the outputs of every layer in the model by iteratively propagating the gradients backwards through the layers of the model (i.e. from the last to first layer).\n",
    "\n",
    "For a layer with parameters, the gradients with respect to the layer outputs are required to calculate gradients with respect to the layer parameters. Therefore by combining back-propagation of gradients through the model with computing the gradients with respect to parameters in the relevant layers one can calculate gradients of the error function with respect to all of the parameters of a multiple-layer model in a very efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Exploring activation functions\n",
    "\n",
    "Among other minor things, you are expected to implement the Exponential Linear Unit (ELU), Scaled Exponential Linear Unit (SELU), Gaussian Error Linear Unit (GELU), and the Inverse Square Root Linear Unit (ISRLU) layers (all in the mlp.layers module). Then run several basic experiments on the MNIST benchmark.\n",
    "\n",
    "Below you are given basic blocks (how to build the model as a whole, define data providers, add optimiser, etc.) to get the whole training pipeline ready.\n",
    "\n",
    "Your tasks here would be:\n",
    "\n",
    "1-1) Add the missing layers mentioned above. Show how you arrived at the derivative of the GELU function (2-3 lines of the equation will suffice).\n",
    "\n",
    "1-2) Establish a baseline network for MNIST using stochastic gradient descent (SGD) and the ReLU activation function. Explore the number of hidden units per layer, number of hidden layers, an appropriate learning rate and initialisation scale range and train for 100 epochs. Justify your choices.\n",
    "\n",
    "1-3) Modify the baseline network to one that uses each of the above activation functions. Your main aim is to compare the generalization performance of each of these activation functions as well as discover the best possible network configuration when the only available options to choose from are the activation functions and their hyperparameters. Explain your findings.\n",
    "\n",
    "1-4) Some of these activations have been shown to improve training when paired with special initialisation strategies. Investigate which of them do, implement them and rerun the corresponding optimal networks found in the previous excercise. Explain your findings. \n",
    "\n",
    "1-5) Select the optimal network from the exploration above. List possible improvements to the network to achieve better a) accuracy, b) training speed and c) inference speed. Implement one of each a) and b) and explain your observations (ie. did your hypothesis hold or not and why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from mlp.layers import AffineLayer, ReluLayer\n",
    "from mlp.errors import CrossEntropyError, CrossEntropySoftmaxError\n",
    "from mlp.models import SingleLayerModel, MultipleLayerModel\n",
    "from mlp.initialisers import UniformInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule\n",
    "from mlp.data_providers import MNISTDataProvider\n",
    "from mlp.optimisers import Optimiser\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 6102016 \n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', rng=rng)\n",
    "input_dim, output_dim = 784, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimise replication of code and allow you to run experiments more quickly a helper function is provided below which trains a model and plots the evolution of the error and classification accuracy of the model (on both training and validation sets) over training.\n",
    "\n",
    "Running the cell below will create a model consisting of an affine layer followed by a softmax transformation and trains it on the MNIST data set by minimising the multi-class cross entropy error function using a basic gradient descent learning rule. By using the helper function defined below, at the end of the training curves of the evolution of the error function and also classification accuracy of the model over the training epochs will be plotted.\n",
    "\n",
    "You should try running the code for various settings of the training hyperparameters defined at the beginning of the cell to get a feel for how these affect how training proceeds. You may wish to create multiple copies of the cell below to allow you to keep track of and compare the results across different hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval):\n",
    "\n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below we describe how to build multiple-layer model architectures for the MNIST classification task. \n",
    "\n",
    "```python\n",
    "layers = [\n",
    "    AffineLayer(input_dim, 100),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(100, output_dim),\n",
    "    ReluLayer()\n",
    "]\n",
    "```\n",
    "\n",
    "An example cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable logging by setting handler to dummy object\n",
    "logger.handlers = [logging.NullHandler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training run hyperparameters\n",
    "batch_size = 100  # number of data points in a batch\n",
    "num_epochs = 100  # number of training epochs to perform\n",
    "stats_interval = 5  # epoch interval between recording and printing stats\n",
    "learning_rate = 0.2  # learning rate for gradient descent\n",
    "\n",
    "init_scales = [0.1]  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for init_scale in init_scales:\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(f'learning_rate={learning_rate} init_scale={init_scale}')\n",
    "    print('-' * 80)\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    # Alter data-provider batch size\n",
    "    train_data.batch_size = batch_size \n",
    "    valid_data.batch_size = batch_size\n",
    "\n",
    "    # Create a parameter initialiser which will sample random uniform values\n",
    "    # from [-init_scale, init_scale]\n",
    "    param_init = UniformInit(-init_scale, init_scale, rng=rng)\n",
    "\n",
    "    # Create a model with two affine layers\n",
    "    hidden_dim = 100\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, param_init, param_init),\n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, param_init, param_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'    final error(train) = {stats[-1, keys[\"error(train)\"]]:.2e}')\n",
    "    print(f'    final error(valid) = {stats[-1, keys[\"error(valid)\"]]:.2e}')\n",
    "    print(f'    final acc(train)   = {stats[-1, keys[\"acc(train)\"]]:.2e}')\n",
    "    print(f'    final acc(valid)   = {stats[-1, keys[\"acc(valid)\"]]:.2e}')\n",
    "    print(f'    run time per epoch = {run_time * 1. / num_epochs:.2f}')\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
