{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Cross validation is a method of splitting all your data into to parts: training and validation.  The training data is used to build the machine learning model, whereas the validation data is used to validate that the model is doing what is expected.  This increases our ability to find and determine the underlying errors in a model.  In a supervised environment, without an initial set to train on the algorithm would be useless.\n",
    "\n",
    "Swapping training with validation helps increase the number of tests.  This id done by splitting the data into two; the first time set 1 is used to train and set two to validate.  The next step would be to swap the roles of set one and two for the subsequent test. Depending on how much data is available, the data is plsit into smaller sets and cross-validated. Given sufficient data, an indefinite amount of cross-validation sets can be setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training Algorithms\n",
    "\n",
    "Common algorithms for training neural networks include the following.\n",
    "\n",
    "1. Back propagation\n",
    "2. Quick propagation\n",
    "3. RProp\n",
    "\n",
    "All of these algorithms find optimal weights for each neuron.  They do so through iterations known as epochs.  For each epoch, a training algorithm goes through the entire Neural Network and compares it against what is expected.  At this point, it learns from past miscalculations.\n",
    "\n",
    "These algorithms have one thing in common: they are trying to find the optimal solution in a convex error surface.  You can think of convex error surface as a bowl with a minimum value in it.  These algorithms known as Gradient Descent minimize the error by using local information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The delta rule\n",
    "This rule simplifies the aim of Gradient descent through iteration than heavy algebraic equations.  Instead of calculating the derivative error function with respect to the weight, we calculate the change in weight for each neuron's weights.  This delta rule is as follows:\n",
    "$$\\delta w_{ji}=\\alpha(t_j-\\phi(h_j))\\phi'(h_j)x_i$$\n",
    "\n",
    "This states that the change in weight for the neuron j's weight number i is:\n",
    "> alpha \\* (expected - calculated) \\* derivative_of_calculated \\*  input_at_i\n",
    "\n",
    "alpha is the learning rate and is a small constant.  The initial idea, through is what powers the idea behind back propagation algorithm, or the general case of the delta rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Back Propagation\n",
    "Back Propagation is the simplest of the three algorithms that determine the weight of a neuron.  The error is defined as $(expected * actual)^2$ where expeceted is the expected output and actual is the calculated number from the neurons. We want to find where the derivative of this value is 0, which is the minimum:\n",
    "$$\\Delta w(t)=-\\alpha(t-y)\\phi'x_i+\\epsilon\\Delta w(t-1)$$\n",
    "\n",
    "$\\epsilon$ is the momentum factor and propels previous weight changes into our current weight change, whereas $\\alpha$ is the learning rate.\n",
    "\n",
    "Back propagation has the disadavantage of taking many epochs to calculate.  Up until 1988, researchers were struggling to train simple Neural Networks.  Their research on how to improve this led to a new algorithm called QuickProp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuickProp\n",
    "Scott Fahlman introduced the QuickProp algorithm after he studied how to improve Back Propagation.  He asserted that Back Propagation took  too long to converge to a solution.  He proposed that we instead take the biggest steps without overstepping the solution.\n",
    "\n",
    "Fahlman determined that there are two ways to improve Back Propagations: making the momentum and learning rate dynamic, and making use of a second derivative of the error with respect to each weight.  In the first case, the algorithm can be optimized for each weight, and in the second case, Newton's method can be utilized to approximate functions.\n",
    "\n",
    "With QuickProp, the main difference from Back Propagation is that you keep a copy of the error derivative computed during the previous epoch, along with the difference between the current an previous values of this weight.\n",
    "\n",
    "To calculate a weight change at time t, the following function can be employed:\n",
    "\n",
    "$$\\Delta w(t)=\\frac{S(t)}{S(t-1)-S(t)}\\Delta w(t-1)$$\n",
    "\n",
    "This carries the risk of changing the weights too much, so there is a new parameter for maximum growth.  No weight is allowed to be greater in magnitude than the maximum growth rate multiplied by the previous step for that weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RProp\n",
    "\n",
    "RProp is the most used algorithm because it converges fast.  It was introduced by Martin Riedmiller in the 1990s and has had improvements since then.  It converges on a solution quickly due to its insight that the algorithm can update the weights many times through an epoch.  Instead of calculating weight changes based on a formula, it uses only the sign for change as well as an increase factor and decrease factor.\n",
    "\n",
    "To see what this algorithm looks like in code, defaults need to be defined. FAAN library has such defaults defined and based on the delta rule the following can be code utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NoMethodError",
     "evalue": "undefined method `*' for nil:NilClass",
     "output_type": "error",
     "traceback": [
      "\u001b[31mNoMethodError\u001b[0m: undefined method `*' for nil:NilClass",
      "\u001b[37m(pry):207:in `block (2 levels) in <main>'\u001b[0m",
      "\u001b[37m(pry):205:in `each'\u001b[0m",
      "\u001b[37m(pry):205:in `each_with_index'\u001b[0m",
      "\u001b[37m(pry):205:in `block in <main>'\u001b[0m",
      "\u001b[37m(pry):204:in `upto'\u001b[0m",
      "\u001b[37m(pry):204:in `<main>'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/pry-0.10.1/lib/pry/pry_instance.rb:355:in `eval'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/pry-0.10.1/lib/pry/pry_instance.rb:355:in `evaluate_ruby'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/pry-0.10.1/lib/pry/pry_instance.rb:323:in `handle_line'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/pry-0.10.1/lib/pry/pry_instance.rb:243:in `block (2 levels) in eval'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/pry-0.10.1/lib/pry/pry_instance.rb:242:in `catch'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/pry-0.10.1/lib/pry/pry_instance.rb:242:in `block in eval'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/pry-0.10.1/lib/pry/pry_instance.rb:241:in `catch'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/pry-0.10.1/lib/pry/pry_instance.rb:241:in `eval'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/iruby-0.2.7/lib/iruby/backend.rb:65:in `eval'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/iruby-0.2.7/lib/iruby/backend.rb:12:in `eval'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/iruby-0.2.7/lib/iruby/kernel.rb:87:in `execute_request'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/iruby-0.2.7/lib/iruby/kernel.rb:47:in `dispatch'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/iruby-0.2.7/lib/iruby/kernel.rb:37:in `run'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/iruby-0.2.7/lib/iruby/command.rb:70:in `run_kernel'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/iruby-0.2.7/lib/iruby/command.rb:34:in `run'\u001b[0m",
      "\u001b[37m/var/lib/gems/2.1.0/gems/iruby-0.2.7/bin/iruby:5:in `<top (required)>'\u001b[0m",
      "\u001b[37m/usr/local/bin/iruby:23:in `load'\u001b[0m",
      "\u001b[37m/usr/local/bin/iruby:23:in `<main>'\u001b[0m"
     ]
    }
   ],
   "source": [
    "neurons=3\n",
    "inputs =4\n",
    "\n",
    "delta_zero=0.1\n",
    "increase_factor=1.2\n",
    "decrease_factor=0.5\n",
    "delta_max = 50.0\n",
    "delta_min=0.0\n",
    "max_epoch = 100\n",
    "deltas = Array.new(inputs){Array.new(neurons){delta_zero}}\n",
    "last_gradient = Array.new(inputs){Array.new(neurons){0.0}}\n",
    "current_gradient = Array.new(inputs){Array.new(neurons){0.0}}\n",
    "\n",
    "sign= -> (x){\n",
    "  if x > 0\n",
    "    1\n",
    "  elsif x < 0\n",
    "    -1\n",
    "  else\n",
    "    0\n",
    "  end\n",
    "}\n",
    "\n",
    "weights = inputs.times.map{|i| rand(-1.0..1.0)}\n",
    "\n",
    "1.upto(max_epoch)do |j|\n",
    "  weights.each_with_index do |i,weight|\n",
    "    # Current gradient is derived from the change of each value at each layer\n",
    "    gradient_momentum=last_gradient[i][j] * current_gradient[i][j]\n",
    "    \n",
    "    if gradient_momentum > 0\n",
    "      deltas[i][j]=[deltas[i][j] * increase_factor, delta_max].min\n",
    "      change_weight=-sign.(current_gradient[i][j]) * delta[i][j]\n",
    "      weights[i]=weight+change_weight\n",
    "      last_gradient[i][j]=current_gradient[i]\n",
    "    elsif gradient_momentum < 0\n",
    "      deltas[i][j]=[deltas[i][j] * decrease_factor,delta_min].max\n",
    "      last_gradient[i][j]=0\n",
    "    else\n",
    "      change_weight=-sign.(current_gradient[i][j]) * deltas[i][j]\n",
    "      weights[i]=weights[i]+change_weight\n",
    "      last_gradient[i][j]=current_gradient[i][j]\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code files\n",
    "\n",
    "1. getdata??\n",
    "2. test/lib/language_spec.rb\n",
    "3. lib/language.rb\n",
    "4. lib/tokenizer.rb\n",
    "5. test/cross_validation_spec.rb\n",
    "6. lib/network.rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.1.5",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": "rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.1.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
