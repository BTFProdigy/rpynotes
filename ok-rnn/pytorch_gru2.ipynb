{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.contrib import layers\n",
    "#from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import my_txtutils as txt\n",
    "import torch\n",
    "import json\n",
    "#tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model parameters\n",
    "#\n",
    "# Usage:\n",
    "#   Training only:\n",
    "#         Leave all the parameters as they are\n",
    "#         Disable validation to run a bit faster (set validation=False below)\n",
    "#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n",
    "#   Training and experimentation (default):\n",
    "#         Keep validation enabled\n",
    "#         You can now play with the parameters anf follow the effects in Tensorboard\n",
    "#         A good choice of parameters ensures that the testing and validation curves stay close\n",
    "#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n",
    "#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n",
    "#\n",
    "nb_epoch=60\n",
    "SEQLEN = 30\n",
    "BATCHSIZE = 200\n",
    "VALI_SEQLEN = SEQLEN\n",
    "ALPHASIZE = txt.ALPHASIZE\n",
    "INTERNALSIZE = 512\n",
    "NLAYERS = 3\n",
    "learning_rate = 0.001  # fixed learning rate\n",
    "dropout_pkeep = 0.8    # some dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# size of the alphabet that we work with\n",
    "ALPHASIZE = 98\n",
    "\n",
    "\n",
    "# Specification of the supported alphabet (subset of ASCII-7)\n",
    "# 10 line feed LF\n",
    "# 32-64 numbers and punctuation\n",
    "# 65-90 upper-case letters\n",
    "# 91-97 more punctuation\n",
    "# 97-122 lower-case letters\n",
    "# 123-126 more punctuation\n",
    "def convert_from_alphabet(a):\n",
    "    \"\"\"Encode a character\n",
    "    :param a: one character\n",
    "    :return: the encoded value\n",
    "    \"\"\"\n",
    "    if a == 9:\n",
    "        return 1\n",
    "    if a == 10:\n",
    "        return 127 - 30  # LF\n",
    "    elif 32 <= a <= 126:\n",
    "        return a - 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "# encoded values:\n",
    "# unknown = 0\n",
    "# tab = 1\n",
    "# space = 2\n",
    "# all chars from 32 to 126 = c-30\n",
    "# LF mapped to 127-30\n",
    "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode a code point\n",
    "    :param c: code point\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return: decoded character\n",
    "    \"\"\"\n",
    "    if c == 1:\n",
    "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
    "    if c == 127 - 30:\n",
    "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
    "    if 32 <= c + 30 <= 126:\n",
    "        return c + 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "def encode_text(s):\n",
    "    \"\"\"Encode a string.\n",
    "    :param s: a text string\n",
    "    :return: encoded list of code points\n",
    "    \"\"\"\n",
    "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
    "\n",
    "\n",
    "def decode_to_text(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode an encoded string.\n",
    "    :param c: encoded list of code points\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
    "\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(probabilities)\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
    "\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "\n",
    "\n",
    "def find_book(index, bookranges):\n",
    "    return next(\n",
    "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def find_book_index(index, bookranges):\n",
    "    return next(\n",
    "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
    "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
    "    print()\n",
    "    # epoch_size in number of batches\n",
    "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
    "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
    "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "    for k in range(batch_size):\n",
    "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
    "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
    "        bookname = find_book(index_in_epoch, bookranges)\n",
    "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
    "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
    "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
    "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
    "        print(print_string.format(decx, decy, loss_string))\n",
    "        index += sequence_len\n",
    "    # box formatting characters:\n",
    "    # │ \\u2502\n",
    "    # ─ \\u2500\n",
    "    # └ \\u2514\n",
    "    # ┘ \\u2518\n",
    "    # ┴ \\u2534\n",
    "    # ┌ \\u250C\n",
    "    # ┐ \\u2510\n",
    "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
    "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
    "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
    "    print(footer)\n",
    "    # print statistics\n",
    "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
    "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
    "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
    "    print()\n",
    "    print(\"TRAINING STATS: {}\".format(stats))\n",
    "\n",
    "\n",
    "class Progress:\n",
    "    \"\"\"Text mode progress bar.\n",
    "    Usage:\n",
    "            p = Progress(30)\n",
    "            p.step()\n",
    "            p.step()\n",
    "            p.step(start=True) # to restart form 0%\n",
    "    The progress bar displays a new header at each restart.\"\"\"\n",
    "    def __init__(self, maxi, size=100, msg=\"\"):\n",
    "        \"\"\"\n",
    "        :param maxi: the number of steps required to reach 100%\n",
    "        :param size: the number of characters taken on the screen by the progress bar\n",
    "        :param msg: the message displayed in the header of the progress bat\n",
    "        \"\"\"\n",
    "        self.maxi = maxi\n",
    "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
    "        self.header_printed = False\n",
    "        self.msg = msg\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, reset=False):\n",
    "        if reset:\n",
    "            self.__init__(self.maxi, self.size, self.msg)\n",
    "        if not self.header_printed:\n",
    "            self.__print_header()\n",
    "        next(self.p)\n",
    "\n",
    "    def __print_header(self):\n",
    "        print()\n",
    "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
    "        print(format_string.format(self.msg))\n",
    "        self.header_printed = True\n",
    "\n",
    "    def __start_progress(self, maxi):\n",
    "        def print_progress():\n",
    "            # Bresenham's algorithm. Yields the number of dots printed.\n",
    "            # This will always print 100 dots in max invocations.\n",
    "            dx = maxi\n",
    "            dy = self.size\n",
    "            d = dy - dx\n",
    "            for x in range(maxi):\n",
    "                k = 0\n",
    "                while d >= 0:\n",
    "                    print('=', end=\"\", flush=True)\n",
    "                    k += 1\n",
    "                    d -= dx\n",
    "                d += dy\n",
    "                yield k\n",
    "\n",
    "        return print_progress\n",
    "\n",
    "\n",
    "def read_data_files(directory, validation=True):\n",
    "    \"\"\"Read data files according to the specified glob pattern\n",
    "    Optionnaly set aside the last file as validation data.\n",
    "    No validation data is returned if there are 5 files or less.\n",
    "    :param directory: for example \"data/*.txt\"\n",
    "    :param validation: if True (default), sets the last file aside as validation data\n",
    "    :return: training data, validation data, list of loaded file names with ranges\n",
    "     If validation is\n",
    "    \"\"\"\n",
    "    codetext = []\n",
    "    bookranges = []\n",
    "    shakelist = glob.glob(directory, recursive=True)\n",
    "    for shakefile in shakelist:\n",
    "        shaketext = open(shakefile, \"r\")\n",
    "        print(\"Loading file \" + shakefile)\n",
    "        start = len(codetext)\n",
    "        codetext.extend(encode_text(shaketext.read()))\n",
    "        end = len(codetext)\n",
    "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
    "        shaketext.close()\n",
    "\n",
    "    if len(bookranges) == 0:\n",
    "        sys.exit(\"No training data has been found. Aborting.\")\n",
    "\n",
    "    # For validation, use roughly 90K of text,\n",
    "    # but no more than 10% of the entire text\n",
    "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "    # 10% of the text is how many files ?\n",
    "    total_len = len(codetext)\n",
    "    validation_len = 0\n",
    "    nb_books1 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books1 += 1\n",
    "        if validation_len > total_len // 10:\n",
    "            break\n",
    "\n",
    "    # 90K of text is how many books ?\n",
    "    validation_len = 0\n",
    "    nb_books2 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books2 += 1\n",
    "        if validation_len > 90*1024:\n",
    "            break\n",
    "\n",
    "    # 20% of the books is how many books ?\n",
    "    nb_books3 = len(bookranges) // 5\n",
    "\n",
    "    # pick the smallest\n",
    "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
    "\n",
    "    if nb_books == 0 or not validation:\n",
    "        cutoff = len(codetext)\n",
    "    else:\n",
    "        cutoff = bookranges[-nb_books][\"start\"]\n",
    "    valitext = codetext[cutoff:]\n",
    "    codetext = codetext[:cutoff]\n",
    "    return codetext, valitext, bookranges\n",
    "\n",
    "\n",
    "def print_data_stats(datalen, valilen, epoch_size):\n",
    "    datalen_mb = datalen/1024.0/1024.0\n",
    "    valilen_kb = valilen/1024.0\n",
    "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
    "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
    "\n",
    "\n",
    "def print_validation_header(validation_start, bookranges):\n",
    "    bookindex = find_book_index(validation_start, bookranges)\n",
    "    books = ''\n",
    "    for i in range(bookindex, len(bookranges)):\n",
    "        books += bookranges[i][\"name\"]\n",
    "        if i < len(bookranges)-1:\n",
    "            books += \", \"\n",
    "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
    "\n",
    "\n",
    "def print_validation_stats(loss, accuracy):\n",
    "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
    "                                                                                                           accuracy))\n",
    "\n",
    "\n",
    "def print_text_generation_header():\n",
    "    print()\n",
    "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
    "\n",
    "\n",
    "def print_text_generation_footer():\n",
    "    print()\n",
    "    print(\"└{:─^111}┘\".format('End of generation'))\n",
    "\n",
    "\n",
    "def frequency_limiter(n, multiple=1, modulo=0):\n",
    "    def limit(i):\n",
    "        return i % (multiple * n) == modulo*multiple\n",
    "    return limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file txts\\acts_new.txt\n",
      "Loading file txts\\gal_eph_new.txt\n",
      "Loading file txts\\heb_new.txt\n",
      "Loading file txts\\jam_jud_new.txt\n",
      "Loading file txts\\john_new.txt\n",
      "Loading file txts\\jud_rev_new.txt\n",
      "Loading file txts\\luke_8_john_new.txt\n",
      "Loading file txts\\mark01_new.txt\n",
      "Loading file txts\\matt02_new.txt\n",
      "Loading file txts\\matt_new.txt\n",
      "Loading file txts\\phil_col_new.txt\n",
      "Loading file txts\\thes_tim_new.txt\n",
      "Loading file txts\\tit_phl_new.txt\n"
     ]
    }
   ],
   "source": [
    "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
    "shakedir = \"txts/*.txt\"\n",
    "#shakedir = \"../tensorflow/**/*.py\"\n",
    "codetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text size is 2.92MB with 192.15KB set aside for validation. There will be 509 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "# display some stats on the data\n",
    "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
    "txt.print_data_stats(len(codetext), len(valitext), epoch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.GRUCell(input_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input,hidden)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(BATCHSIZE, self.hidden_size)\n",
    "\n",
    "rnn = RNN(ALPHASIZE, INTERNALSIZE, ALPHASIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# loss fn\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#from ok_seq2seq import EncoderRNN \\\n",
    "#                        ,DecoderRNN \\\n",
    "#                        ,AttnDecoderRNN \\\n",
    "#                        ,evaluateRandomly \\\n",
    "#                        ,teacher_forcing_ratio \n",
    "\n",
    "#torch.nn.GRUCell(input_size: int, hidden_size: int, bias: bool = True)\n",
    "# Parameters\n",
    "# input_size – The number of expected features in the input x\n",
    "\n",
    "# hidden_size – The number of features in the hidden state h\n",
    "\n",
    "# bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "\n",
    "# Inputs: input, hidden\n",
    "# input of shape (batch, input_size): tensor containing input features\n",
    "\n",
    "# hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\n",
    "\n",
    "# Outputs: h’\n",
    "# h’ of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training fn\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    lint = []\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "        lint.append(output)\n",
    "    input = torch.stack(lint).transpose(0,1).transpose(1,2)\n",
    "#     print(f'is={input.size()}, cs={category_tensor.size()}')\n",
    "#     print(f'is[1:]={input.size()[1:]}, cs[1:]={category_tensor.size()[1:]}')\n",
    "#     print(f'is[2:]={input.size()[2:]}, cs[2:]={category_tensor.size()[2:]}')\n",
    "    loss = criterion(input, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return torch.stack(lint).transpose(0,1), loss.item()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# for display: init the progress bar\n",
    "DISPLAY_FREQ = 50\n",
    "_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n",
    "progress = txt.Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n",
    "\n",
    "# init\n",
    "istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def one_hot(chcode):\n",
    "    tensor = torch.zeros(1, ALPHASIZE)\n",
    "    tensor[0][chcode] = 1\n",
    "    return tensor\n",
    "\n",
    "def mb2t(rows):\n",
    "    rows=rows.transpose()\n",
    "    tensor = torch.zeros(rows.shape[0], rows.shape[1], ALPHASIZE)\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, letter_code in enumerate(row):\n",
    "            tensor[i][j][letter_code] = 1\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def lin2txt(lt):\n",
    "    return ''.join([chr(txt.convert_to_alphabet(c))  if c != 0 else '' for c in lt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programs\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 of  10 (0m 1s) 4.5646  okunwengiapu mieye ma  diepak / B<r11D,D,n5#orBrr#rDBsrr,rr1,r ✗ okunwengiapu mieye ma  diepaku\n",
      "epoch 0 of  10 (0m 1s) 4.5646 ara mi nwose omine mgba oriari / B<r11D,D,n5#orBrr#rDBsrr,rr1,r ✗ ra mi nwose omine mgba oriari \n",
      "epoch 0 of  10 (0m 21s) 4.5273 mame bereniapuma dumo bie, pik / n,or rrDre reoo,r, o, r rD e r ✗ ame bereniapuma dumo bie, piki\n",
      "epoch 0 of  10 (0m 21s) 4.5273 ma bu vinpikibia bara.  pita b / n,or rrDre reoo,r, o, r rD e r ✗ a bu vinpikibia bara.  pita be\n",
      "epoch 0 of  10 (0m 40s) 4.4916 a na miemieaye ma na, i da be  /   e  o ro r or    e            ✗  na miemieaye ma na, i da be k\n",
      "epoch 0 of  10 (0m 40s) 4.4916 e apu ma dikisima dupari mi vi /   e  o ro r or    e            ✗  apu ma dikisima dupari mi vin\n",
      "epoch 0 of  10 (1m 0s) 4.4543 mi duko na fiafia teme be devi /                                ✗ i duko na fiafia teme be devid\n",
      "epoch 0 of  10 (1m 0s) 4.4543 ni ma tamuo be nwo tokoni sime /                                ✗ i ma tamuo be nwo tokoni simem\n",
      "epoch 0 of  10 (1m 21s) 4.4146 oyi gbori-okunwengiapu finji m /                                ✗ yi gbori-okunwengiapu finji ma\n",
      "epoch 0 of  10 (1m 21s) 4.4146  ini nyanaye mamgba now dia gb /                                ✗ ini nyanaye mamgba now dia gbo\n",
      "epoch 0 of  10 (1m 42s) 4.3756 isiapu ma bereniapu ma se kpot /                                ✗ siapu ma bereniapu ma se kpoti\n",
      "epoch 0 of  10 (1m 42s) 4.3756 ow naa kirikiri o biki koro se /                                ✗ w naa kirikiri o biki koro se \n",
      "epoch 0 of  10 (2m 4s) 4.3310 me. tamuno be, o berenioki bar /                                ✗ e. tamuno be, o berenioki bara\n",
      "epoch 0 of  10 (2m 4s) 4.3310 u ma na siteme bie sime apu ma /                                ✗  ma na siteme bie sime apu ma \n",
      "epoch 0 of  10 (2m 26s) 4.2895 a warabe, wa piki o bara fiafi /                                ✗  warabe, wa piki o bara fiafia\n",
      "epoch 0 of  10 (2m 26s) 4.2895  oki vinpiki bome, ini kuro ye /                                ✗ oki vinpiki bome, ini kuro ye \n",
      "epoch 0 of  10 (2m 48s) 4.2348 ekeme.  bereniapu ma iniaogbok /                                ✗ keme.  bereniapu ma iniaogboku\n",
      "epoch 0 of  10 (2m 48s) 4.2348 , se oria ogbo mi piki fi boro /                                ✗  se oria ogbo mi piki fi borom\n",
      "epoch 0 of  10 (3m 8s) 4.1814 achua o deki chiemame. ani kir /                                ✗ chua o deki chiemame. ani kiri\n",
      "epoch 0 of  10 (3m 8s) 4.1814 rusin se mun igbiki bere nwo j /                                ✗ usin se mun igbiki bere nwo ji\n",
      "epoch 0 of  10 (3m 32s) 4.1208 mun dumo bie chu-chu bo be now /                                ✗ un dumo bie chu-chu bo be now \n",
      "epoch 0 of  10 (3m 32s) 4.1208 loko dieapu ma na nwo chukurum /                                ✗ oko dieapu ma na nwo chukuruma\n",
      "epoch 1 of  10 (3m 56s) 4.0555  na fi oboku bu yedia okime. o /                                ✗ na fi oboku bu yedia okime. o \n",
      "epoch 1 of  10 (3m 56s) 4.0555 a apu ma jin se bie ini angabi /                                ✗  apu ma jin se bie ini angabia\n",
      "epoch 1 of  10 (4m 17s) 3.9769  gbeinme, piki izrel bie sime  /                                ✗ gbeinme, piki izrel bie sime a\n",
      "epoch 1 of  10 (4m 17s) 3.9769 apu ma bara fe idupu mi bie di /                                ✗ pu ma bara fe idupu mi bie dip\n",
      "epoch 1 of  10 (4m 38s) 3.8886 karaye sotein be, o poki mi na /                                ✗ araye sotein be, o poki mi na \n",
      "epoch 1 of  10 (4m 38s) 3.8886 midia ama mi bie angame. aniny /                                ✗ idia ama mi bie angame. aninyo\n",
      "epoch 1 of  10 (5m 1s) 3.8081  pakuma. i bara isun pakuma ob /                                ✗ pakuma. i bara isun pakuma obi\n",
      "epoch 1 of  10 (5m 1s) 3.8081 piribia erechi.  okuma minea b /                                ✗ iribia erechi.  okuma minea bi\n",
      "epoch 1 of  10 (5m 23s) 3.6920 werima i nyanaye mi ma i nwo d /                                ✗ erima i nyanaye mi ma i nwo de\n",
      "epoch 1 of  10 (5m 23s) 3.6920 mi ani iya amanyanabo ngada-e  /                                ✗ i ani iya amanyanabo ngada-e n\n",
      "epoch 1 of  10 (5m 45s) 3.5868 pura pakuma ini se monoaye na  /                                ✗ ura pakuma ini se monoaye na e\n",
      "epoch 1 of  10 (5m 45s) 3.5868 i siki. nyanabo jizos i teme /                                ✗  siki. nyanabo jizos i teme \n",
      "epoch 1 of  10 (6m 6s) 3.4830 ow naa siki ani inia bubuamame /                                ✗ w naa siki ani inia bubuamame,\n",
      "epoch 1 of  10 (6m 6s) 3.4830 me, owuapu na eremeni na. saim /                                ✗ e, owuapu na eremeni na. saimo\n",
      "epoch 1 of  10 (6m 30s) 3.3794 wose berechiri mi beme,  juo  /                                ✗ ose berechiri mi beme,  juo i\n",
      "epoch 1 of  10 (6m 30s) 3.3794  ye ma be gboriye soni paka ip /                                ✗ ye ma be gboriye soni paka ipi\n",
      "epoch 1 of  10 (6m 49s) 3.3109  ogboku. grik okwein kon-ari j /                                ✗ ogboku. grik okwein kon-ari ju\n",
      "epoch 1 of  10 (6m 49s) 3.3109 ri kulumasam.  firitibinibo be /                                ✗ i kulumasam.  firitibinibo be \n",
      "epoch 1 of  10 (7m 9s) 3.2383 u ma na stivin be na berejinea /                                ✗  ma na stivin be na berejineam\n",
      "epoch 1 of  10 (7m 9s) 3.2383 sotoru bo enekubu bia o kelema /                                ✗ otoru bo enekubu bia o kelemam\n",
      "epoch 2 of  10 (7m 28s) 3.1891 se o pirike, okuma tamuno be o /                                ✗ e o pirike, okuma tamuno be o \n",
      "epoch 2 of  10 (7m 28s) 3.1891 ebia. ani ananayas be mun wari /                                ✗ bia. ani ananayas be mun wari \n",
      "epoch 2 of  10 (7m 47s) 3.1391  firima ye-e, na ori na piki b /                                ✗ firima ye-e, na ori na piki be\n",
      "epoch 2 of  10 (7m 47s) 3.1391 dukobo-e now e ye, se ini mgba /                                ✗ ukobo-e now e ye, se ini mgba \n",
      "epoch 2 of  10 (8m 7s) 3.1234 e jinbo-jinbo mie sime-ari ye  /                                ✗  jinbo-jinbo mie sime-ari ye o\n",
      "epoch 2 of  10 (8m 7s) 3.1234 raivala now chin nama-e) aria  /                                ✗ aivala now chin nama-e) aria s\n",
      "epoch 2 of  10 (8m 26s) 3.0758 se o firimbia, ani o nwo i fir /                                ✗ e o firimbia, ani o nwo i firi\n",
      "epoch 2 of  10 (8m 26s) 3.0758  orime, ania bie o belema tamu /                                ✗ orime, ania bie o belema tamun\n",
      "epoch 2 of  10 (8m 45s) 3.0500 lame. o tamuno be torukubu ibi /                                ✗ ame. o tamuno be torukubu ibi \n",
      "epoch 2 of  10 (8m 45s) 3.0500 ian mi fie ye be koroma nwo ko /                                ✗ an mi fie ye be koroma nwo kon\n",
      "epoch 2 of  10 (9m 4s) 3.0151  beri ma gbeinme. anisiki inim /                                ✗ beri ma gbeinme. anisiki inimg\n",
      "epoch 2 of  10 (9m 4s) 3.0151  ju bo be tamuno teke bara mi  /                                ✗ ju bo be tamuno teke bara mi g\n",
      "epoch 2 of  10 (9m 24s) 3.0076 ro ini nwose chin ye mi ori o /                                ✗ o ini nwose chin ye mi ori or\n",
      "epoch 2 of  10 (9m 24s) 3.0076 muno be bara mi. o nyo goyegoy /                                ✗ uno be bara mi. o nyo goyegoye\n",
      "epoch 2 of  10 (9m 42s) 2.9783 e teke, miese o sib u barasin  /                                ✗  teke, miese o sib u barasin i\n",
      "epoch 2 of  10 (9m 42s) 2.9783 o jereusalem chochi mi piri be /                                ✗  jereusalem chochi mi piri ber\n",
      "epoch 2 of  10 (10m 2s) 2.9565 i mi-e,  ini se mun ba-emun an /                                ✗  mi-e,  ini se mun ba-emun ana\n",
      "epoch 2 of  10 (10m 2s) 2.9565 siki a nyanabo be duko ye mi n /                                ✗ iki a nyanabo be duko ye mi nw\n",
      "epoch 2 of  10 (10m 22s) 2.9738 am askos bie sime juapu tekewa /                                ✗ m askos bie sime juapu tekewar\n",
      "epoch 2 of  10 (10m 22s) 2.9738 me. antiok bie mi ini tatari k /                                ✗ e. antiok bie mi ini tatari kr\n",
      "epoch 3 of  10 (10m 41s) 2.9340 o oki damaskos bosam i tekeari /                                ✗  oki damaskos bosam i tekeari \n",
      "epoch 3 of  10 (10m 41s) 2.9340 koli wari mi bie biapakame. en /                                ✗ oli wari mi bie biapakame. enj\n",
      "epoch 3 of  10 (11m 0s) 2.9219 ni o babia bara. okuma gbori e /                                ✗ i o babia bara. okuma gbori en\n",
      "epoch 3 of  10 (11m 0s) 2.9219 i bo book mi finji se ori ori  /                                ✗  bo book mi finji se ori ori s\n",
      "epoch 3 of  10 (11m 20s) 2.9380 wosam. sise se iya monoye teli /                                ✗ osam. sise se iya monoye telim\n",
      "epoch 3 of  10 (11m 20s) 2.9380  simion (kurukurubo ini nwose  /                                ✗ simion (kurukurubo ini nwose c\n",
      "epoch 3 of  10 (11m 39s) 2.8923 kidiki kiri nwo lasa bo-a til /                                ✗ idiki kiri nwo lasa bo-a tili\n",
      "epoch 3 of  10 (11m 39s) 2.8923 anabia i torumgbolu punbia, gb /                                ✗ nabia i torumgbolu punbia, gbo\n",
      "epoch 3 of  10 (11m 58s) 2.9027 n siki ma a pulasa ye, anikani /                                ✗  siki ma a pulasa ye, anikanik\n",
      "epoch 3 of  10 (11m 58s) 2.9027 nkoro samuel be buo lame. se i /                                ✗ koro samuel be buo lame. se in\n",
      "epoch 3 of  10 (12m 17s) 2.8860 irime. okuma pita be o miese o /                                ✗ rime. okuma pita be o miese o \n",
      "epoch 3 of  10 (12m 17s) 2.8860 dupu bie chuame. okuma tamuno  /                                ✗ upu bie chuame. okuma tamuno b\n",
      "epoch 3 of  10 (12m 37s) 2.8880  ani nemmime, jizos kraist be  /                                ✗ ani nemmime, jizos kraist be b\n",
      "epoch 3 of  10 (12m 37s) 2.8880 ri. tomoni ma kobirima mi bie  /                                ✗ i. tomoni ma kobirima mi bie p\n",
      "epoch 3 of  10 (12m 56s) 2.9099 i fiafia teme be da okisam, an /                                ✗  fiafia teme be da okisam, ani\n",
      "epoch 3 of  10 (12m 56s) 2.9099 om bie  ani gbori bara mi ani  /                                ✗ m bie  ani gbori bara mi ani a\n",
      "epoch 3 of  10 (13m 22s) 2.8763 pirime, enjelbo be nwose o bem /                                ✗ irime, enjelbo be nwose o beme\n",
      "epoch 3 of  10 (13m 22s) 2.8763 mbulo na ngwengwe na nwose ama /                                ✗ bulo na ngwengwe na nwose ama \n",
      "epoch 3 of  10 (13m 45s) 2.8383 eme be na bereni na bie bein b /                                ✗ me be na bereni na bie bein bo\n",
      "epoch 3 of  10 (13m 45s) 2.8383 bereniapu ma bu chua se inia k /                                ✗ ereniapu ma bu chua se inia ku\n",
      "epoch 4 of  10 (14m 5s) 2.8590 li bie pakabome herod be ikoli /                                ✗ i bie pakabome herod be ikoli \n",
      "epoch 4 of  10 (14m 5s) 2.8590 mi ibi bara nwose ini okime, o /                                ✗ i ibi bara nwose ini okime, ok\n",
      "epoch 4 of  10 (14m 24s) 2.8559 o siki, a book mi finji ka bu, /                                ✗  siki, a book mi finji ka bu, \n",
      "epoch 4 of  10 (14m 24s) 2.8559  be jinseapu ma nwose yechin o /                                ✗ be jinseapu ma nwose yechin ok\n",
      "epoch 4 of  10 (14m 44s) 2.8718 nima mume. banabas be na sol b /                                ✗ ima mume. banabas be na sol be\n",
      "epoch 4 of  10 (14m 44s) 2.8718 nmame, ani antiok na, siria na /                                ✗ mame, ani antiok na, siria na,\n",
      "epoch 4 of  10 (15m 3s) 2.8415 o bie being bara bu ani o gamu /                                ✗  bie being bara bu ani o gamun\n",
      "epoch 4 of  10 (15m 3s) 2.8415 okwein mi nwo se ye die piki a /                                ✗ kwein mi nwo se ye die piki an\n",
      "epoch 4 of  10 (15m 22s) 2.8475 se kura mesi na gbasi ye bu or /                                ✗ e kura mesi na gbasi ye bu ori\n",
      "epoch 4 of  10 (15m 22s) 2.8475 ia kienbipi enbene-bene senime /                                ✗ a kienbipi enbene-bene senime.\n",
      "epoch 4 of  10 (15m 41s) 2.8547  some beerkon joizos be bara s /                                ✗ some beerkon joizos be bara si\n",
      "epoch 4 of  10 (15m 41s) 2.8547 ikase iri kokoma nyanabo be be /                                ✗ kase iri kokoma nyanabo be ber\n",
      "epoch 4 of  10 (16m 1s) 2.8211 a bie a nwengiari firima ani o /                                ✗  bie a nwengiari firima ani o \n",
      "epoch 4 of  10 (16m 1s) 2.8211 i-a ikoliapu ma ini pokiabe.   /                                ✗ -a ikoliapu ma ini pokiabe.  b\n",
      "epoch 4 of  10 (16m 20s) 2.8211 e toroko sa apu ma na juapu ma /                                ✗  toroko sa apu ma na juapu ma \n",
      "epoch 4 of  10 (16m 20s) 2.8211  anigose, o deinma bu so. okum /                                ✗ anigose, o deinma bu so. okuma\n",
      "epoch 4 of  10 (16m 40s) 2.8154 ini saki inia ama okwein laika /                                ✗ ni saki inia ama okwein laikao\n",
      "epoch 4 of  10 (16m 40s) 2.8154 u se jasin nwo chin gbori owub /                                ✗  se jasin nwo chin gbori owubo\n",
      "epoch 4 of  10 (16m 59s) 2.7900 a mi bu ori na banabas be na s /                                ✗  mi bu ori na banabas be na so\n",
      "epoch 4 of  10 (16m 59s) 2.7900 ume, pol be ini firimame na in /                                ✗ me, pol be ini firimame na ini\n",
      "epoch 5 of  10 (17m 18s) 2.7943 giapu ma na piki seniapu ma na /                                ✗ iapu ma na piki seniapu ma na \n",
      "epoch 5 of  10 (17m 18s) 2.7943 , piki ania bie sime yegoyegoy /                                ✗  piki ania bie sime yegoyegoye\n",
      "epoch 5 of  10 (17m 37s) 2.8121 amuno be inia bu boro se jinse /                                ✗ muno be inia bu boro se jinsea\n",
      "epoch 5 of  10 (17m 37s) 2.8121  nwo beme. ani pol be berechir /                                ✗ nwo beme. ani pol be berechiri\n",
      "epoch 5 of  10 (17m 56s) 2.8150 kama osi se piri mama apu nwo  /                                ✗ ama osi se piri mama apu nwo s\n",
      "epoch 5 of  10 (17m 56s) 2.8150  se piki mengisarame. gbori di /                                ✗ se piki mengisarame. gbori din\n",
      "epoch 5 of  10 (18m 15s) 2.7828 anga kele sime so siki, minapu /                                ✗ nga kele sime so siki, minapu \n",
      "epoch 5 of  10 (18m 15s) 2.7828 me ori ini na sime bolo sikila /                                ✗ e ori ini na sime bolo sikila,\n",
      "epoch 5 of  10 (18m 34s) 2.7827 ie tolu boro muari siki ini ok /                                ✗ e tolu boro muari siki ini oku\n",
      "epoch 5 of  10 (18m 34s) 2.7827 da obudukoapu o nwo orime, se  /                                ✗ a obudukoapu o nwo orime, se o\n",
      "epoch 5 of  10 (18m 54s) 2.7901 a-e, tayatira-ama bo-e, awuwu  /                                ✗ -e, tayatira-ama bo-e, awuwu b\n",
      "epoch 5 of  10 (18m 54s) 2.7901  nyanabo jizos be ere nwose pi /                                ✗ nyanabo jizos be ere nwose pik\n",
      "epoch 5 of  10 (19m 14s) 2.7725 bo be pririme na ori ibioku se /                                ✗ o be pririme na ori ibioku se \n",
      "epoch 5 of  10 (19m 14s) 2.7725 n-a soye okibo ania firinwengi /                                ✗ -a soye okibo ania firinwengia\n",
      "epoch 5 of  10 (19m 41s) 2.7877  bereinsam. ani be ene ba rom  /                                ✗ bereinsam. ani be ene ba rom b\n",
      "epoch 5 of  10 (19m 41s) 2.7877  bere mi dekibo ye-e, juapu ma /                                ✗ bere mi dekibo ye-e, juapu ma \n",
      "epoch 5 of  10 (20m 4s) 2.7686  na sailas be na busome ani ta /                                ✗ na sailas be na busome ani tam\n",
      "epoch 5 of  10 (20m 4s) 2.7686  ori ari irima tolu boro se to /                                ✗ ori ari irima tolu boro se tom\n",
      "epoch 5 of  10 (20m 24s) 2.7858 i tomoni ma chukuruma se amafi /                                ✗  tomoni ma chukuruma se amafie\n",
      "epoch 5 of  10 (20m 24s) 2.7858 a  wa toroko mun owoin-aru mi  /                                ✗   wa toroko mun owoin-aru mi l\n",
      "epoch 5 of  10 (20m 43s) 2.7820 aniatibi ari ominea ama mi bie /                                ✗ niatibi ari ominea ama mi bie \n",
      "epoch 5 of  10 (20m 43s) 2.7820 a bufuka na sime i koruabe. ok /                                ✗  bufuka na sime i koruabe. oku\n",
      "epoch 6 of  10 (21m 4s) 2.7622 aniatibi o karakara berekon to /                                ✗ niatibi o karakara berekon tom\n",
      "epoch 6 of  10 (21m 4s) 2.7622 wose kuroma firinwengi bu wa i /                                ✗ ose kuroma firinwengi bu wa in\n",
      "epoch 6 of  10 (21m 27s) 2.7617 n ini taitos jostos nwose chin /                                ✗  ini taitos jostos nwose chin \n",
      "epoch 6 of  10 (21m 27s) 2.7617 lip, okunwengibo be wari mi wa /                                ✗ ip, okunwengibo be wari mi wa \n",
      "epoch 6 of  10 (21m 47s) 2.7489 e nab u so se doku paka siria  /                                ✗  nab u so se doku paka siria m\n",
      "epoch 6 of  10 (21m 47s) 2.7489 nisiki nwose pol be beme, mina /                                ✗ isiki nwose pol be beme, minab\n",
      "epoch 6 of  10 (22m 7s) 2.7282 tibi ori be kuroma okwein bu,  /                                ✗ ibi ori be kuroma okwein bu, o\n",
      "epoch 6 of  10 (22m 7s) 2.7282 e kuro bara se pol be olome. i /                                ✗  kuro bara se pol be olome. in\n",
      "epoch 6 of  10 (22m 29s) 2.7404  bugbiripumaye nwo mieme. anig /                                ✗ bugbiripumaye nwo mieme. anigo\n",
      "epoch 6 of  10 (22m 29s) 2.7404 i ori okwein owunitibinibo be  /                                ✗  ori okwein owunitibinibo be p\n",
      "epoch 6 of  10 (22m 49s) 2.7234 idonia firima ye-e, okuma ori  /                                ✗ donia firima ye-e, okuma ori e\n",
      "epoch 6 of  10 (22m 49s) 2.7234  bia bia enekubu bunemika bu b /                                ✗ bia bia enekubu bunemika bu bi\n",
      "epoch 6 of  10 (23m 10s) 2.7772  ani tiwari mi bie. min siki m /                                ✗ ani tiwari mi bie. min siki mi\n",
      "epoch 6 of  10 (23m 10s) 2.7772 ie mi nwo oki eke i gbolomaye  /                                ✗ e mi nwo oki eke i gbolomaye b\n",
      "epoch 6 of  10 (23m 38s) 2.7494 ma bobia ye so ofori-e. o min  / i                              ✗ a bobia ye so ofori-e. o min b\n",
      "epoch 6 of  10 (23m 38s) 2.7494 ri ikasi warabe.  pol be berec / i                              ✗ i ikasi warabe.  pol be berech\n",
      "epoch 6 of  10 (23m 58s) 2.7467 ine obu balabalama ma, o pa du /                                ✗ ne obu balabalama ma, o pa dum\n",
      "epoch 6 of  10 (23m 58s) 2.7467 tibi mi piki gele teinme, se o /                                ✗ ibi mi piki gele teinme, se ol\n",
      "epoch 6 of  10 (24m 18s) 2.7706 ri ye-e, na ini inia si ma bu  /                         i      ✗ i ye-e, na ini inia si ma bu v\n",
      "epoch 6 of  10 (24m 18s) 2.7706 , se i beme, a be asemenibo be /                         i      ✗  se i beme, a be asemenibo be \n",
      "epoch 7 of  10 (24m 41s) 2.7431 gba werima belema mi seni o pi /                   i            ✗ ba werima belema mi seni o pir\n",
      "epoch 7 of  10 (24m 41s) 2.7431 e ikoli bie chubia ye, ini se  /                   i            ✗  ikoli bie chubia ye, ini se o\n",
      "epoch 7 of  10 (25m 3s) 2.7470 inbo beme, se wa owoin-aru mi  /                            i   ✗ nbo beme, se wa owoin-aru mi b\n",
      "epoch 7 of  10 (25m 3s) 2.7470 me ibi kubie nwose wa dukoari  /                            i   ✗ e ibi kubie nwose wa dukoari k\n",
      "epoch 7 of  10 (25m 23s) 2.7620 ene ba mi bu, pol be mine na m /        i             i       i ✗ ne ba mi bu, pol be mine na mu\n",
      "epoch 7 of  10 (25m 23s) 2.7620 ori torusedikiari ye mi nwo na /        i             i       i ✗ ri torusedikiari ye mi nwo nay\n",
      "epoch 7 of  10 (25m 43s) 2.7403  bumiefiafiama mi okibia ene m / b           i  i             i ✗ bumiefiafiama mi okibia ene ma\n",
      "epoch 7 of  10 (25m 43s) 2.7403  pol be okweinma mun se bo ibi / b           i  i             i ✗ pol be okweinma mun se bo ibim\n",
      "epoch 7 of  10 (26m 4s) 2.7397 o oki owuni bukikima wari bie  /                   i            ✗  oki owuni bukikima wari bie c\n",
      "epoch 7 of  10 (26m 4s) 2.7397 m bie sime siza be soni. festo /                   i            ✗  bie sime siza be soni. festos\n",
      "epoch 7 of  10 (26m 24s) 2.7054 kos bie sime ju minapu ma now  /           i     i      i       ✗ os bie sime ju minapu ma now d\n",
      "epoch 7 of  10 (26m 24s) 2.7054 umaye ma ma ini ari inia tamun /           i     i      i       ✗ maye ma ma ini ari inia tamuno\n",
      "epoch 7 of  10 (26m 44s) 2.7302 bu.  pol be chin se o bo tamun /                            i   ✗ u.  pol be chin se o bo tamuno\n",
      "epoch 7 of  10 (26m 44s) 2.7302 me tomonibo nwose ikoli sein,  /                            i   ✗ e tomonibo nwose ikoli sein, b\n",
      "epoch 7 of  10 (27m 4s) 2.7544 abe opu igbiki nwo gbein bu, o /                                ✗ be opu igbiki nwo gbein bu, ow\n",
      "epoch 7 of  10 (27m 4s) 2.7544 i bara deki ani mie kurommame. /                                ✗  bara deki ani mie kurommame. \n",
      "epoch 7 of  10 (27m 24s) 2.7081 sise se dumobia omin mi now ok /           i      i   i         ✗ ise se dumobia omin mi now okw\n",
      "epoch 7 of  10 (27m 24s) 2.7081 re mi dukome na ini inia sima  /           i      i   i         ✗ e mi dukome na ini inia sima b\n",
      "epoch 7 of  10 (27m 46s) 2.7385 mun se ani duko pol be piri ye / i                        i     ✗ un se ani duko pol be piri ye-\n",
      "epoch 7 of  10 (27m 46s) 2.7385 ulios be pirime. ori owunitibi / i                        i     ✗ lios be pirime. ori owunitibid\n",
      "epoch 8 of  10 (28m 5s) 2.7214 u ma, be tomonibo be kunme, se /   i        i                   ✗  ma, be tomonibo be kunme, se \n",
      "epoch 8 of  10 (28m 5s) 2.7214 okuma owutibinibo be, arukulo- /   i        i                   ✗ kuma owutibinibo be, arukulo-d\n",
      "epoch 8 of  10 (28m 25s) 2.7305 i bu iya yenemi torokoisun mi  /              i    i        i   ✗  bu iya yenemi torokoisun mi d\n",
      "epoch 8 of  10 (28m 25s) 2.7305  daye piki now deki toru mi bi /              i    i        i   ✗ daye piki now deki toru mi bie\n",
      "epoch 8 of  10 (28m 44s) 2.7220 in mi bereni i piriabe na a mi /    i    i        i          i  ✗ n mi bereni i piriabe na a min\n",
      "epoch 8 of  10 (28m 44s) 2.7220 sibia bara nwo dokime.ini aru  /    i    i        i          i  ✗ ibia bara nwo dokime.ini aru t\n",
      "epoch 8 of  10 (29m 6s) 2.7024 i oria ikiapu ma piri ini o bu /    i          i    i           ✗  oria ikiapu ma piri ini o bup\n",
      "epoch 8 of  10 (29m 6s) 2.7024 a ton werime na ini ikoliapu m /    i          i    i           ✗  ton werime na ini ikoliapu ma\n",
      "epoch 8 of  10 (29m 25s) 2.7150 e bo la siki jerusalem bie wer /                i     i       i ✗  bo la siki jerusalem bie weri\n",
      "epoch 8 of  10 (29m 25s) 2.7150 be so o sime jukuru mi bie mum /                i     i       i ✗ e so o sime jukuru mi bie mume\n",
      "epoch 8 of  10 (29m 45s) 2.7059 ogbo, inibo piki siki nyana se /                                ✗ gbo, inibo piki siki nyana se \n",
      "epoch 8 of  10 (29m 45s) 2.7059  se fi gbein ye ofori-e. okuma /                                ✗ se fi gbein ye ofori-e. okuma \n",
      "epoch 8 of  10 (30m 7s) 2.7105 a tonsam o se o firima. okuma  /        i          i i      i   ✗  tonsam o se o firima. okuma o\n",
      "epoch 8 of  10 (30m 7s) 2.7105 ma ini some, se kuro puko bu w /        i          i i      i   ✗ a ini some, se kuro puko bu we\n",
      "epoch 8 of  10 (30m 25s) 2.6984 e o fiapu sisema-sisemame iri  /               i      i i   i   ✗  o fiapu sisema-sisemame iri s\n",
      "epoch 8 of  10 (30m 25s) 2.6984 pakaye ma na, min oku mi be ku /               i      i i   i   ✗ akaye ma na, min oku mi be kur\n",
      "epoch 8 of  10 (30m 44s) 2.7114 moni ma bie inia kiri nwo okib / i    i             i           ✗ oni ma bie inia kiri nwo okibi\n",
      "epoch 8 of  10 (30m 44s) 2.7114 zos be nwose nin beme, sikima  / i    i             i           ✗ os be nwose nin beme, sikima n\n",
      "epoch 8 of  10 (31m 5s) 2.6981  se tamuno be bara biari ye an / b     i         i     i        ✗ se tamuno be bara biari ye ani\n",
      "epoch 8 of  10 (31m 5s) 2.6981  nwo beme, fiafiadiri mi duko  / b     i         i     i        ✗ nwo beme, fiafiadiri mi duko n\n",
      "epoch 9 of  10 (31m 26s) 2.7135 , okimun toru doku mi bo balaf /      i     i       i  i        ✗  okimun toru doku mi bo balafa\n",
      "epoch 9 of  10 (31m 26s) 2.7135 e ini o se oyi gboriokunwengia /      i     i       i  i        ✗  ini o se oyi gboriokunwengiap\n",
      "epoch 9 of  10 (31m 46s) 2.7102 igose ini ivala mi sin se owoi /                 i              ✗ gose ini ivala mi sin se owoin\n",
      "epoch 9 of  10 (31m 46s) 2.7102 pu ma bereniapu ma se kpotime, /                 i              ✗ u ma bereniapu ma se kpotime, \n",
      "epoch 9 of  10 (32m 6s) 2.6880 on dikime, se dokibo orime na  /        i              i i      ✗ n dikime, se dokibo orime na a\n",
      "epoch 9 of  10 (32m 6s) 2.6880 , o berenioki bara na, oria ne /        i              i i      ✗  o berenioki bara na, oria nem\n",
      "epoch 9 of  10 (32m 26s) 2.7033  join ma olo diki ma nwo fimas / b     i           i  i     i   ✗ join ma olo diki ma nwo fimasi\n",
      "epoch 9 of  10 (32m 26s) 2.7033 bara fiafia teme be now okisam / b     i           i  i     i   ✗ ara fiafia teme be now okisam,\n",
      "epoch 9 of  10 (32m 47s) 2.7071  inia kubie dima se now beme,  / b             i         i i    ✗ inia kubie dima se now beme, b\n",
      "epoch 9 of  10 (32m 47s) 2.7071 dumo kon oku mi okunwengiapu m / b             i         i i    ✗ umo kon oku mi okunwengiapu ma\n",
      "epoch 9 of  10 (33m 8s) 2.6985 ini i se mianga bome. a se min /          i      i i        i   ✗ ni i se mianga bome. a se mine\n",
      "epoch 9 of  10 (33m 8s) 2.6985 e buo ma andana iko na ma kur /          i      i i        i   ✗  buo ma andana iko na ma kuro\n",
      "epoch 9 of  10 (33m 33s) 2.6976  beri ma inikimasam pikina, in / b  i  i       i   i            ✗ beri ma inikimasam pikina, ini\n",
      "epoch 9 of  10 (33m 33s) 2.6976 isemasam, se mine wa onyechiea / b  i  i       i   i            ✗ semasam, se mine wa onyechieap\n",
      "epoch 9 of  10 (33m 54s) 2.6703 em mi biebu pentikost ene mi b /  i i  i                   i  i ✗ m mi biebu pentikost ene mi bu\n",
      "epoch 9 of  10 (33m 54s) 2.6703 pakabobia apu ma bie a boro se /  i i  i                   i  i ✗ akabobia apu ma bie a boro se \n",
      "epoch 9 of  10 (34m 16s) 2.6752  bu. jon be mengi se nweni sar / b        i  i        i       i ✗ bu. jon be mengi se nweni sara\n",
      "epoch 9 of  10 (34m 16s) 2.6752 a toroko gwosa bara bu chie si / b        i  i        i       i ✗  toroko gwosa bara bu chie sim\n",
      "epoch 9 of  10 (34m 37s) 2.6706 ms ya be na. ini siki mamgba g / i     i       i       i i      ✗ s ya be na. ini siki mamgba gb\n",
      "epoch 9 of  10 (34m 37s) 2.6706 e wa bara na ori ye na piki na / i     i       i       i i      ✗  wa bara na ori ye na piki naa\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 250\n",
    "plot_every = 100\n",
    "\n",
    "vloss = []\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "iter=0\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=nb_epoch):\n",
    "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    category =  [lin2txt(l) for l in y_]\n",
    "    lines = [lin2txt(l) for l in x]\n",
    "    category_tensor=mb2t(y_)\n",
    "    line_tensor=mb2t(x)\n",
    "    output, loss = train(torch.tensor(y_,dtype=torch.long), line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess = [lin2txt([ch.argmax(dim=0) for ch in line]) for line in output]\n",
    "        for i in range(2):\n",
    "            correct = '✓' if guess[i] == category[i] else '✗ %s' % category[i] \n",
    "            print('epoch %d of  %d (%s) %.4f %s / %s %s' % (epoch, nb_epoch, timeSince(start), loss, lines[i], guess[0], correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0 and len(valitext) > 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, BATCHSIZE, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        line_tensor = mb2t(vali_x)\n",
    "        output, loss = train(torch.tensor(vali_y, device=device, dtype=torch.long), line_tensor)\n",
    "        vloss.append(loss)\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x=[mb2t(ch) for ch in a[0]]\n",
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is=torch.Size([1, 98]), hs=torch.Size([1, 128])\n",
      "torch.Size([1, 98])\n"
     ]
    }
   ],
   "source": [
    "#input = lineToTensor('Albert')\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "output, next_hidden = rnn(a[0][0], hidden)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
