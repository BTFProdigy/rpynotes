{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.contrib import layers\n",
    "#from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import my_txtutils as txt\n",
    "#tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model parameters\n",
    "#\n",
    "# Usage:\n",
    "#   Training only:\n",
    "#         Leave all the parameters as they are\n",
    "#         Disable validation to run a bit faster (set validation=False below)\n",
    "#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n",
    "#   Training and experimentation (default):\n",
    "#         Keep validation enabled\n",
    "#         You can now play with the parameters anf follow the effects in Tensorboard\n",
    "#         A good choice of parameters ensures that the testing and validation curves stay close\n",
    "#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n",
    "#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n",
    "#\n",
    "nb_epoch=75\n",
    "SEQLEN = 30\n",
    "BATCHSIZE = 256\n",
    "ALPHASIZE = txt.ALPHASIZE\n",
    "INTERNALSIZE = 512\n",
    "NLAYERS = 3\n",
    "learning_rate = 0.001  # fixed learning rate\n",
    "dropout_pkeep = 0.8    # some dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# size of the alphabet that we work with\n",
    "ALPHASIZE = 98\n",
    "\n",
    "\n",
    "# Specification of the supported alphabet (subset of ASCII-7)\n",
    "# 10 line feed LF\n",
    "# 32-64 numbers and punctuation\n",
    "# 65-90 upper-case letters\n",
    "# 91-97 more punctuation\n",
    "# 97-122 lower-case letters\n",
    "# 123-126 more punctuation\n",
    "def convert_from_alphabet(a):\n",
    "    \"\"\"Encode a character\n",
    "    :param a: one character\n",
    "    :return: the encoded value\n",
    "    \"\"\"\n",
    "    if a == 9:\n",
    "        return 1\n",
    "    if a == 10:\n",
    "        return 127 - 30  # LF\n",
    "    elif 32 <= a <= 126:\n",
    "        return a - 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "# encoded values:\n",
    "# unknown = 0\n",
    "# tab = 1\n",
    "# space = 2\n",
    "# all chars from 32 to 126 = c-30\n",
    "# LF mapped to 127-30\n",
    "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode a code point\n",
    "    :param c: code point\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return: decoded character\n",
    "    \"\"\"\n",
    "    if c == 1:\n",
    "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
    "    if c == 127 - 30:\n",
    "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
    "    if 32 <= c + 30 <= 126:\n",
    "        return c + 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "def encode_text(s):\n",
    "    \"\"\"Encode a string.\n",
    "    :param s: a text string\n",
    "    :return: encoded list of code points\n",
    "    \"\"\"\n",
    "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
    "\n",
    "\n",
    "def decode_to_text(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode an encoded string.\n",
    "    :param c: encoded list of code points\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
    "\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(probabilities)\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
    "\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "\n",
    "\n",
    "def find_book(index, bookranges):\n",
    "    return next(\n",
    "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def find_book_index(index, bookranges):\n",
    "    return next(\n",
    "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
    "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
    "    print()\n",
    "    # epoch_size in number of batches\n",
    "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
    "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
    "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "    for k in range(batch_size):\n",
    "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
    "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
    "        bookname = find_book(index_in_epoch, bookranges)\n",
    "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
    "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
    "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
    "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
    "        print(print_string.format(decx, decy, loss_string))\n",
    "        index += sequence_len\n",
    "    # box formatting characters:\n",
    "    # │ \\u2502\n",
    "    # ─ \\u2500\n",
    "    # └ \\u2514\n",
    "    # ┘ \\u2518\n",
    "    # ┴ \\u2534\n",
    "    # ┌ \\u250C\n",
    "    # ┐ \\u2510\n",
    "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
    "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
    "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
    "    print(footer)\n",
    "    # print statistics\n",
    "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
    "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
    "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
    "    print()\n",
    "    print(\"TRAINING STATS: {}\".format(stats))\n",
    "\n",
    "\n",
    "class Progress:\n",
    "    \"\"\"Text mode progress bar.\n",
    "    Usage:\n",
    "            p = Progress(30)\n",
    "            p.step()\n",
    "            p.step()\n",
    "            p.step(start=True) # to restart form 0%\n",
    "    The progress bar displays a new header at each restart.\"\"\"\n",
    "    def __init__(self, maxi, size=100, msg=\"\"):\n",
    "        \"\"\"\n",
    "        :param maxi: the number of steps required to reach 100%\n",
    "        :param size: the number of characters taken on the screen by the progress bar\n",
    "        :param msg: the message displayed in the header of the progress bat\n",
    "        \"\"\"\n",
    "        self.maxi = maxi\n",
    "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
    "        self.header_printed = False\n",
    "        self.msg = msg\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, reset=False):\n",
    "        if reset:\n",
    "            self.__init__(self.maxi, self.size, self.msg)\n",
    "        if not self.header_printed:\n",
    "            self.__print_header()\n",
    "        next(self.p)\n",
    "\n",
    "    def __print_header(self):\n",
    "        print()\n",
    "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
    "        print(format_string.format(self.msg))\n",
    "        self.header_printed = True\n",
    "\n",
    "    def __start_progress(self, maxi):\n",
    "        def print_progress():\n",
    "            # Bresenham's algorithm. Yields the number of dots printed.\n",
    "            # This will always print 100 dots in max invocations.\n",
    "            dx = maxi\n",
    "            dy = self.size\n",
    "            d = dy - dx\n",
    "            for x in range(maxi):\n",
    "                k = 0\n",
    "                while d >= 0:\n",
    "                    print('=', end=\"\", flush=True)\n",
    "                    k += 1\n",
    "                    d -= dx\n",
    "                d += dy\n",
    "                yield k\n",
    "\n",
    "        return print_progress\n",
    "\n",
    "\n",
    "def read_data_files(directory, validation=True):\n",
    "    \"\"\"Read data files according to the specified glob pattern\n",
    "    Optionnaly set aside the last file as validation data.\n",
    "    No validation data is returned if there are 5 files or less.\n",
    "    :param directory: for example \"data/*.txt\"\n",
    "    :param validation: if True (default), sets the last file aside as validation data\n",
    "    :return: training data, validation data, list of loaded file names with ranges\n",
    "     If validation is\n",
    "    \"\"\"\n",
    "    codetext = []\n",
    "    bookranges = []\n",
    "    shakelist = glob.glob(directory, recursive=True)\n",
    "    for shakefile in shakelist:\n",
    "        shaketext = open(shakefile, \"r\")\n",
    "        print(\"Loading file \" + shakefile)\n",
    "        start = len(codetext)\n",
    "        codetext.extend(encode_text(shaketext.read()))\n",
    "        end = len(codetext)\n",
    "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
    "        shaketext.close()\n",
    "\n",
    "    if len(bookranges) == 0:\n",
    "        sys.exit(\"No training data has been found. Aborting.\")\n",
    "\n",
    "    # For validation, use roughly 90K of text,\n",
    "    # but no more than 10% of the entire text\n",
    "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "    # 10% of the text is how many files ?\n",
    "    total_len = len(codetext)\n",
    "    validation_len = 0\n",
    "    nb_books1 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books1 += 1\n",
    "        if validation_len > total_len // 10:\n",
    "            break\n",
    "\n",
    "    # 90K of text is how many books ?\n",
    "    validation_len = 0\n",
    "    nb_books2 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books2 += 1\n",
    "        if validation_len > 90*1024:\n",
    "            break\n",
    "\n",
    "    # 20% of the books is how many books ?\n",
    "    nb_books3 = len(bookranges) // 5\n",
    "\n",
    "    # pick the smallest\n",
    "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
    "\n",
    "    if nb_books == 0 or not validation:\n",
    "        cutoff = len(codetext)\n",
    "    else:\n",
    "        cutoff = bookranges[-nb_books][\"start\"]\n",
    "    valitext = codetext[cutoff:]\n",
    "    codetext = codetext[:cutoff]\n",
    "    return codetext, valitext, bookranges\n",
    "\n",
    "\n",
    "def print_data_stats(datalen, valilen, epoch_size):\n",
    "    datalen_mb = datalen/1024.0/1024.0\n",
    "    valilen_kb = valilen/1024.0\n",
    "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
    "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
    "\n",
    "\n",
    "def print_validation_header(validation_start, bookranges):\n",
    "    bookindex = find_book_index(validation_start, bookranges)\n",
    "    books = ''\n",
    "    for i in range(bookindex, len(bookranges)):\n",
    "        books += bookranges[i][\"name\"]\n",
    "        if i < len(bookranges)-1:\n",
    "            books += \", \"\n",
    "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
    "\n",
    "\n",
    "def print_validation_stats(loss, accuracy):\n",
    "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
    "                                                                                                           accuracy))\n",
    "\n",
    "\n",
    "def print_text_generation_header():\n",
    "    print()\n",
    "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
    "\n",
    "\n",
    "def print_text_generation_footer():\n",
    "    print()\n",
    "    print(\"└{:─^111}┘\".format('End of generation'))\n",
    "\n",
    "\n",
    "def frequency_limiter(n, multiple=1, modulo=0):\n",
    "    def limit(i):\n",
    "        return i % (multiple * n) == modulo*multiple\n",
    "    return limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file txts\\acts_new.txt\n",
      "Loading file txts\\gal_eph_new.txt\n",
      "Loading file txts\\heb_new.txt\n",
      "Loading file txts\\jam_jud_new.txt\n",
      "Loading file txts\\john_new.txt\n",
      "Loading file txts\\jud_rev_new.txt\n",
      "Loading file txts\\luke_8_john_new.txt\n",
      "Loading file txts\\mark01_new.txt\n",
      "Loading file txts\\matt02_new.txt\n",
      "Loading file txts\\matt_new.txt\n",
      "Loading file txts\\phil_col_new.txt\n",
      "Loading file txts\\thes_tim_new.txt\n",
      "Loading file txts\\tit_phl_new.txt\n"
     ]
    }
   ],
   "source": [
    "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
    "shakedir = \"txts/*.txt\"\n",
    "#shakedir = \"../tensorflow/**/*.py\"\n",
    "codetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text size is 5.83MB with 384.30KB set aside for validation. There will be 796 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "# display some stats on the data\n",
    "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
    "txt.print_data_stats(len(codetext), len(valitext), epoch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "source": [
    "#\n",
    "# the model (see FAQ in README.md)\n",
    "#\n",
    "lr = tf.placeholder(tf.float32, name='lr')  # learning rate\n",
    "pkeep = tf.placeholder(tf.float32, name='pkeep')  # dropout parameter\n",
    "batchsize = tf.placeholder(tf.int32, name='batchsize')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# inputs\n",
    "X = tf.placeholder(tf.uint8, [None, None], name='X')    # [ BATCHSIZE, SEQLEN ]\n",
    "Xo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n",
    "# expected outputs = same sequence shifted by 1 since we are trying to predict the next character\n",
    "Y_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  # [ BATCHSIZE, SEQLEN ]\n",
    "Yo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n",
    "# input state\n",
    "Hin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
    "\n",
    "# using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times\n",
    "# dynamic_rnn infers SEQLEN from the size of the inputs Xo\n",
    "\n",
    "# How to properly apply dropout in RNNs: see README.md\n",
    "cells = [rnn.GRUCell(INTERNALSIZE) for _ in range(NLAYERS)]\n",
    "# \"naive dropout\" implementation\n",
    "dropcells = [rnn.DropoutWrapper(cell,input_keep_prob=pkeep) for cell in cells]\n",
    "multicell = rnn.MultiRNNCell(dropcells, state_is_tuple=False)\n",
    "multicell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)  # dropout for the softmax layer\n",
    "\n",
    "Yr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)\n",
    "# Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]\n",
    "# H:  [ BATCHSIZE, INTERNALSIZE*NLAYERS ] # this is the last state in the sequence\n",
    "\n",
    "H = tf.identity(H, name='H')  # just to give it a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(BATCHSIZE, self.hidden_size)\n",
    "\n",
    "n_hidden = INTERNALSIZE\n",
    "rnn = RNN(ALPHASIZE, n_hidden, ALPHASIZE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": true
    }
   },
   "source": [
    "# Softmax layer implementation:\n",
    "# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n",
    "# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n",
    "\n",
    "Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\n",
    "Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\n",
    "loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\n",
    "Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\n",
    "Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# loss fn\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#from ok_seq2seq import EncoderRNN \\\n",
    "#                        ,DecoderRNN \\\n",
    "#                        ,AttnDecoderRNN \\\n",
    "#                        ,evaluateRandomly \\\n",
    "#                        ,teacher_forcing_ratio \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# stats for display\n",
    "seqloss = tf.reduce_mean(loss, 1)\n",
    "batchloss = tf.reduce_mean(seqloss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\n",
    "loss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\n",
    "acc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\n",
    "summaries = tf.summary.merge([loss_summary, acc_summary])\n",
    "\n",
    "# Init Tensorboard stuff. This will save Tensorboard information into a different\n",
    "# folder at each run named 'log/<timestamp>/'. Two sets of data are saved so that\n",
    "# you can compare training and validation curves visually in Tensorboard.\n",
    "timestamp = str(math.trunc(time.time()))\n",
    "summary_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-training\")\n",
    "validation_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-validation\")\n",
    "\n",
    "# Init for saving models. They will be saved into a directory named 'checkpoints'.\n",
    "# Only the last checkpoint is kept.\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")\n",
    "saver = tf.train.Saver(max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "\n",
    "evaluateRandomly(encoder1, attn_decoder1)\n",
    "\n",
    "\"\"\"Visualizing Attention\n",
    "---------------------\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "plt.matshow(attentions.numpy())\n",
    "\n",
    "\"\"\"For a better viewing experience we will do the extra work of adding axes\n",
    "and labels:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training fn\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    lint = []\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "        lint.append(output)\n",
    "    input = torch.stack(lint).transpose(0,1).transpose(1,2)\n",
    "#     print(f'is={input.size()}, cs={category_tensor.size()}')\n",
    "#     print(f'is[1:]={input.size()[1:]}, cs[1:]={category_tensor.size()[1:]}')\n",
    "#     print(f'is[2:]={input.size()[2:]}, cs[2:]={category_tensor.size()[2:]}')\n",
    "    loss = criterion(input, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return torch.stack(lint).transpose(0,1), loss.item()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# for display: init the progress bar\n",
    "DISPLAY_FREQ = 50\n",
    "_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n",
    "progress = txt.Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n",
    "\n",
    "# init\n",
    "istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# init train\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def one_hot(chcode):\n",
    "    tensor = torch.zeros(1, ALPHASIZE)\n",
    "    tensor[0][chcode] = 1\n",
    "    return tensor\n",
    "\n",
    "def mb2t(rows):\n",
    "    rows=rows.transpose()\n",
    "    tensor = torch.zeros(rows.shape[0], rows.shape[1], ALPHASIZE)\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, letter_code in enumerate(row):\n",
    "            tensor[i][j][letter_code] = 1\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def lin2txt(lt):\n",
    "    return ''.join([chr(txt.convert_to_alphabet(c))  if c != 0 else '' for c in lt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 of  75 (0m 1s) 4.5776  okunwengiapu  / XX)D),RDRX7X&XRo7XRXFDXX)X3X)D ✗  okunwengiapu m\n",
      "epoch 0 of  75 (0m 1s) 4.5776 ie gbolomaye mi / XX)D),RDRX7X&XRo7XRXFDXX)X3X)D ✗ e gbolomaye mi \n",
      "epoch 0 of  75 (2m 55s) 2.5496 wo sobie mun ok /  ✗ o sobie mun oku\n",
      "epoch 0 of  75 (2m 55s) 2.5496 ikina bu teme b /  ✗ kina bu teme be\n",
      "epoch 0 of  75 (5m 46s) 1.7227  isiapu ma bere /              ✗ isiapu ma beren\n",
      "epoch 0 of  75 (5m 46s) 1.7227 , mine na omin /              ✗  mine na omine\n",
      "epoch 0 of  75 (8m 27s) 1.6633 aist be ere bu  /              ✗ ist be ere bu o\n",
      "epoch 0 of  75 (8m 27s) 1.6633 e ye-e, nde iri /              ✗  ye-e, nde iri \n",
      "epoch 1 of  75 (11m 17s) 1.6163  mun dumo bie c /               ✗ mun dumo bie ch\n",
      "epoch 1 of  75 (11m 17s) 1.6163 . o tonme na in /               ✗  o tonme na ini\n",
      "epoch 1 of  75 (13m 55s) 1.5833  ofori-e, aniat /               ✗ ofori-e, aniati\n",
      "epoch 1 of  75 (13m 55s) 1.5833 ye mamgba nwo d /               ✗ e mamgba nwo di\n",
      "epoch 1 of  75 (16m 47s) 1.5915  werima i nyana /               ✗ werima i nyanay\n",
      "epoch 1 of  75 (16m 47s) 1.5915 ime. owuapu, o  /               ✗ me. owuapu, o p\n",
      "epoch 2 of  75 (19m 31s) 1.5827 reme, tamuno be /               ✗ eme, tamuno be \n",
      "epoch 2 of  75 (19m 31s) 1.5827 a teme be na tu /               ✗  teme be na tub\n",
      "epoch 2 of  75 (22m 9s) 1.5517 pu ma na stivin /               ✗ u ma na stivin \n",
      "epoch 2 of  75 (22m 9s) 1.5517 u karakarake. m /               ✗  karakarake. mi\n",
      "epoch 2 of  75 (24m 48s) 1.5425 lu ini nwose wa /               ✗ u ini nwose war\n",
      "epoch 2 of  75 (24m 48s) 1.5425 i siseme, ori o /               ✗  siseme, ori o \n",
      "epoch 3 of  75 (27m 25s) 1.5425  lame. o tamuno /                ✗ lame. o tamuno \n",
      "epoch 3 of  75 (27m 25s) 1.5425 ime bo. pita be /                ✗ me bo. pita be,\n",
      "epoch 3 of  75 (29m 58s) 1.5418 os be ere bu ke /                ✗ s be ere bu ken\n",
      "epoch 3 of  75 (29m 58s) 1.5418 ri ini se firim /                ✗ i ini se firima\n",
      "epoch 3 of  75 (32m 27s) 1.5205 dam askos bie s /                ✗ am askos bie si\n",
      "epoch 3 of  75 (32m 27s) 1.5205 be opu mi nwo b /                ✗ e opu mi nwo bo\n",
      "epoch 4 of  75 (35m 1s) 1.5207 piki grik okwei /                ✗ iki grik okwein\n",
      "epoch 4 of  75 (35m 1s) 1.5207 inia minapu, ju /                ✗ nia minapu, jud\n",
      "epoch 4 of  75 (37m 37s) 1.5212 in siki ma a pu /                ✗ n siki ma a pul\n",
      "epoch 4 of  75 (37m 37s) 1.5212 bu ngule sarame /                ✗ u ngule sarame \n",
      "epoch 4 of  75 (40m 13s) 1.5122 bume o fibusise /                ✗ ume o fibusise \n",
      "epoch 4 of  75 (40m 13s) 1.5122 omabo nwo mie y /                ✗ mabo nwo mie ye\n",
      "epoch 5 of  75 (43m 9s) 1.5006 teme be na bere /                ✗ eme be na beren\n",
      "epoch 5 of  75 (43m 9s) 1.5006 ekeapu ma bie b /                ✗ keapu ma bie bi\n",
      "epoch 5 of  75 (46m 10s) 1.5026 me na ini ini b /                ✗ e na ini ini ba\n",
      "epoch 5 of  75 (46m 10s) 1.5026  mi dukome, se  /                ✗ mi dukome, se a\n",
      "epoch 5 of  75 (48m 53s) 1.4924  se kura mesi n /                ✗ se kura mesi na\n",
      "epoch 5 of  75 (48m 53s) 1.4924 ini pol be na b /                ✗ ni pol be na ba\n",
      "epoch 5 of  75 (51m 55s) 1.4881  wolome. okuma  /                ✗ wolome. okuma p\n",
      "epoch 5 of  75 (51m 55s) 1.4881 ian mi bie o ma /                ✗ an mi bie o mas\n",
      "epoch 6 of  75 (55m 17s) 1.4705 ba mi bu ori na /                ✗ a mi bu ori na \n",
      "epoch 6 of  75 (55m 17s) 1.4705  na biebele, in /                ✗ na biebele, ini\n",
      "epoch 6 of  75 (58m 52s) 1.4763  oku-e, min mi  /                ✗ oku-e, min mi n\n",
      "epoch 6 of  75 (58m 52s) 1.4763  ogbo apu na st /                ✗ ogbo apu na sto\n",
      "epoch 6 of  75 (61m 47s) 1.4772 bie tolu boro m /                ✗ ie tolu boro mu\n",
      "epoch 6 of  75 (61m 47s) 1.4772 mun, a jinseapu /                ✗ un, a jinseapu \n",
      "epoch 7 of  75 (64m 42s) 1.4578 ma mine mgba wa /   i    i       ✗ a mine mgba wa \n",
      "epoch 7 of  75 (64m 42s) 1.4578  ini nwo bereni /   i    i       ✗ ini nwo bereni-\n",
      "epoch 7 of  75 (68m 12s) 1.4658 ki tomoni ma ch /     i    i     ✗ i tomoni ma chu\n",
      "epoch 7 of  75 (68m 12s) 1.4658 bpu ma firifirm /     i    i     ✗ pu ma firifirm \n",
      "epoch 7 of  75 (71m 4s) 1.4374 roma so. pol be /     i          ✗ oma so. pol be \n",
      "epoch 7 of  75 (71m 4s) 1.4374 bia, miese esia /     i          ✗ ia, miese esia \n",
      "epoch 8 of  75 (73m 53s) 1.4439 atibi ori be ku /                 ✗ tibi ori be kur\n",
      "epoch 8 of  75 (73m 53s) 1.4439  bie kpereki se /                 ✗ bie kpereki se \n",
      "epoch 8 of  75 (76m 52s) 1.4350 iokuma minea fi /       i        ✗ okuma minea fir\n",
      "epoch 8 of  75 (76m 52s) 1.4350 ome.)  bebe ama /       i        ✗ me.)  bebe ama \n",
      "epoch 8 of  75 (80m 1s) 1.4262 mine obu balaba /      i  i   i  ✗ ine obu balabal\n",
      "epoch 8 of  75 (80m 1s) 1.4262 ma muari ye sis /      i  i   i  ✗ a muari ye sise\n",
      "epoch 9 of  75 (83m 22s) 1.4085  se wa some. wa / i ii ii i   i  ✗ se wa some. wa \n",
      "epoch 9 of  75 (83m 22s) 1.4085  dukoma din mi  / i ii ii i   i  ✗ dukoma din mi b\n",
      "epoch 9 of  75 (86m 1s) 1.4166 , bumiefiafiama /  i i        i  ✗  bumiefiafiama \n",
      "epoch 9 of  75 (86m 1s) 1.4166 o pol be punuma /  i i        i  ✗  pol be punumam\n",
      "epoch 9 of  75 (88m 46s) 1.4335 ko i piribia. i /    ii i        ✗ o i piribia. i \n",
      "epoch 9 of  75 (88m 46s) 1.4335  ani-erechi o p /    ii i        ✗ ani-erechi o po\n",
      "epoch 10 of  75 (91m 29s) 1.4061  sise se dumobi / i i ii ii i i  ✗ sise se dumobia\n",
      "epoch 10 of  75 (91m 29s) 1.4061 e ori so bara n / i i ii ii i i  ✗  ori so bara na\n",
      "epoch 10 of  75 (94m 28s) 1.4132 n asun se nweng /   i   i ii  i  ✗  asun se nwengi\n",
      "epoch 10 of  75 (94m 28s) 1.4132 i be na duko si /   i   i ii  i  ✗  be na duko sim\n",
      "epoch 10 of  75 (97m 4s) 1.3814 ni oria ikiapu  /  i i  i i  i   ✗ i oria ikiapu m\n",
      "epoch 10 of  75 (97m 4s) 1.3814  jine mun libia /  i i  i i  i   ✗ jine mun libia \n",
      "epoch 10 of  75 (100m 15s) 1.3802 apu barachua ok /  i  i i i   i i ✗ pu barachua oki\n",
      "epoch 10 of  75 (100m 15s) 1.3802 molta bie   wa  /  i  i i i   i i ✗ olta bie   wa d\n",
      "epoch 11 of  75 (103m 11s) 1.4002 omoni ma bie in /  i i ii ii  i i ✗ moni ma bie ini\n",
      "epoch 11 of  75 (103m 11s) 1.4002 aniatii mina to /  i i ii ii  i i ✗ niatii mina tom\n",
      "epoch 11 of  75 (106m 1s) 1.3797 ru saki memema  /   i i ii i i i ✗ u saki memema f\n",
      "epoch 11 of  75 (106m 1s) 1.3797  iniatorukubu o /   i i ii i i i ✗ iniatorukubu o \n",
      "epoch 11 of  75 (108m 27s) 1.3644 e join ma olo d /  ii  i i i i ii ✗  join ma olo di\n",
      "epoch 11 of  75 (108m 27s) 1.3644 m bie bome, jua /  ii  i i i i ii ✗  bie bome, juap\n",
      "epoch 12 of  75 (111m 8s) 1.3565  weri sime bara / i i ii i oi i  ✗ weri sime bara,\n",
      "epoch 12 of  75 (111m 8s) 1.3565 no bakuba bo be / i i ii i oi i  ✗ o bakuba bo be,\n",
      "epoch 12 of  75 (113m 41s) 1.3686 e bu. jon be me /  ii   i i i ii  ✗  bu. jon be men\n",
      "epoch 12 of  75 (113m 41s) 1.3686  duko pele na o /  ii   i i i ii  ✗ duko pele na or\n",
      "epoch 12 of  75 (116m 4s) 1.3590  bie anga juapu / i  oii  oi  i  ✗ bie anga juapu \n",
      "epoch 12 of  75 (116m 4s) 1.3590 erechiri mi be  / i  oii  oi  i  ✗ rechiri mi be a\n",
      "epoch 13 of  75 (118m 27s) 1.3327 bere nwo dukome /  i ii  oi i i  ✗ ere nwo dukome,\n",
      "epoch 13 of  75 (118m 27s) 1.3327 o boroma okunwe /  i ii  oi i i  ✗  boroma okunwen\n",
      "epoch 13 of  75 (120m 47s) 1.3408 ekewari mi bie  /  i   i oi oi  b ✗ kewari mi bie n\n",
      "epoch 13 of  75 (120m 47s) 1.3408 e ere o se yedi /  i   i oi oi  b ✗  ere o se yedie\n",
      "epoch 13 of  75 (123m 6s) 1.3340 ekereme, omine  /  i i i   ii i b ✗ kereme, omine t\n",
      "epoch 13 of  75 (123m 6s) 1.3340  ngeriminabu ny /  i i i   ii i b ✗ ngeriminabu nya\n",
      "epoch 14 of  75 (125m 25s) 1.3400 nwose inimgba o /   i b i i   b  ✗ wose inimgba og\n",
      "epoch 14 of  75 (125m 25s) 1.3400 ni mie se ini i /   i b i i   b  ✗ i mie se ini in\n",
      "epoch 14 of  75 (127m 48s) 1.3420 mi angasiki kor /  b i  i i bi i ✗ i angasiki koru\n",
      "epoch 14 of  75 (127m 48s) 1.3420 e chua joswa be /  b i  i i bi i ✗  chua joswa be \n",
      "epoch 14 of  75 (130m 16s) 1.3205  ye mi now bere / i bi oi i i i  ✗ ye mi now beren\n",
      "epoch 14 of  75 (130m 16s) 1.3205 iafia teme be n / i bi oi i i i  ✗ afia teme be ny\n",
      "epoch 15 of  75 (132m 39s) 1.3129 ma o lekirime,  /  b ii i i i  b ✗ a o lekirime, s\n",
      "epoch 15 of  75 (132m 39s) 1.3129 le agbamiebo be /  b ii i i i  b ✗ e agbamiebo be \n",
      "epoch 15 of  75 (135m 34s) 1.3015 i yee. ani tamu /  ii   b i bi i  ✗  yee. ani tamun\n",
      "epoch 15 of  75 (135m 34s) 1.3015  nyanabo be ere /  ii   b i bi i  ✗ nyanabo be ere \n",
      "epoch 15 of  75 (138m 3s) 1.3091 ,  uri ani tam /  bi   b i bi i ✗   uri ani tamu\n",
      "epoch 15 of  75 (138m 3s) 1.3091 anabo jin siki  /  bi   b i bi i ✗ nabo jin siki m\n",
      "epoch 16 of  75 (140m 33s) 1.3084 lu mgba se bere /  bi i bi bi i  ✗ u mgba se beren\n",
      "epoch 16 of  75 (140m 33s) 1.3084  o piki ton wa  /  bi i bi bi i  ✗ o piki ton wa p\n",
      "epoch 16 of  75 (143m 2s) 1.3200  okuma gbori en / ii i bii i b i ✗ okuma gbori ene\n",
      "epoch 16 of  75 (143m 2s) 1.3200 tomoni ini nwo  / ii i bii i b i ✗ omoni ini nwo o\n",
      "epoch 16 of  75 (145m 29s) 1.2830 ama apu nwo chi /  i b i bi  bii  ✗ ma apu nwo chin\n",
      "epoch 16 of  75 (145m 29s) 1.2830 bie kele simeme /  i b i bi  bii  ✗ ie kele simeme.\n",
      "epoch 16 of  75 (148m 29s) 1.2924 i mamgba nyanab /  ii i   bi  i i ✗  mamgba nyanabo\n",
      "epoch 16 of  75 (148m 29s) 1.2924 o famame, se or /  ii i   bi  i i ✗  famame, se ori\n",
      "epoch 17 of  75 (150m 59s) 1.3011 ma gose ani ini /  bi i b i bei  ✗ a gose ani ini \n",
      "epoch 17 of  75 (150m 59s) 1.3011 ari tamuno okwe /  bi i b i bei  ✗ ri tamuno okwei\n",
      "epoch 17 of  75 (153m 28s) 1.2938 se a beme. okum /  b ii i  b i i ✗ e a beme. okuma\n",
      "epoch 17 of  75 (153m 28s) 1.2938 udukoapu nwo ny /  b ii i  b i i ✗ dukoapu nwo nya\n",
      "epoch 17 of  75 (155m 53s) 1.2751 ri mi bie smun  /  bi bi  bi     ✗ i mi bie smun s\n",
      "epoch 17 of  75 (155m 53s) 1.2751 i eke. aninakar /  bi bi  bi     ✗  eke. aninakara\n",
      "epoch 18 of  75 (158m 21s) 1.2673  boari inyosara / bi  i b i  i i  ✗ boari inyosara \n",
      "epoch 18 of  75 (158m 21s) 1.2673 dukoari ye ma n / bi  i b i  i i  ✗ ukoari ye ma nw\n",
      "epoch 18 of  75 (160m 46s) 1.2691  o sobie ene se / bioi i  b i bi  ✗ o sobie ene sen\n",
      "epoch 18 of  75 (160m 46s) 1.2691 o miese a duabo / bioi i  b i bi  ✗  miese a duabor\n",
      "epoch 18 of  75 (163m 14s) 1.2646 eapu ma nwose y /   i bi bi  i bi ✗ apu ma nwose ye\n",
      "epoch 18 of  75 (163m 14s) 1.2646  anisiki ini vi /   i bi bi  i bi ✗ anisiki ini vin\n",
      "epoch 19 of  75 (165m 38s) 1.2732 se o chochi ma  /  b iii i  bi b ✗ e o chochi ma m\n",
      "epoch 19 of  75 (165m 38s) 1.2732 u ma na grikapu /  b iii i  bi b ✗  ma na grikapu \n",
      "epoch 19 of  75 (169m 8s) 1.2542 omabe, isia iko /  i i  b i  b i  ✗ mabe, isia ikol\n",
      "epoch 19 of  75 (169m 8s) 1.2542 i gbelame, fiaf /  i i  b i  b i  ✗  gbelame, fiafi\n",
      "epoch 19 of  75 (174m 11s) 1.2738 me. aninyo sime /   b i i  bi i  ✗ e. aninyo sime \n",
      "epoch 19 of  75 (174m 11s) 1.2738 lokume, fisos  /   b i i  bi i  ✗ okume, fisos a\n",
      "epoch 20 of  75 (177m 14s) 1.2663 apu obu some, s /  i b i bi i  bi ✗ pu obu some, se\n",
      "epoch 20 of  75 (177m 14s) 1.2663 a nyo mi. o duk /  i b i bi i  bi ✗  nyo mi. o duko\n",
      "epoch 20 of  75 (179m 33s) 1.2538  be na o duko y / bi bi b oi i bi ✗ be na o duko ye\n",
      "epoch 20 of  75 (179m 33s) 1.2538 yo da berreniap / bi bi b oi i bi ✗ o da berreniapu\n",
      "epoch 20 of  75 (181m 49s) 1.2496 pu ma, se nwose /  bi  bi bi  i  ✗ u ma, se nwose \n",
      "epoch 20 of  75 (181m 49s) 1.2496 ari mi na tubur /  bi  bi bi  i  ✗ ri mi na tuburu\n",
      "epoch 21 of  75 (184m 7s) 1.2273 rima sime ogono /  i bi i b i i  ✗ ima sime ogono \n",
      "epoch 21 of  75 (184m 7s) 1.2273 i ye i nwo nemi /  i bi i b i i  ✗  ye i nwo nemib\n",
      "epoch 21 of  75 (186m 24s) 1.2586  pol be ori okw / bi ibi b i b i  ✗ pol be ori okwe\n",
      "epoch 21 of  75 (186m 24s) 1.2586  balafame nwofa / bi ibi b i b i  ✗ balafame nwofa \n",
      "epoch 21 of  75 (188m 43s) 1.2356 iye goyegoye no /  i bi i i i bi  ✗ ye goyegoye now\n",
      "epoch 21 of  75 (188m 43s) 1.2356 a ori na, piki  /  i bi i i i bi  ✗  ori na, piki t\n",
      "epoch 21 of  75 (191m 0s) 1.2119 i na sime owuap /  bi oi i b i  i ✗  na sime owuapu\n",
      "epoch 21 of  75 (191m 0s) 1.2119 ia, min mi gose /  bi oi i b i  i ✗ a, min mi gose \n",
      "epoch 22 of  75 (193m 18s) 1.2165 l be ori na kob / bi b i bi bi i ✗  be ori na kobi\n",
      "epoch 22 of  75 (193m 18s) 1.2165 a bere nwose i  / bi b i bi bi i ✗  bere nwose i s\n",
      "epoch 22 of  75 (195m 47s) 1.2238 a bara mi nwo m /  oi i bi bii bi ✗  bara mi nwo mi\n",
      "epoch 22 of  75 (195m 47s) 1.2238  mgba piriabe,  /  oi i bi bii bi ✗ mgba piriabe, o\n",
      "epoch 22 of  75 (198m 5s) 1.2128 lisias be bo la /  i  ime bi bi  ✗ isias be bo la \n",
      "epoch 22 of  75 (198m 5s) 1.2128 agame ani se ow /  i  ime bi bi  ✗ game ani se owo\n",
      "epoch 23 of  75 (200m 26s) 1.2228  sabamame o bim / bi i i a b oi i ✗ sabamame o bime\n",
      "epoch 23 of  75 (200m 26s) 1.2228 ine mgba deinma / bi i i a b oi i ✗ ne mgba deinma \n",
      "epoch 23 of  75 (203m 8s) 1.2178 no be piribia,  /  bi bi i i   b ✗ o be piribia, m\n",
      "epoch 23 of  75 (203m 8s) 1.2178 ke, aniatii min /  bi bi i i   b ✗ e, aniatii mina\n",
      "epoch 23 of  75 (205m 52s) 1.2108  piki ineda obo / bi i b i a b i  ✗ piki ineda obok\n",
      "epoch 23 of  75 (205m 52s) 1.2108 buno iniatoruku / bi i b i a b i  ✗ uno iniatorukub\n",
      "epoch 24 of  75 (209m 0s) 1.2021 , ani gbori sik /  b i bii i bi i ✗  ani gbori siki\n",
      "epoch 24 of  75 (209m 0s) 1.2021 u rom bie bome, /  b i bii i bi i ✗  rom bie bome, \n",
      "epoch 24 of  75 (211m 49s) 1.2007  ania bu weri s / b i  be bi i bi ✗ ania bu weri si\n",
      "epoch 24 of  75 (211m 49s) 1.2007  ogono bakuba b / b i  be bi i bi ✗ ogono bakuba bo\n",
      "epoch 24 of  75 (214m 35s) 1.1766 omaye bu  jon b /  i a be bbi ibe ✗ maye bu  jon be\n",
      "epoch 24 of  75 (214m 35s) 1.1766 t be duko pele  /  i a be bbi ibe ✗  be duko pele n\n",
      "epoch 25 of  75 (217m 45s) 1.2094 m mi bie anga j / ima be  b i  bi ✗  mi bie anga ju\n",
      "epoch 25 of  75 (217m 45s) 1.2094 o  berechiri mi / ima be  b i  bi ✗   berechiri mi \n",
      "epoch 25 of  75 (220m 38s) 1.1734 oma bere nwo du /  i be i bii bi  ✗ ma bere nwo duk\n",
      "epoch 25 of  75 (220m 38s) 1.1734 i nwo boroma ok /  i be i bii bi  ✗  nwo boroma oku\n",
      "epoch 25 of  75 (223m 25s) 1.1992 oputekewari mi  /  i   i a i bi b ✗ putekewari mi b\n",
      "epoch 25 of  75 (223m 25s) 1.1992 bo be ere o se  /  i   i a i bi b ✗ o be ere o se y\n",
      "epoch 26 of  75 (226m 14s) 1.1992 ni pekereme, om /   bi i i a  b i ✗ i pekereme, omi\n",
      "epoch 26 of  75 (226m 14s) 1.1992 kuma ngeriminab /   bi i i a  b i ✗ uma ngeriminabu\n",
      "epoch 26 of  75 (228m 56s) 1.1855 ema nwose inimg /  i bii i b i i  ✗ ma nwose inimgb\n",
      "epoch 26 of  75 (228m 56s) 1.1855 se ini mie se i /  i bii i b i i  ✗ e ini mie se in\n",
      "epoch 26 of  75 (231m 50s) 1.1843 ook mi angasiki /   imi b i  i i  ✗ ok mi angasiki \n",
      "epoch 26 of  75 (231m 50s) 1.1843 ni se chua josw /   imi b i  i i  ✗ i se chua joswa\n",
      "epoch 27 of  75 (234m 43s) 1.1764 kuma ye mi now  / i a be bi bi ob ✗ uma ye mi now b\n",
      "epoch 27 of  75 (234m 43s) 1.1764 ni fiafia teme  / i a be bi bi ob ✗ i fiafia teme b\n",
      "epoch 27 of  75 (237m 48s) 1.1740 apu ma o lekiri /  i bi b oi i i  ✗ pu ma o lekirim\n",
      "epoch 27 of  75 (237m 48s) 1.1740 o gele agbamieb /  i bi b oi i i  ✗  gele agbamiebo\n",
      "epoch 27 of  75 (240m 49s) 1.1881  piri yee  ani  / bi i be  bnni b ✗ piri yee  ani t\n",
      "epoch 27 of  75 (240m 49s) 1.1881 a se nyanabo be / bi i be  bnni b ✗  se nyanabo be \n",
      "epoch 27 of  75 (243m 44s) 1.1797 beme,  uri ani / e a  ba a b i  ✗ eme,  uri ani \n",
      "epoch 27 of  75 (243m 44s) 1.1797 , nyanabo jin s / e a  ba a b i  ✗  nyanabo jin si\n",
      "epoch 28 of  75 (246m 40s) 1.1718 mgbolu mgba se  / i e a bi e be b ✗ gbolu mgba se b\n",
      "epoch 28 of  75 (246m 40s) 1.1718 iki  o piki ton / i e a bi e be b ✗ ki  o piki ton \n",
      "epoch 28 of  75 (249m 36s) 1.1798 ara  okuma gbor /  i bbri i bii i ✗ ra  okuma gbori\n",
      "epoch 28 of  75 (249m 36s) 1.1798 ona tomoni ini  /  i bbri i bii i ✗ na tomoni ini n\n",
      "epoch 28 of  75 (252m 49s) 1.1655 ie mama apu nwo /   bi a b i bii  ✗ e mama apu nwo \n",
      "epoch 28 of  75 (252m 49s) 1.1655 ria bie kele si /   bi a b i bii  ✗ ia bie kele sim\n",
      "epoch 29 of  75 (255m 58s) 1.1681 omoni mamgba ny /  i a bi a e bie ✗ moni mamgba nya\n",
      "epoch 29 of  75 (255m 58s) 1.1681 o nwo famame, s /  i a bi a e bie ✗  nwo famame, se\n",
      "epoch 29 of  75 (258m 50s) 1.1674 buo ma gose ani / e  bi bi i b i  ✗ uo ma gose ani \n",
      "epoch 29 of  75 (258m 50s) 1.1674  tatari tamuno  / e  bi bi i b i  ✗ tatari tamuno o\n",
      "epoch 29 of  75 (261m 55s) 1.1590  nwose a beme   / bii i b ie i bb ✗ nwose a beme  o\n",
      "epoch 29 of  75 (261m 55s) 1.1590 a obudukoapu nw / bii i b ie i bb ✗  obudukoapu nwo\n",
      "epoch 30 of  75 (264m 46s) 1.1662 kewari mi bie s / i e i bi be  be ✗ ewari mi bie sm\n",
      "epoch 30 of  75 (264m 46s) 1.1662  piri eke  anin / i e i bi be  be ✗ piri eke  anina\n",
      "epoch 30 of  75 (267m 36s) 1.1810 , na boari inyo /  bi bi  i b ie  ✗  na boari inyos\n",
      "epoch 30 of  75 (267m 36s) 1.1810  ma dukoari ye  /  bi bi  i b ie  ✗ ma dukoari ye m\n",
      "epoch 30 of  75 (270m 28s) 1.1514 boro o sobie en / e i b oi i  b i ✗ oro o sobie ene\n",
      "epoch 30 of  75 (270m 28s) 1.1514 a nwo miese a d / e i b oi i  b i ✗  nwo miese a du\n",
      "epoch 31 of  75 (273m 24s) 1.1547 jinseapu ma nwo / i ia  i ba bii  ✗ inseapu ma nwos\n",
      "epoch 31 of  75 (273m 24s) 1.1547 ame  anisiki in / i ia  i ba bii  ✗ me  anisiki ini\n",
      "epoch 31 of  75 (276m 19s) 1.1504 me, se o chochi / i  be b bii ia  ✗ e, se o chochi \n",
      "epoch 31 of  75 (276m 19s) 1.1504 juapu ma na gri / i  be b bii ia  ✗ uapu ma na grik\n",
      "epoch 31 of  75 (279m 12s) 1.1582 be bomabe, isia / e be i e  b i   ✗ e bomabe, isia \n",
      "epoch 31 of  75 (279m 12s) 1.1582 i ini gbelame,  / e be i e  b i   ✗  ini gbelame, f\n",
      "epoch 32 of  75 (281m 54s) 1.1343 o mume  aninyo  /  bi a bbni ia b ✗  mume  aninyo s\n",
      "epoch 32 of  75 (281m 54s) 1.1343 ki olokume, fi /  bi a bbni ia b ✗ i olokume, fis\n",
      "epoch 32 of  75 (284m 32s) 1.1347  owuapu obu som / b i  i b i be i ✗ owuapu obu some\n",
      "epoch 32 of  75 (284m 32s) 1.1347 inbia nyo mi  o / b i  i b i be i ✗ nbia nyo mi  o \n",
      "epoch 32 of  75 (287m 9s) 1.1524 wila be na o du / i i be bi b bi  ✗ ila be na o duk\n",
      "epoch 32 of  75 (287m 9s) 1.1524 aninyo da berre / i i be bi b bi  ✗ ninyo da berren\n",
      "epoch 32 of  75 (289m 43s) 1.1542 na apu ma, se n / i b i bi  be oi ✗ a apu ma, se nw\n",
      "epoch 32 of  75 (289m 43s) 1.1542 ekewari mi na t / i b i bi  be oi ✗ kewari mi na tu\n",
      "epoch 33 of  75 (292m 24s) 1.1691 kobirima sime o / i e i i bi i b  ✗ obirima sime og\n",
      "epoch 33 of  75 (292m 24s) 1.1691 biari ye i nwo  / i e i i bi i b  ✗ iari ye i nwo n\n",
      "epoch 33 of  75 (295m 0s) 1.1486 ime  pol be ori /  i bbi ibe bni  ✗ me  pol be ori \n",
      "epoch 33 of  75 (295m 0s) 1.1486 be o balafame n /  i bbi ibe bni  ✗ e o balafame nw\n",
      "epoch 33 of  75 (297m 57s) 1.1322 sa fiye goyegoy / e bi e bi e a e ✗ a fiye goyegoye\n",
      "epoch 33 of  75 (297m 57s) 1.1322 pu na ori na, p / e bi e bi e a e ✗ u na ori na, pi\n",
      "epoch 34 of  75 (300m 41s) 1.1273   iri na sime o / bnni bi bi i b  ✗  iri na sime ow\n",
      "epoch 34 of  75 (300m 41s) 1.1273 iribia, min mi  / bnni bi bi i b  ✗ ribia, min mi g\n",
      "epoch 34 of  75 (303m 35s) 1.1371   pol be ori na / bni ibe b i bi  ✗  pol be ori na \n",
      "epoch 34 of  75 (303m 35s) 1.1371  oria bere nwos / bni ibe b i bi  ✗ oria bere nwose\n",
      "epoch 34 of  75 (306m 29s) 1.1408 irisa bara mi n /  i i be i bi bi ✗ risa bara mi nw\n",
      "epoch 34 of  75 (306m 29s) 1.1408 weni mgba piria /  i i be i bi bi ✗ eni mgba piriab\n",
      "epoch 35 of  75 (309m 36s) 1.1302 bo, lisias be b / e  be i  ime be ✗ o, lisias be bo\n",
      "epoch 35 of  75 (309m 36s) 1.1302 se gagame ani s / e  be i  ime be ✗ e gagame ani se\n",
      "epoch 35 of  75 (313m 1s) 1.1280 e mi sabamame o /  bi bi e a a b  ✗  mi sabamame o \n",
      "epoch 35 of  75 (313m 1s) 1.1280 bu mine mgba de /  bi bi e a a b  ✗ u mine mgba dei\n",
      "epoch 35 of  75 (316m 42s) 1.1208 tamuno be pirib / i i   be bi i e ✗ amuno be piribi\n",
      "epoch 35 of  75 (316m 42s) 1.1208 ri eke, aniatii / i i   be bi i e ✗ i eke, aniatii \n",
      "epoch 36 of  75 (320m 19s) 1.1105  ini piki ineda / bni bi i bni a  ✗ ini piki ineda \n",
      "epoch 36 of  75 (320m 19s) 1.1105 mabo taitos be, / bni bi i bni a  ✗ abo taitos be, \n",
      "epoch 36 of  75 (323m 31s) 1.1248  yee, ani gbori / be   b i bii i  ✗ yee, ani gbori \n",
      "epoch 36 of  75 (323m 31s) 1.1248 ema o pirime. m / be   b i bii i  ✗ ma o pirime. mi\n",
      "epoch 36 of  75 (326m 40s) 1.1277 a ma ania bu we /  bi bni  be ba  ✗  ma ania bu wer\n",
      "epoch 36 of  75 (326m 40s) 1.1277 e bereniapu ma  /  bi bni  be ba  ✗  bereniapu ma k\n",
      "epoch 37 of  75 (329m 46s) 1.1338 a siria na sali /  be i  bi bi i  ✗  siria na salis\n",
      "epoch 37 of  75 (329m 46s) 1.1338 a me na sera ma /  be i  bi bi i  ✗  me na sera ma \n",
      "epoch 37 of  75 (332m 10s) 1.1076 gbolomaye bu, a / ie a i e be  b  ✗ bolomaye bu, an\n",
      "epoch 37 of  75 (332m 10s) 1.1076 a dumo ma now d / ie a i e be  b  ✗  dumo ma now da\n",
      "epoch 37 of  75 (334m 28s) 1.1254 obia bo be bobi /  i  be be be e  ✗ bia bo be bobia\n",
      "epoch 37 of  75 (334m 28s) 1.1254  kraist be diki /  i  be be be e  ✗ kraist be dikib\n",
      "epoch 38 of  75 (336m 44s) 1.1188 wo duko o piriy / i bi o b bi i e ✗ o duko o piriye\n",
      "epoch 38 of  75 (336m 44s) 1.1188 me o kokoma oku / i bi o b bi i e ✗ e o kokoma oku \n",
      "epoch 38 of  75 (339m 5s) 1.1245  be kraist mgbe / be bii  i bi e  ✗ be kraist mgbes\n",
      "epoch 38 of  75 (339m 5s) 1.1245 na gboriye mie  / be bii  i bi e  ✗ a gboriye mie b\n",
      "epoch 38 of  75 (341m 23s) 1.1144  na ini min mie / bi bni ba iba   ✗ na ini min mies\n",
      "epoch 38 of  75 (341m 23s) 1.1144 ri ominea kubie / bi bni ba iba   ✗ i ominea kubie \n",
      "epoch 38 of  75 (343m 41s) 1.1066 oku diki. ani o /  i bi i  b i b  ✗ ku diki. ani o \n",
      "epoch 38 of  75 (343m 41s) 1.1066 uma konkon dumo /  i bi i  b i b  ✗ ma konkon dumo \n",
      "epoch 39 of  75 (345m 59s) 1.1189  se kraist jizo / be oii  i bi i  ✗ se kraist jizos\n",
      "epoch 39 of  75 (345m 59s) 1.1189 uno dabo be be. / be oii  i bi i  ✗ no dabo be be. \n",
      "epoch 39 of  75 (348m 17s) 1.1118 anaka yeton mi  /  i i be e iba b ✗ naka yeton mi w\n",
      "epoch 39 of  75 (348m 17s) 1.1118 mi be deinma mi /  i i be e iba b ✗ i be deinma mi \n",
      "epoch 39 of  75 (350m 37s) 1.0977 eni se belema b /  i be oi e e be ✗ ni se belema bu\n",
      "epoch 39 of  75 (350m 37s) 1.0977 bia erechi ari  /  i be oi e e be ✗ ia erechi ari o\n",
      "epoch 40 of  75 (352m 54s) 1.1119 bo, piki sise f / e  bi i be i oi ✗ o, piki sise fi\n",
      "epoch 40 of  75 (352m 54s) 1.1119 o pakabia ye of / e  bi i be i oi ✗  pakabia ye ofo\n",
      "epoch 40 of  75 (355m 12s) 1.0960 u o weri ani ba /  b bi i b i be  ✗  o weri ani bar\n",
      "epoch 40 of  75 (355m 12s) 1.0960  aniokuma nde o /  b bi i b i be  ✗ aniokuma nde ol\n",
      "epoch 40 of  75 (357m 31s) 1.0987 mabia erechi. i / i e  b i  i  bn ✗ abia erechi. ir\n",
      "epoch 40 of  75 (357m 31s) 1.0987 risam omine ine / i e  b i  i  bn ✗ isam omine ined\n",
      "epoch 41 of  75 (359m 50s) 1.1007 ki, a nwose nwe / i  bniio i oio  ✗ i, a nwose nwen\n",
      "epoch 41 of  75 (359m 50s) 1.1007 maau ani mi nem / i  bniio i oio  ✗ aau ani mi nemi\n",
      "epoch 41 of  75 (362m 24s) 1.0881 e bu bipiogboso /  be be i  oe e  ✗  bu bipiogboso \n",
      "epoch 41 of  75 (362m 24s) 1.0881  bara, na ini m /  be be i  oe e  ✗ bara, na ini mi\n",
      "epoch 41 of  75 (365m 13s) 1.0841 oni nwo sobia y /  i bio bi e  be ✗ ni nwo sobia ye\n",
      "epoch 41 of  75 (365m 13s) 1.0841 ia nemi na ye k /  i bio bi e  be ✗ a nemi na ye ka\n",
      "epoch 42 of  75 (368m 5s) 1.1004  bupele na bupe / be o e bi be o  ✗ bupele na bupel\n",
      "epoch 42 of  75 (368m 5s) 1.1004 akarama koroma  / be o e bi be o  ✗ karama koroma w\n",
      "epoch 42 of  75 (370m 57s) 1.0915 amaso dumo soku /  i e bi a bi i  ✗ maso dumo soku \n",
      "epoch 42 of  75 (370m 57s) 1.0915 ose, ani bele w /  i e bi a bi i  ✗ se, ani bele wa\n",
      "epoch 42 of  75 (373m 48s) 1.0660 ose, so bie bu. /  i  be be  be   ✗ se, so bie bu. \n",
      "epoch 42 of  75 (373m 48s) 1.0660 oroma ma yebuso /  i  be be  be   ✗ roma ma yebusok\n",
      "epoch 43 of  75 (376m 30s) 1.0950  fi simeari ogb / bi bi i  i b ie ✗ fi simeari ogbo\n",
      "epoch 43 of  75 (376m 30s) 1.0950 i warabe. aniok / bi bi i  i b ie ✗  warabe. anioku\n",
      "epoch 43 of  75 (379m 1s) 1.1024 ie bu. iri ider /   be  bni bni i ✗ e bu. iri ideri\n",
      "epoch 43 of  75 (379m 1s) 1.1024 ibe kuro ma nae /   be  bni bni i ✗ be kuro ma nae.\n",
      "epoch 43 of  75 (381m 24s) 1.0848 ye ma ambura ba / e ba bnie i be  ✗ e ma ambura bar\n",
      "epoch 43 of  75 (381m 24s) 1.0848 i oria toku be  / e ba bnie i be  ✗  oria toku be s\n",
      "epoch 43 of  75 (383m 50s) 1.0826 oma nyanabo be  /  i bie i e be o ✗ ma nyanabo be b\n",
      "epoch 43 of  75 (383m 50s) 1.0826  mine simie jin /  i bie i e be o ✗ mine simie jins\n",
      "epoch 44 of  75 (386m 12s) 1.1011 ka furotokuo in / i bi i o i  bni ✗ a furotokuo ini\n",
      "epoch 44 of  75 (386m 12s) 1.1011 amuno be ebraha / i bi i o i  bni ✗ muno be ebraham\n",
      "epoch 44 of  75 (388m 33s) 1.0786 ein mi tomonibo /   iba bi i   e  ✗ in mi tomonibo \n",
      "epoch 44 of  75 (388m 33s) 1.0786 i ibiokwein mi  /   iba bi i   e  ✗  ibiokwein mi d\n",
      "epoch 44 of  75 (390m 56s) 1.0850 inme, se bari j /  ia  be oe i bi ✗ nme, se bari ji\n",
      "epoch 44 of  75 (390m 56s) 1.0850 turu mi nwo su  /  ia  be oe i bi ✗ uru mi nwo su m\n",
      "epoch 45 of  75 (393m 41s) 1.0834  se chin ogono  / be oii ibni i b ✗ se chin ogono s\n",
      "epoch 45 of  75 (393m 41s) 1.0834 mioku, ari i ba / be oii ibni i b ✗ ioku, ari i bar\n",
      "epoch 45 of  75 (396m 29s) 1.0975 iribia  pol be  /  i e  bbi ibe b ✗ ribia  pol be n\n",
      "epoch 45 of  75 (396m 29s) 1.0975 oria owuapuawo  /  i e  bbi ibe b ✗ ria owuapuawo b\n",
      "epoch 45 of  75 (399m 19s) 1.0895 kuroma oloko mi / i i a b i i ba  ✗ uroma oloko mi \n",
      "epoch 45 of  75 (399m 19s) 1.0895 i nwo diepakuma / i i a b i i ba  ✗  nwo diepakumab\n",
      "epoch 46 of  75 (402m 12s) 1.1007 inibo ikuye nwo /  i e bni e bio  ✗ nibo ikuye nwo \n",
      "epoch 46 of  75 (402m 12s) 1.1007 amuno be, ani y /  i e bni e bio  ✗ muno be, ani ye\n",
      "epoch 46 of  75 (405m 2s) 1.0852 na apu ma seima / i bni ba bi  i  ✗ a apu ma seimab\n",
      "epoch 46 of  75 (405m 2s) 1.0852 rikiria pekere  / i bni ba bi  i  ✗ ikiria pekere m\n",
      "epoch 46 of  75 (407m 41s) 1.0836 e mgba wa sime  /  bioe ba bi i b ✗  mgba wa sime o\n",
      "epoch 46 of  75 (407m 41s) 1.0836  siki, min ini  /  bioe ba bi i b ✗ siki, min ini b\n",
      "epoch 47 of  75 (410m 23s) 1.0807 uma mioku tamuo /  a bi  i bimi   ✗ ma mioku tamuo \n",
      "epoch 47 of  75 (410m 23s) 1.0807 mamgba  anibe k /  a bi  i bimi   ✗ amgba  anibe ku\n",
      "epoch 47 of  75 (413m 4s) 1.0804 n mi mieme, tam / iba bi  a  ba i ✗  mi mieme, tamu\n",
      "epoch 47 of  75 (413m 4s) 1.0804     kraist be e / iba bi  a  ba i ✗    kraist be en\n",
      "epoch 47 of  75 (415m 38s) 1.0783 ipipokika apu m /  i i i i b i ba ✗ pipokika apu ma\n",
      "epoch 47 of  75 (415m 38s) 1.0783 rame,     o se  /  i i i i b i ba ✗ ame,     o se b\n",
      "epoch 48 of  75 (418m 16s) 1.0569 iabe nwobe bara /   i bao o oe i  ✗ abe nwobe bara \n",
      "epoch 48 of  75 (418m 16s) 1.0569 umo sime tauno  /   i bao o oe i  ✗ mo sime tauno b\n",
      "epoch 48 of  75 (420m 49s) 1.0744 i kokoma duabor /  bi i i bi  i i ✗  kokoma duaboro\n",
      "epoch 48 of  75 (420m 49s) 1.0744 dikitibifoin na /  bi i i bi  i i ✗ ikitibifoin na \n",
      "epoch 48 of  75 (423m 13s) 1.0659  barana tatari  / be i a bimimi b ✗ barana tatari m\n",
      "epoch 48 of  75 (423m 13s) 1.0659 ri ye goyegoye  / be i a bimimi b ✗ i ye goyegoye o\n",
      "epoch 48 of  75 (425m 55s) 1.0531 ko bara bu, mim / i be i be  ba i ✗ o bara bu, mimg\n",
      "epoch 48 of  75 (425m 55s) 1.0531 u mi dinmasam n / i be i be  ba i ✗  mi dinmasam nw\n",
      "epoch 49 of  75 (428m 38s) 1.0715 i ye me na piri /  be bi ni bi i  ✗  ye me na pirip\n",
      "epoch 49 of  75 (428m 38s) 1.0715  biejuadapu ma  /  be bi ni bi i  ✗ biejuadapu ma n\n",
      "epoch 49 of  75 (431m 31s) 1.0680 i okuma, tamuno /  b i a  ba i a  ✗  okuma, tamuno \n",
      "epoch 49 of  75 (431m 31s) 1.0680 se minea tibi g /  b i a  ba i a  ✗ e minea tibi go\n",
      "epoch 49 of  75 (434m 41s) 1.0691  wa piri bara b / bo biri be i be ✗ wa piri bara bu\n",
      "epoch 49 of  75 (434m 41s) 1.0691 me, oloko mi du / bo biri be i be ✗ e, oloko mi duk\n",
      "epoch 50 of  75 (437m 19s) 1.0523 oku mi-e. ori o /  i ba    b i b  ✗ ku mi-e. ori ob\n",
      "epoch 50 of  75 (437m 19s) 1.0523 e nwo miebia ba /  i ba    b i b  ✗  nwo miebia bar\n",
      "epoch 50 of  75 (439m 48s) 1.0731  nyana borosa i / bie i be i e bn ✗ nyana borosa ib\n",
      "epoch 50 of  75 (439m 48s) 1.0731  piki aizik be  / bie i be i e bn ✗ piki aizik be n\n",
      "epoch 50 of  75 (442m 16s) 1.0750 bu tomoni ma be / e bari i ba bi  ✗ u tomoni ma be \n",
      "epoch 50 of  75 (442m 16s) 1.0750 a o nwose bieko / e bari i ba bi  ✗  o nwose biekor\n",
      "epoch 51 of  75 (444m 41s) 1.0668 mi bu ani omine / a be bni b i i  ✗ i bu ani omine \n",
      "epoch 51 of  75 (444m 41s) 1.0668  ini nwo jike s / a be bni b i i  ✗ ini nwo jike se\n",
      "epoch 51 of  75 (447m 6s) 1.0654 sa se mi be kor / e be oi be oi i ✗ a se mi be koro\n",
      "epoch 51 of  75 (447m 6s) 1.0654 a kerenibipi o  / e be oi be oi i ✗  kerenibipi o n\n",
      "epoch 51 of  75 (449m 34s) 1.0565 ani mine iniabu /  i ba i bni  i  ✗ ni mine iniabu \n",
      "epoch 51 of  75 (449m 34s) 1.0565 , nyanabo be mi /  i ba i bni  i  ✗  nyanabo be mi \n",
      "epoch 52 of  75 (452m 29s) 1.0624 a apu ma. nweni /  bni ba  bao i  ✗  apu ma. nweni \n",
      "epoch 52 of  75 (452m 29s) 1.0624  se dukopakuma  /  bni ba  bao i  ✗ se dukopakuma a\n",
      "epoch 52 of  75 (455m 13s) 1.0680 luma teke    ta / i a bi i obbnin ✗ uma teke    tam\n",
      "epoch 52 of  75 (455m 13s) 1.0680 e mimgba. tona  / i a bi i obbnin ✗  mimgba. tona k\n",
      "epoch 52 of  75 (457m 55s) 1.0720 o o piribia  or /  bnbiri e  bbri ✗  o piribia  ori\n",
      "epoch 52 of  75 (457m 55s) 1.0720 ono gbana na, f /  bnbiri e  bbri ✗ no gbana na, fi\n",
      "epoch 53 of  75 (460m 40s) 1.0574 sese inia bu ma / e e o i  be ba  ✗ ese inia bu ma \n",
      "epoch 53 of  75 (460m 40s) 1.0574 omine orisam, m / e e o i  be ba  ✗ mine orisam, mi\n",
      "epoch 53 of  75 (463m 17s) 1.0436 uko bara mi, an /  o be i bi  oni ✗ ko bara mi, ani\n",
      "epoch 53 of  75 (463m 17s) 1.0436 ieye bu karagba /  o be i bi  oni ✗ eye bu karagbas\n",
      "epoch 53 of  75 (466m 0s) 1.0658 o belema o piri /  be e e b biri  ✗  belema o pirib\n",
      "epoch 53 of  75 (466m 0s) 1.0658 i, ateli oforie /  be e e b biri  ✗ , ateli oforie,\n",
      "epoch 54 of  75 (468m 39s) 1.0302 anyana boe, pik /  ia a be   oiri ✗ nyana boe, piki\n",
      "epoch 54 of  75 (468m 39s) 1.0302 eke, okuma sime /  ia a be   oiri ✗ ke, okuma sime \n",
      "epoch 54 of  75 (471m 27s) 1.0396 iatibi o dumo g /   a e b bi a bi ✗ atibi o dumo gb\n",
      "epoch 54 of  75 (471m 27s) 1.0396 ame dikitibifoi /   a e b bi a bi ✗ me dikitibifoin\n",
      "epoch 54 of  75 (474m 0s) 1.0691 igbe mi be ogon /  io ba be oni i ✗ gbe mi be ogono\n",
      "epoch 54 of  75 (474m 0s) 1.0691  bara bu ebraha /  io ba be oni i ✗ bara bu ebraham\n",
      "epoch 54 of  75 (476m 30s) 1.0425 i dikida mie ye /  bi i i ba  be  ✗  dikida mie ye \n",
      "epoch 54 of  75 (476m 30s) 1.0425 rusior piki sib /  bi i i ba  be  ✗ usior piki siba\n",
      "epoch 55 of  75 (479m 5s) 1.0463 toruse dikiariy / i i e oi i  i e ✗ oruse dikiariye\n",
      "epoch 55 of  75 (479m 5s) 1.0463 e o gbin bae. o / i i e oi i  i e ✗  o gbin bae. or\n",
      "epoch 55 of  75 (482m 0s) 1.0612  chin siki, o b / bii ibe i  o be ✗ chin siki, o bi\n",
      "epoch 55 of  75 (482m 0s) 1.0612 okuma ini ngiri / bii ibe i  o be ✗ kuma ini ngiri \n",
      "epoch 55 of  75 (484m 28s) 1.0555 raye nwo mieme  / i e bio bi  a b ✗ aye nwo mieme s\n",
      "epoch 55 of  75 (484m 28s) 1.0555 be     toku be  / i e bio bi  a b ✗ e     toku be i\n",
      "epoch 56 of  75 (486m 53s) 1.0351  sefi dumo nwo  / be i bi a bio b ✗ sefi dumo nwo k\n",
      "epoch 56 of  75 (486m 53s) 1.0351 na nwo beme, a  / be i bi a bio b ✗ a nwo beme, a k\n",
      "epoch 56 of  75 (491m 40s) 1.0479 niokuma ikowari / i  u a bni o i  ✗ iokuma ikowari \n",
      "epoch 56 of  75 (491m 40s) 1.0479  miese o bereni / i  u a bni o i  ✗ miese o bereni \n",
      "epoch 56 of  75 (496m 41s) 1.0514 ye nwo mie mi   / e bio bi  bi bb ✗ e nwo mie mi   \n",
      "epoch 56 of  75 (496m 41s) 1.0514 be bipipoki mi  / e bio bi  bi bb ✗ e bipipoki mi d\n",
      "epoch 57 of  75 (501m 41s) 1.0518 ka kala siki bu / i bi a be i be  ✗ a kala siki bu \n",
      "epoch 57 of  75 (501m 41s) 1.0518  buokorome, ani / i bi a be i be  ✗ buokorome, ani \n",
      "epoch 57 of  75 (506m 47s) 1.0445  nemikase tamun / bi e i e oimi a ✗ nemikase tamuno\n",
      "epoch 57 of  75 (506m 47s) 1.0445 ie isi agbamiea / bi e i e oimi a ✗ e isi agbamieap\n",
      "epoch 57 of  75 (511m 59s) 1.0425 e siki, tomoni  /  be i  oari i b ✗  siki, tomoni m\n",
      "epoch 57 of  75 (511m 59s) 1.0425  goligoli agbam /  be i  oari i b ✗ goligoli agbami\n",
      "epoch 58 of  75 (516m 56s) 1.0499 aye ma o bara l /  e ba b be i bi ✗ ye ma o bara la\n",
      "epoch 58 of  75 (516m 56s) 1.0499 gbo sibubarasin /  e ba b be i bi ✗ bo sibubarasin \n",
      "epoch 58 of  75 (522m 4s) 1.0629 obia, melkizede /  o   oa ea i e  ✗ bia, melkizedek\n",
      "epoch 58 of  75 (522m 4s) 1.0629 oro, ani fieari /  o   oa ea i e  ✗ ro, ani fieariy\n",
      "epoch 58 of  75 (527m 12s) 1.0553 a   ini miesa s /  bbnni ba  e ae ✗    ini miesa si\n",
      "epoch 58 of  75 (527m 12s) 1.0553 ne mi nwo naa y /  bbnni ba  e ae ✗ e mi nwo naa ye\n",
      "epoch 59 of  75 (532m 8s) 1.0426 sam bebe o ton  / e abe e b birio ✗ am bebe o ton w\n",
      "epoch 59 of  75 (532m 8s) 1.0426  aniatibi ari i / e abe e b birio ✗ aniatibi ari ik\n",
      "epoch 59 of  75 (537m 14s) 1.0600  la  gbori agba / bi bbio i bnio  ✗ la  gbori agba \n",
      "epoch 59 of  75 (537m 14s) 1.0600 uroma tomonibo  / bi bbio i bnio  ✗ roma tomonibo g\n",
      "epoch 59 of  75 (542m 20s) 1.0444 ritein ken be n / i i  ioirebe ni ✗ itein ken be ne\n",
      "epoch 59 of  75 (542m 20s) 1.0444   juapu gele ag / i i  ioirebe ni ✗  juapu gele agb\n",
      "epoch 59 of  75 (547m 25s) 1.0292 me   bereni mi  / a nbne e a ba b ✗ e   bereni mi m\n",
      "epoch 59 of  75 (547m 25s) 1.0292 i-e bebe, ye o  / a nbne e a ba b ✗ -e bebe, ye o t\n",
      "epoch 60 of  75 (552m 25s) 1.0401 meanaga dabo be / a  i a bi o be  ✗ eanaga dabo be \n",
      "epoch 60 of  75 (552m 25s) 1.0401 iri ye mi. okum / a  i a bi o be  ✗ ri ye mi. okuma\n",
      "epoch 60 of  75 (557m 29s) 1.0240 uro kubie nyana /  i bi o  bie a  ✗ ro kubie nyana \n",
      "epoch 60 of  75 (557m 29s) 1.0240 ibo be ori ined /  i bi o  bie a  ✗ bo be ori ineda\n",
      "epoch 60 of  75 (562m 25s) 1.0503 oma berejine ok /  i be e e i b u ✗ ma berejine okw\n",
      "epoch 60 of  75 (562m 25s) 1.0503 ni min mi-e, ny /  i be e e i b u ✗ i min mi-e, nya\n",
      "epoch 61 of  75 (567m 22s) 1.0329 i bara se nweni /  be i be oio e  ✗  bara se nweni \n",
      "epoch 61 of  75 (567m 22s) 1.0329  ibi bere mi se /  be i be oio e  ✗ ibi bere mi se \n",
      "epoch 61 of  75 (572m 20s) 1.0548 o nwo miebia ok /  bio bi  e  b u ✗  nwo miebia oku\n",
      "epoch 61 of  75 (572m 20s) 1.0548 e ominea olomu- /  bio bi  e  b u ✗  ominea olomu-a\n",
      "epoch 61 of  75 (577m 27s) 1.0382 sima anikanika  / e i b i i a i b ✗ ima anikanika b\n",
      "epoch 61 of  75 (577m 27s) 1.0382 ara nwose dumok / e i b i i a i b ✗ ra nwose dumoko\n",
      "epoch 62 of  75 (582m 28s) 1.0320 ibo vinpiki bom /  i ba ii i be a ✗ bo vinpiki bome\n",
      "epoch 62 of  75 (582m 28s) 1.0320 arakarama inibo /  i ba ii i be a ✗ rakarama inibo \n",
      "epoch 62 of  75 (587m 33s) 1.0442  bu o boro naa  / be b be i bi  b ✗ bu o boro naa s\n",
      "epoch 62 of  75 (587m 33s) 1.0442 rubo-e, anikani / be b be i bi  b ✗ ubo-e, anikanik\n",
      "epoch 62 of  75 (592m 8s) 1.0257 miese oru bere  / a  e o i be e b ✗ iese oru bere n\n",
      "epoch 62 of  75 (592m 8s) 1.0257  now be apu ma  / a  e o i be e b ✗ now be apu ma d\n",
      "epoch 63 of  75 (594m 43s) 1.0382   biebele dumo  / bnee e e bi a b ✗  biebele dumo n\n",
      "epoch 63 of  75 (594m 43s) 1.0382  tamuno be bu b / bnee e e bi a b ✗ tamuno be bu bo\n",
      "epoch 63 of  75 (598m 19s) 1.0451 ni boro mun tam / i be i ba  oami ✗ i boro mun tamu\n",
      "epoch 63 of  75 (598m 19s) 1.0451 kokomaye-e. obi / i be i ba  oami ✗ okomaye-e. obir\n",
      "epoch 63 of  75 (602m 34s) 1.0557 nju bu jinbo-ji / ia be bi io  a  ✗ ju bu jinbo-jin\n",
      "epoch 63 of  75 (602m 34s) 1.0557 monikiri mi na  / ia be bi io  a  ✗ onikiri mi na g\n",
      "epoch 64 of  75 (607m 3s) 1.0216  i bubeleme  mi / bnio o e e nbi  ✗ i bubeleme  min\n",
      "epoch 64 of  75 (607m 3s) 1.0216 e soni, inibo p / bnio o e e nbi  ✗  soni, inibo pa\n",
      "epoch 64 of  75 (612m 22s) 1.0229 ini budekichiem /  i be o e ia  e ✗ ni budekichiema\n",
      "epoch 64 of  75 (612m 22s) 1.0229 bu ofori-e.  an /  i be o e ia  e ✗ u ofori-e.  ani\n",
      "epoch 64 of  75 (617m 44s) 1.0345 e obu nemi, mie /  b u ma e  oa   ✗  obu nemi, mies\n",
      "epoch 64 of  75 (617m 44s) 1.0345 uno be pirime.  /  b u ma e  oa   ✗ no be pirime. t\n",
      "epoch 65 of  75 (623m 4s) 1.0305 e piri belema m /  biri be e e ba ✗  piri belema mi\n",
      "epoch 65 of  75 (623m 4s) 1.0305 dumo mi nyaname /  biri be e e ba ✗ umo mi nyaname \n",
      "epoch 65 of  75 (628m 14s) 1.0192 nemi na karakar / i e bi bi i a i ✗ emi na karakara\n",
      "epoch 65 of  75 (628m 14s) 1.0192 amuno be piki s / i e bi bi i a i ✗ muno be piki se\n",
      "epoch 65 of  75 (633m 17s) 1.0083 os kraist be, o /  ibii  i be  o  ✗ s kraist be, o \n",
      "epoch 65 of  75 (633m 17s) 1.0083 en o seinma a k /  ibii  i be  o  ✗ n o seinma a ko\n",
      "epoch 65 of  75 (638m 22s) 1.0187 abu na onyechie /  u ba bria  i   ✗ bu na onyechie \n",
      "epoch 65 of  75 (638m 22s) 1.0187 aye ma bie wa t /  u ba bria  i   ✗ ye ma bie wa ta\n",
      "epoch 66 of  75 (643m 55s) 1.0440  faye so o piri / bi e be brbiri  ✗ faye so o piri \n",
      "epoch 66 of  75 (643m 55s) 1.0440 me si temema so / bi e be brbiri  ✗ e si temema so \n",
      "epoch 66 of  75 (649m 48s) 1.0189 diri mi kien-ap / i i bi bi  ia i ✗ iri mi kien-apu\n",
      "epoch 66 of  75 (649m 48s) 1.0189 owusoari aye ma / i i bi bi  ia i ✗ wusoari aye ma \n",
      "epoch 66 of  75 (655m 33s) 1.0191 ara labia. oko  /  i bi o   b u b ✗ ra labia. oko s\n",
      "epoch 66 of  75 (655m 33s) 1.0191 ana apu wa nwos /  i bi o   b u b ✗ na apu wa nwose\n",
      "epoch 67 of  75 (661m 21s) 1.0041 ase tomonibo nw /  i oiki   e bao ✗ se tomonibo nwo\n",
      "epoch 67 of  75 (661m 21s) 1.0041 ki se ngiri gbe /  i oiki   e bao ✗ i se ngiri gbei\n",
      "epoch 67 of  75 (663m 52s) 1.0091 a tuburu ma. mi /  bimo i ba  aa  ✗  tuburu ma. min\n",
      "epoch 67 of  75 (663m 52s) 1.0091 yo mi. aniatibi /  bimo i ba  aa  ✗ o mi. aniatibi \n",
      "epoch 67 of  75 (666m 26s) 1.0388 e piki o basam, /  biri b be e a  ✗  piki o basam, \n",
      "epoch 67 of  75 (666m 26s) 1.0388 u sime ye nwo s /  biri b be e a  ✗  sime ye nwo so\n",
      "epoch 68 of  75 (669m 26s) 1.0303 dikibugerere na / i i e o i e bi  ✗ ikibugerere na \n",
      "epoch 68 of  75 (669m 26s) 1.0303  buabalaka piki / i i e o i e bi  ✗ buabalaka piki \n",
      "epoch 68 of  75 (673m 47s) 1.0173 na piki fiafia  / i biri bi  i  b ✗ a piki fiafia s\n",
      "epoch 68 of  75 (673m 47s) 1.0173  dikidikiariyee / i biri bi  i  b ✗ dikidikiariyee.\n",
      "epoch 68 of  75 (679m 7s) 1.0191 e wa fi si piri /  bo bikbe iiri  ✗  wa fi si piri \n",
      "epoch 68 of  75 (679m 7s) 1.0191 e wa nyanabo pi /  bo bikbe iiri  ✗  wa nyanabo pik\n",
      "epoch 69 of  75 (683m 31s) 1.0257 napu ma na piki / i i ba bi biri  ✗ apu ma na piki \n",
      "epoch 69 of  75 (683m 31s) 1.0257  ba oku ini nwo / i i ba bi biri  ✗ ba oku ini nwos\n",
      "epoch 69 of  75 (686m 58s) 1.0259 firinwengi gbei / i i io eonbio e ✗ irinwengi gbein\n",
      "epoch 69 of  75 (686m 58s) 1.0259 ia. okuma, tamu / i i io eonbio e ✗ a. okuma, tamun\n",
      "epoch 69 of  75 (689m 37s) 1.0191 ea ikirima nyan /   bni i a bie a ✗ a ikirima nyana\n",
      "epoch 69 of  75 (689m 37s) 1.0191 wa piri piki mi /   bni i a bie a ✗ a piri piki min\n",
      "epoch 70 of  75 (692m 6s) 1.0287 bie tamuno nemi / e  bimi a bi e  ✗ ie tamuno nemia\n",
      "epoch 70 of  75 (692m 6s) 1.0287 wose biekoroma. / e  bimi a bi e  ✗ ose biekoroma. \n",
      "epoch 70 of  75 (694m 36s) 1.0242 teke, o bereni  / ome  o be e a b ✗ eke, o bereni y\n",
      "epoch 70 of  75 (694m 36s) 1.0242 ko, piki o bube / ome  o be e a b ✗ o, piki o bubel\n",
      "epoch 70 of  75 (697m 3s) 1.0321 e ani enekubu m /  bni b i i o ba ✗  ani enekubu mi\n",
      "epoch 70 of  75 (697m 3s) 1.0321 a dabo be belem /  bni b i i o ba ✗  dabo be belema\n",
      "epoch 70 of  75 (699m 39s) 0.9899  yee, ini mine  / be   oni bi i b ✗ yee, ini mine n\n",
      "epoch 70 of  75 (699m 39s) 0.9899 mi, iwo oloko b / be   oni bi i b ✗ i, iwo oloko ba\n",
      "epoch 71 of  75 (702m 4s) 1.0146 rachua inibo pi / i ai  bni i bir ✗ achua inibo pir\n",
      "epoch 71 of  75 (702m 4s) 1.0146 iabia, anisiki  / i ai  bni i bir ✗ abia, anisiki a\n",
      "epoch 71 of  75 (704m 30s) 1.0324 ri balafa so be / i be a a be be  ✗ i balafa so bel\n",
      "epoch 71 of  75 (704m 30s) 1.0324 ngwe boroso oku / i be a a be be  ✗ gwe boroso oku,\n",
      "epoch 71 of  75 (706m 52s) 1.0193 rijine  kuluma  / i i   bbi o a b ✗ ijine  kuluma o\n",
      "epoch 71 of  75 (706m 52s) 1.0193 iye nwo kan bo  / i i   bbi o a b ✗ ye nwo kan bo g\n",
      "epoch 72 of  75 (709m 15s) 1.0198 a, piki minea b /   oiri ba i  be ✗ , piki minea bu\n",
      "epoch 72 of  75 (709m 15s) 1.0198 ngi na ala meng /   oiri ba i  be ✗ gi na ala mengi\n",
      "epoch 72 of  75 (711m 36s) 1.0061  siki  aniatibi / be i bbni  ine  ✗ siki  aniatibi \n",
      "epoch 72 of  75 (711m 36s) 1.0061  maa kuluma ene / be i bbni  ine  ✗ maa kuluma ene \n",
      "epoch 72 of  75 (714m 4s) 1.0199  nwo sosam  o p / bio be e abnrbi ✗ nwo sosam  o po\n",
      "epoch 72 of  75 (714m 4s) 1.0199 a     kraist du / bio be e abnrbi ✗      kraist duk\n",
      "epoch 73 of  75 (716m 28s) 1.0215 bie ani kiri ny / e  bni bi i bie ✗ ie ani kiri nya\n",
      "epoch 73 of  75 (716m 28s) 1.0215 bojinbo belema  / e  bni bi i bie ✗ ojinbo belema  \n",
      "epoch 73 of  75 (718m 51s) 1.0466 obuto simeoku a /  o ombe i  u b  ✗ buto simeoku an\n",
      "epoch 73 of  75 (718m 51s) 1.0466 ist be na minab /  o ombe i  u b  ✗ st be na minabu\n",
      "epoch 73 of  75 (721m 15s) 1.0123 mieme jizos kra / a  a ni o  bii  ✗ ieme jizos krai\n",
      "epoch 73 of  75 (721m 15s) 1.0123 yana koru siki  / a  a ni o  bii  ✗ ana koru siki i\n",
      "epoch 74 of  75 (723m 36s) 1.0066 mua dapu ma bar / a  bi i ba bi i ✗ ua dapu ma bara\n",
      "epoch 74 of  75 (723m 36s) 1.0066 u tetema se omi / a  bi i ba bi i ✗  tetema se omin\n",
      "epoch 74 of  75 (725m 58s) 1.0163 , tamuno be dik /  oomi   be niki ✗  tamuno be diki\n",
      "epoch 74 of  75 (725m 58s) 1.0163 i wa piri opu y /  oomi   be niki ✗  wa piri opu ye\n",
      "epoch 74 of  75 (728m 30s) 0.9925  min mi mie  om / ba ioa na  bbri ✗ min mi mie  omi\n",
      "epoch 74 of  75 (728m 30s) 0.9925 o be na, jin so / ba ioa na  bbri ✗  be na, jin son\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 250\n",
    "plot_every = 100\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "iter=0\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=nb_epoch):\n",
    "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    category =  [lin2txt(l) for l in y_]\n",
    "    lines = [lin2txt(l) for l in x]\n",
    "    category_tensor=mb2t(y_)\n",
    "    line_tensor=mb2t(x)\n",
    "    output, loss = train(torch.tensor(y_,dtype=torch.long), line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess = [lin2txt([ch.argmax(dim=0) for ch in line]) for line in output]\n",
    "        for i in range(2):\n",
    "            correct = '✓' if guess[i] == category[i] else '✗ %s' % category[i] \n",
    "            print('epoch %d of  %d (%s) %.4f %s / %s %s' % (epoch, nb_epoch, timeSince(start), loss, lines[i], guess[0], correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 30)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x=[mb2t(ch) for ch in a[0]]\n",
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is=torch.Size([1, 98]), hs=torch.Size([1, 128])\n",
      "torch.Size([1, 98])\n"
     ]
    }
   ],
   "source": [
    "#input = lineToTensor('Albert')\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "output, next_hidden = rnn(a[0][0], hidden)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# training loop\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10):\n",
    "\n",
    "    # train on one minibatch\n",
    "    # feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n",
    "    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n",
    "\n",
    "    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n",
    "    if step % _50_BATCHES == 0:\n",
    "        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n",
    "        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n",
    "        summary_writer.add_summary(smm, step)\n",
    "\n",
    "    # run a validation step every 50 batches\n",
    "    # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n",
    "    # so we cut it up and batch the pieces (slightly inaccurate)\n",
    "    # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n",
    "    if step % _50_BATCHES == 0 and len(valitext) > 0:\n",
    "        VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n",
    "        bsize = len(valitext) // VALI_SEQLEN\n",
    "        txt.print_validation_header(len(codetext), bookranges)\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n",
    "        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n",
    "                     batchsize: bsize}\n",
    "        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_validation_stats(ls, acc)\n",
    "        # save validation data for Tensorboard\n",
    "        validation_writer.add_summary(smm, step)\n",
    "\n",
    "    # display a short text generated with the current weights and biases (every 150 batches)\n",
    "    if step // 3 % _50_BATCHES == 0:\n",
    "        txt.print_text_generation_header()\n",
    "        ry = np.array([[txt.convert_from_alphabet(ord(\"K\"))]])\n",
    "        rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n",
    "        for k in range(1000):\n",
    "            ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n",
    "            rc = txt.sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n",
    "            print(chr(txt.convert_to_alphabet(rc)), end=\"\")\n",
    "            ry = np.array([[rc]])\n",
    "        txt.print_text_generation_footer()\n",
    "\n",
    "    # save a checkpoint (every 500 batches)\n",
    "    if step // 10 % _50_BATCHES == 0:\n",
    "        saved_file = saver.save(sess, 'checkpoints/rnn_train_' + timestamp, global_step=step)\n",
    "        print(\"Saved file: \" + saved_file)\n",
    "\n",
    "    # display progress bar\n",
    "    progress.step(reset=step % _50_BATCHES == 0)\n",
    "\n",
    "    # loop state around\n",
    "    istate = ostate\n",
    "    step += BATCHSIZE * SEQLEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
