{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.contrib import layers\n",
    "#from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import my_txtutils as txt\n",
    "#tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model parameters\n",
    "#\n",
    "# Usage:\n",
    "#   Training only:\n",
    "#         Leave all the parameters as they are\n",
    "#         Disable validation to run a bit faster (set validation=False below)\n",
    "#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n",
    "#   Training and experimentation (default):\n",
    "#         Keep validation enabled\n",
    "#         You can now play with the parameters anf follow the effects in Tensorboard\n",
    "#         A good choice of parameters ensures that the testing and validation curves stay close\n",
    "#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n",
    "#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n",
    "#\n",
    "nb_epoch=60\n",
    "SEQLEN = 30\n",
    "BATCHSIZE = 200\n",
    "VALI_SEQLEN = SEQLEN\n",
    "ALPHASIZE = txt.ALPHASIZE\n",
    "INTERNALSIZE = 512\n",
    "NLAYERS = 3\n",
    "learning_rate = 0.001  # fixed learning rate\n",
    "dropout_pkeep = 0.8    # some dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# size of the alphabet that we work with\n",
    "ALPHASIZE = 98\n",
    "\n",
    "\n",
    "# Specification of the supported alphabet (subset of ASCII-7)\n",
    "# 10 line feed LF\n",
    "# 32-64 numbers and punctuation\n",
    "# 65-90 upper-case letters\n",
    "# 91-97 more punctuation\n",
    "# 97-122 lower-case letters\n",
    "# 123-126 more punctuation\n",
    "def convert_from_alphabet(a):\n",
    "    \"\"\"Encode a character\n",
    "    :param a: one character\n",
    "    :return: the encoded value\n",
    "    \"\"\"\n",
    "    if a == 9:\n",
    "        return 1\n",
    "    if a == 10:\n",
    "        return 127 - 30  # LF\n",
    "    elif 32 <= a <= 126:\n",
    "        return a - 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "# encoded values:\n",
    "# unknown = 0\n",
    "# tab = 1\n",
    "# space = 2\n",
    "# all chars from 32 to 126 = c-30\n",
    "# LF mapped to 127-30\n",
    "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode a code point\n",
    "    :param c: code point\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return: decoded character\n",
    "    \"\"\"\n",
    "    if c == 1:\n",
    "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
    "    if c == 127 - 30:\n",
    "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
    "    if 32 <= c + 30 <= 126:\n",
    "        return c + 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "def encode_text(s):\n",
    "    \"\"\"Encode a string.\n",
    "    :param s: a text string\n",
    "    :return: encoded list of code points\n",
    "    \"\"\"\n",
    "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
    "\n",
    "\n",
    "def decode_to_text(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode an encoded string.\n",
    "    :param c: encoded list of code points\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
    "\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(probabilities)\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
    "\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "\n",
    "\n",
    "def find_book(index, bookranges):\n",
    "    return next(\n",
    "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def find_book_index(index, bookranges):\n",
    "    return next(\n",
    "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
    "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
    "    print()\n",
    "    # epoch_size in number of batches\n",
    "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
    "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
    "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "    for k in range(batch_size):\n",
    "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
    "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
    "        bookname = find_book(index_in_epoch, bookranges)\n",
    "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
    "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
    "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
    "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
    "        print(print_string.format(decx, decy, loss_string))\n",
    "        index += sequence_len\n",
    "    # box formatting characters:\n",
    "    # │ \\u2502\n",
    "    # ─ \\u2500\n",
    "    # └ \\u2514\n",
    "    # ┘ \\u2518\n",
    "    # ┴ \\u2534\n",
    "    # ┌ \\u250C\n",
    "    # ┐ \\u2510\n",
    "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
    "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
    "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
    "    print(footer)\n",
    "    # print statistics\n",
    "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
    "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
    "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
    "    print()\n",
    "    print(\"TRAINING STATS: {}\".format(stats))\n",
    "\n",
    "\n",
    "class Progress:\n",
    "    \"\"\"Text mode progress bar.\n",
    "    Usage:\n",
    "            p = Progress(30)\n",
    "            p.step()\n",
    "            p.step()\n",
    "            p.step(start=True) # to restart form 0%\n",
    "    The progress bar displays a new header at each restart.\"\"\"\n",
    "    def __init__(self, maxi, size=100, msg=\"\"):\n",
    "        \"\"\"\n",
    "        :param maxi: the number of steps required to reach 100%\n",
    "        :param size: the number of characters taken on the screen by the progress bar\n",
    "        :param msg: the message displayed in the header of the progress bat\n",
    "        \"\"\"\n",
    "        self.maxi = maxi\n",
    "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
    "        self.header_printed = False\n",
    "        self.msg = msg\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, reset=False):\n",
    "        if reset:\n",
    "            self.__init__(self.maxi, self.size, self.msg)\n",
    "        if not self.header_printed:\n",
    "            self.__print_header()\n",
    "        next(self.p)\n",
    "\n",
    "    def __print_header(self):\n",
    "        print()\n",
    "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
    "        print(format_string.format(self.msg))\n",
    "        self.header_printed = True\n",
    "\n",
    "    def __start_progress(self, maxi):\n",
    "        def print_progress():\n",
    "            # Bresenham's algorithm. Yields the number of dots printed.\n",
    "            # This will always print 100 dots in max invocations.\n",
    "            dx = maxi\n",
    "            dy = self.size\n",
    "            d = dy - dx\n",
    "            for x in range(maxi):\n",
    "                k = 0\n",
    "                while d >= 0:\n",
    "                    print('=', end=\"\", flush=True)\n",
    "                    k += 1\n",
    "                    d -= dx\n",
    "                d += dy\n",
    "                yield k\n",
    "\n",
    "        return print_progress\n",
    "\n",
    "\n",
    "def read_data_files(directory, validation=True):\n",
    "    \"\"\"Read data files according to the specified glob pattern\n",
    "    Optionnaly set aside the last file as validation data.\n",
    "    No validation data is returned if there are 5 files or less.\n",
    "    :param directory: for example \"data/*.txt\"\n",
    "    :param validation: if True (default), sets the last file aside as validation data\n",
    "    :return: training data, validation data, list of loaded file names with ranges\n",
    "     If validation is\n",
    "    \"\"\"\n",
    "    codetext = []\n",
    "    bookranges = []\n",
    "    shakelist = glob.glob(directory, recursive=True)\n",
    "    for shakefile in shakelist:\n",
    "        shaketext = open(shakefile, \"r\")\n",
    "        print(\"Loading file \" + shakefile)\n",
    "        start = len(codetext)\n",
    "        codetext.extend(encode_text(shaketext.read()))\n",
    "        end = len(codetext)\n",
    "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
    "        shaketext.close()\n",
    "\n",
    "    if len(bookranges) == 0:\n",
    "        sys.exit(\"No training data has been found. Aborting.\")\n",
    "\n",
    "    # For validation, use roughly 90K of text,\n",
    "    # but no more than 10% of the entire text\n",
    "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "    # 10% of the text is how many files ?\n",
    "    total_len = len(codetext)\n",
    "    validation_len = 0\n",
    "    nb_books1 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books1 += 1\n",
    "        if validation_len > total_len // 10:\n",
    "            break\n",
    "\n",
    "    # 90K of text is how many books ?\n",
    "    validation_len = 0\n",
    "    nb_books2 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books2 += 1\n",
    "        if validation_len > 90*1024:\n",
    "            break\n",
    "\n",
    "    # 20% of the books is how many books ?\n",
    "    nb_books3 = len(bookranges) // 5\n",
    "\n",
    "    # pick the smallest\n",
    "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
    "\n",
    "    if nb_books == 0 or not validation:\n",
    "        cutoff = len(codetext)\n",
    "    else:\n",
    "        cutoff = bookranges[-nb_books][\"start\"]\n",
    "    valitext = codetext[cutoff:]\n",
    "    codetext = codetext[:cutoff]\n",
    "    return codetext, valitext, bookranges\n",
    "\n",
    "\n",
    "def print_data_stats(datalen, valilen, epoch_size):\n",
    "    datalen_mb = datalen/1024.0/1024.0\n",
    "    valilen_kb = valilen/1024.0\n",
    "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
    "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
    "\n",
    "\n",
    "def print_validation_header(validation_start, bookranges):\n",
    "    bookindex = find_book_index(validation_start, bookranges)\n",
    "    books = ''\n",
    "    for i in range(bookindex, len(bookranges)):\n",
    "        books += bookranges[i][\"name\"]\n",
    "        if i < len(bookranges)-1:\n",
    "            books += \", \"\n",
    "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
    "\n",
    "\n",
    "def print_validation_stats(loss, accuracy):\n",
    "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
    "                                                                                                           accuracy))\n",
    "\n",
    "\n",
    "def print_text_generation_header():\n",
    "    print()\n",
    "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
    "\n",
    "\n",
    "def print_text_generation_footer():\n",
    "    print()\n",
    "    print(\"└{:─^111}┘\".format('End of generation'))\n",
    "\n",
    "\n",
    "def frequency_limiter(n, multiple=1, modulo=0):\n",
    "    def limit(i):\n",
    "        return i % (multiple * n) == modulo*multiple\n",
    "    return limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file txts\\acts_new.txt\n",
      "Loading file txts\\gal_eph_new.txt\n",
      "Loading file txts\\heb_new.txt\n",
      "Loading file txts\\jam_jud_new.txt\n",
      "Loading file txts\\john_new.txt\n",
      "Loading file txts\\jud_rev_new.txt\n",
      "Loading file txts\\luke_8_john_new.txt\n",
      "Loading file txts\\mark01_new.txt\n",
      "Loading file txts\\matt02_new.txt\n",
      "Loading file txts\\matt_new.txt\n",
      "Loading file txts\\phil_col_new.txt\n",
      "Loading file txts\\thes_tim_new.txt\n",
      "Loading file txts\\tit_phl_new.txt\n"
     ]
    }
   ],
   "source": [
    "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
    "shakedir = \"txts/*.txt\"\n",
    "#shakedir = \"../tensorflow/**/*.py\"\n",
    "codetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text size is 2.92MB with 192.15KB set aside for validation. There will be 509 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "# display some stats on the data\n",
    "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
    "txt.print_data_stats(len(codetext), len(valitext), epoch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.GRUCell(input_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input,hidden)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(BATCHSIZE, self.hidden_size)\n",
    "\n",
    "rnn = RNN(ALPHASIZE, INTERNALSIZE, ALPHASIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# loss fn\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#from ok_seq2seq import EncoderRNN \\\n",
    "#                        ,DecoderRNN \\\n",
    "#                        ,AttnDecoderRNN \\\n",
    "#                        ,evaluateRandomly \\\n",
    "#                        ,teacher_forcing_ratio \n",
    "\n",
    "#torch.nn.GRUCell(input_size: int, hidden_size: int, bias: bool = True)\n",
    "# Parameters\n",
    "# input_size – The number of expected features in the input x\n",
    "\n",
    "# hidden_size – The number of features in the hidden state h\n",
    "\n",
    "# bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "\n",
    "# Inputs: input, hidden\n",
    "# input of shape (batch, input_size): tensor containing input features\n",
    "\n",
    "# hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\n",
    "\n",
    "# Outputs: h’\n",
    "# h’ of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training fn\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    lint = []\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "        lint.append(output)\n",
    "    input = torch.stack(lint).transpose(0,1).transpose(1,2)\n",
    "#     print(f'is={input.size()}, cs={category_tensor.size()}')\n",
    "#     print(f'is[1:]={input.size()[1:]}, cs[1:]={category_tensor.size()[1:]}')\n",
    "#     print(f'is[2:]={input.size()[2:]}, cs[2:]={category_tensor.size()[2:]}')\n",
    "    loss = criterion(input, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return torch.stack(lint).transpose(0,1), loss.item()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# for display: init the progress bar\n",
    "DISPLAY_FREQ = 50\n",
    "_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n",
    "progress = txt.Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n",
    "\n",
    "# init\n",
    "istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def one_hot(chcode):\n",
    "    tensor = torch.zeros(1, ALPHASIZE)\n",
    "    tensor[0][chcode] = 1\n",
    "    return tensor\n",
    "\n",
    "def mb2t(rows):\n",
    "    rows=rows.transpose()\n",
    "    tensor = torch.zeros(rows.shape[0], rows.shape[1], ALPHASIZE)\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, letter_code in enumerate(row):\n",
    "            tensor[i][j][letter_code] = 1\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def lin2txt(lt):\n",
    "    return ''.join([chr(txt.convert_to_alphabet(c))  if c != 0 else '' for c in lt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 of 60 (0:00:01.179313) 4.5851  okunwengiapu mieye ma  diepak / gy93\\Q3u9yu83g3y3u3u3u--yy38iX ✗ calculating stats..\n",
      "epoch 0 of 60 (0:00:01.179814) 4.5851 ara mi nwose omine mgba oriari / gy93\\Q3u9yu83g3y3u3u3u--yy38iX ✗ calculating stats..\n",
      "epoch 0 of 60 (0:02:33.111162) 4.3744 isiapu ma bereniapu ma se kpot /                                ✗ calculating stats..\n",
      "epoch 0 of 60 (0:02:33.112164) 4.3744 ow naa kirikiri o biki koro se /                                ✗ calculating stats..\n",
      "epoch 0 of 60 (0:05:02.553725) 4.0979 mun dumo bie chu-chu bo be now /                                ✗ calculating stats..\n",
      "epoch 0 of 60 (0:05:02.553725) 4.0979 loko dieapu ma na nwo chukurum /                                ✗ calculating stats..\n",
      "epoch 1 of 60 (0:07:40.093278) 3.6081 werima i nyanaye mi ma i nwo d /                                ✗ average batch rate per hr = 7.82,  eta = 7:32:25\n",
      "epoch 1 of 60 (0:07:40.093278) 3.6081 mi ani iya amanyanabo ngada-e  /                                ✗ average batch rate per hr = 7.82,  eta = 7:32:25\n",
      "epoch 1 of 60 (0:10:09.091413) 3.1581 u ma na stivin be na berejinea /                                ✗ average batch rate per hr = 5.91,  eta = 9:58:56\n",
      "epoch 1 of 60 (0:10:09.092416) 3.1581 sotoru bo enekubu bia o kelema /                                ✗ average batch rate per hr = 5.91,  eta = 9:58:56\n",
      "epoch 2 of 60 (0:12:39.744442) 2.9966 lame. o tamuno be torukubu ibi /                                ✗ average batch rate per hr = 9.48,  eta = 6:07:12\n",
      "epoch 2 of 60 (0:12:39.745444) 2.9966 ian mi fie ye be koroma nwo ko /                                ✗ average batch rate per hr = 9.48,  eta = 6:07:12\n",
      "epoch 2 of 60 (0:15:05.982770) 2.9386 am askos bie sime juapu tekewa /                                ✗ average batch rate per hr = 7.95,  eta = 7:17:53\n",
      "epoch 2 of 60 (0:15:05.982770) 2.9386 me. antiok bie mi ini tatari k /                                ✗ average batch rate per hr = 7.95,  eta = 7:17:53\n",
      "epoch 3 of 60 (0:17:33.016993) 2.8823 n siki ma a pulasa ye, anikani /                                ✗ average batch rate per hr = 10.26,  eta = 5:33:27\n",
      "epoch 3 of 60 (0:17:33.016993) 2.8823 nkoro samuel be buo lame. se i /                                ✗ average batch rate per hr = 10.26,  eta = 5:33:27\n",
      "epoch 3 of 60 (0:19:59.331200) 2.8202 eme be na bereni na bie bein b /                                ✗ average batch rate per hr = 9.01,  eta = 6:19:47\n",
      "epoch 3 of 60 (0:19:59.331200) 2.8202 bereniapu ma bu chua se inia k /                                ✗ average batch rate per hr = 9.01,  eta = 6:19:47\n",
      "epoch 4 of 60 (0:22:25.844714) 2.8309 se kura mesi na gbasi ye bu or /                                ✗ average batch rate per hr = 10.70,  eta = 5:14:01\n",
      "epoch 4 of 60 (0:22:25.844714) 2.8309 ia kienbipi enbene-bene senime /                                ✗ average batch rate per hr = 10.70,  eta = 5:14:01\n",
      "epoch 4 of 60 (0:24:52.246640) 2.7757 a mi bu ori na banabas be na s /                                ✗ average batch rate per hr = 9.65,  eta = 5:48:11\n",
      "epoch 4 of 60 (0:24:52.246640) 2.7757 ume, pol be ini firimame na in /                                ✗ average batch rate per hr = 9.65,  eta = 5:48:11\n",
      "epoch 5 of 60 (0:27:29.856114) 2.7722 ie tolu boro muari siki ini ok /                                ✗ average batch rate per hr = 10.91,  eta = 5:02:28\n",
      "epoch 5 of 60 (0:27:29.856114) 2.7722 da obudukoapu o nwo orime, se  /                                ✗ average batch rate per hr = 10.91,  eta = 5:02:28\n",
      "epoch 5 of 60 (0:30:00.049431) 2.7773 i tomoni ma chukuruma se amafi /                                ✗ average batch rate per hr = 10.00,  eta = 5:30:00\n",
      "epoch 5 of 60 (0:30:00.049431) 2.7773 a  wa toroko mun owoin-aru mi  /                                ✗ average batch rate per hr = 10.00,  eta = 5:30:00\n",
      "epoch 6 of 60 (0:32:27.779339) 2.7190 tibi ori be kuroma okwein bu,  /                                ✗ average batch rate per hr = 11.09,  eta = 4:52:10\n",
      "epoch 6 of 60 (0:32:27.779837) 2.7190 e kuro bara se pol be olome. i /                                ✗ average batch rate per hr = 11.09,  eta = 4:52:10\n",
      "epoch 6 of 60 (0:34:54.677427) 2.7398 ine obu balabalama ma, o pa du /                                ✗ average batch rate per hr = 10.31,  eta = 5:14:12\n",
      "epoch 6 of 60 (0:34:54.677427) 2.7398 tibi mi piki gele teinme, se o /                                ✗ average batch rate per hr = 10.31,  eta = 5:14:12\n",
      "epoch 7 of 60 (0:37:21.851417) 2.7332  bumiefiafiama mi okibia ene m / b              i             i ✗ average batch rate per hr = 11.24,  eta = 4:42:54\n",
      "epoch 7 of 60 (0:37:21.851417) 2.7332  pol be okweinma mun se bo ibi / b              i             i ✗ average batch rate per hr = 11.24,  eta = 4:42:54\n",
      "epoch 7 of 60 (0:39:49.017522) 2.7022 sise se dumobia omin mi now ok /                  i   i         ✗ average batch rate per hr = 10.55,  eta = 5:01:28\n",
      "epoch 7 of 60 (0:39:49.018011) 2.7022 re mi dukome na ini inia sima  /                  i   i         ✗ average batch rate per hr = 10.55,  eta = 5:01:28\n",
      "epoch 8 of 60 (0:42:16.581559) 2.6957 i oria ikiapu ma piri ini o bu /               i    i   i    i  ✗ average batch rate per hr = 11.35,  eta = 4:34:47\n",
      "epoch 8 of 60 (0:42:16.581559) 2.6957 a ton werime na ini ikoliapu m /               i    i   i    i  ✗ average batch rate per hr = 11.35,  eta = 4:34:47\n",
      "epoch 8 of 60 (0:44:43.495458) 2.7042 moni ma bie inia kiri nwo okib / i    i       i   i i  i        ✗ average batch rate per hr = 10.73,  eta = 4:50:42\n",
      "epoch 8 of 60 (0:44:43.495959) 2.7042 zos be nwose nin beme, sikima  / i    i       i   i i  i        ✗ average batch rate per hr = 10.73,  eta = 4:50:42\n",
      "epoch 9 of 60 (0:47:11.241411) 2.6969  join ma olo diki ma nwo fimas / b     i        i  i  i     i   ✗ average batch rate per hr = 11.44,  eta = 4:27:23\n",
      "epoch 9 of 60 (0:47:11.242414) 2.6969 bara fiafia teme be now okisam / b     i        i  i  i     i   ✗ average batch rate per hr = 11.44,  eta = 4:27:23\n",
      "epoch 9 of 60 (0:49:38.666882) 2.6707  bu. jon be mengi se nweni sar / b           i        i       i ✗ average batch rate per hr = 10.88,  eta = 4:41:19\n",
      "epoch 9 of 60 (0:49:38.667385) 2.6707 a toroko gwosa bara bu chie si / b           i        i       i ✗ average batch rate per hr = 10.88,  eta = 4:41:19\n",
      "epoch 10 of 60 (0:52:07.470917) 2.6646 ere nwo dukome, o min mi nwo d /  i          i    ii i i ii     ✗ average batch rate per hr = 11.51,  eta = 4:20:37\n",
      "epoch 10 of 60 (0:52:07.471917) 2.6646  bie chu apu ma ini oputekewar /  i          i    ii i i ii     ✗ average batch rate per hr = 11.51,  eta = 4:20:37\n",
      "epoch 10 of 60 (0:54:34.248333) 2.6721 kereme, omine tomoni ma be tib /          i i    i    i  i    i ✗ average batch rate per hr = 10.99,  eta = 4:32:51\n",
      "epoch 10 of 60 (0:54:34.248333) 2.6721 a sime se fi nyo mi ini inia f /          i i    i    i  i    i ✗ average batch rate per hr = 10.99,  eta = 4:32:51\n",
      "epoch 11 of 60 (0:57:01.512938) 2.6371 i angasiki koruabe okuma wa bk /  b i      ii i  i   i i    ii  ✗ average batch rate per hr = 11.57,  eta = 4:14:01\n",
      "epoch 11 of 60 (0:57:01.512938) 2.6371 e mi nab u, ini mengi se ini s /  b i      ii i  i   i i    ii  ✗ average batch rate per hr = 11.57,  eta = 4:14:01\n",
      "epoch 11 of 60 (0:59:29.139269) 2.6483 a o lekirime, se fero be piki  /  b      i i        i  i  i i i ✗ average batch rate per hr = 11.10,  eta = 4:24:58\n",
      "epoch 11 of 60 (0:59:29.139269) 2.6483  ini mgba o dikibalafame. anis /  b      i i        i  i  i i i ✗ average batch rate per hr = 11.10,  eta = 4:24:58\n",
      "epoch 12 of 60 (1:01:56.402345) 2.6430  uri ani tamuno kuro mie, op / b   i   i i  i i ii i ii   i i ✗ average batch rate per hr = 11.62,  eta = 4:07:45\n",
      "epoch 12 of 60 (1:01:56.402345) 2.6430 ro nyana apu ma o nwo gwome. t / b   i   i i  i i ii i ii   i i ✗ average batch rate per hr = 11.62,  eta = 4:07:45\n",
      "epoch 12 of 60 (1:04:23.526637) 2.6198 okuma gbori ene din bie sol be /    i   i i i i    i i  ii  ii  ✗ average batch rate per hr = 11.18,  eta = 4:17:34\n",
      "epoch 12 of 60 (1:04:23.526637) 2.6198  pirime nyanabo be ikoli bie o /    i   i i i i    i i  ii  ii  ✗ average batch rate per hr = 11.18,  eta = 4:17:34\n",
      "epoch 13 of 60 (1:06:51.814143) 2.6282  mamgba nyanabo be. izrel ama  / bi i    i  i i ii  i  i  i i i ✗ average batch rate per hr = 11.67,  eta = 4:01:44\n",
      "epoch 13 of 60 (1:06:51.814143) 2.6282 piri se inia kubie kuromame na / bi i    i  i i ii  i  i  i i i ✗ average batch rate per hr = 11.67,  eta = 4:01:44\n",
      "epoch 13 of 60 (1:09:18.255063) 2.6139 e a beme. okuma a kuroma dukom /  b ii i    i i i ii i i i  i i ✗ average batch rate per hr = 11.25,  eta = 4:10:33\n",
      "epoch 13 of 60 (1:09:18.256094) 2.6139  vinpiki bobia, nyanabo be oko /  b ii i    i i i ii i i i  i i ✗ average batch rate per hr = 11.25,  eta = 4:10:33\n",
      "epoch 14 of 60 (1:11:44.090765) 2.5995 boari inyosara ene mi bie ini  / e  i i i    i i i ii ii  i i i ✗ average batch rate per hr = 11.71,  eta = 3:55:42\n",
      "epoch 14 of 60 (1:11:44.091768) 2.5995 ari book ma finji oku nwo ori  / e  i i i    i i i ii ii  i i i ✗ average batch rate per hr = 11.71,  eta = 3:55:42\n",
      "epoch 14 of 60 (1:14:09.618350) 2.6278 apu ma nwose yechin oku mi, na /     i  i    i     io i oi  oi  ✗ average batch rate per hr = 11.33,  eta = 4:03:40\n",
      "epoch 14 of 60 (1:14:09.618350) 2.6278 ios ama bie ini yi bo, ma bolo /     i  i    i     io i oi  oi  ✗ average batch rate per hr = 11.33,  eta = 4:03:40\n",
      "epoch 15 of 60 (1:16:35.745444) 2.6105 mabe, isia ikoliapu ma ini pok / i      i  i i    i oi o i ii i ✗ average batch rate per hr = 11.75,  eta = 3:49:47\n",
      "epoch 15 of 60 (1:16:35.745444) 2.6105 fisos bie na piki kalaye kenge / i      i  i i    i oi o i ii i ✗ average batch rate per hr = 11.75,  eta = 3:49:47\n",
      "epoch 15 of 60 (1:19:03.346117) 2.5965 pu obu some, se berrenime, ini /   b i  i i  oi oi ii i i  o i  ✗ average batch rate per hr = 11.38,  eta = 3:57:10\n",
      "epoch 15 of 60 (1:19:03.346117) 2.5965 mine ibioku sime wa nwose ini  /   b i  i i  oi oi ii i i  o i  ✗ average batch rate per hr = 11.38,  eta = 3:57:10\n",
      "epoch 16 of 60 (1:21:31.663093) 2.6053 u ma, se nwose ini beme, wuap /  bi  ii ii    o i ii i  o    i ✗ average batch rate per hr = 11.78,  eta = 3:44:12\n",
      "epoch 16 of 60 (1:21:31.663093) 2.6053 ekubu mi gose, ani miese i bu  /  bi  ii ii    o i ii i  o    i ✗ average batch rate per hr = 11.78,  eta = 3:44:12\n",
      "epoch 16 of 60 (1:23:56.850864) 2.6104 pol be ori okwein so siki, ori /    bi   i i i   ioi oi i  o i  ✗ average batch rate per hr = 11.44,  eta = 3:50:51\n",
      "epoch 16 of 60 (1:23:56.850864) 2.6104 vina be piri, se pol be sabama /    bi   i i i   ioi oi i  o i  ✗ average batch rate per hr = 11.44,  eta = 3:50:51\n",
      "epoch 17 of 60 (1:26:24.105703) 2.5742  na sime owuapu ma enekubu mi  / bi ii i i    i oi o i i e oi o ✗ average batch rate per hr = 11.81,  eta = 3:38:32\n",
      "epoch 17 of 60 (1:26:24.106705) 2.5742  nwo naabia,festos be nwo peke / bi ii i i    i oi o i i e oi o ✗ average batch rate per hr = 11.81,  eta = 3:38:32\n",
      "epoch 17 of 60 (1:28:56.947085) 2.5656  bara mi nwo mieme. ani din, m / bi i ii ii  oi  i  o i oi i oi ✗ average batch rate per hr = 11.47,  eta = 3:44:59\n",
      "epoch 17 of 60 (1:28:56.947085) 2.5656 uma bolo ka siki bu, ogono for / bi i ii ii  oi  i  o i oi i oi ✗ average batch rate per hr = 11.47,  eta = 3:44:59\n",
      "epoch 18 of 60 (1:31:23.493031) 2.5573 sabamame o bime ini o se o kor /   i i i b oi i b i i oi o oi i ✗ average batch rate per hr = 11.82,  eta = 3:33:14\n",
      "epoch 18 of 60 (1:31:23.493031) 2.5573 omaye mi. o piki bukuroma mies /   i i i b oi i b i i oi o oi i ✗ average batch rate per hr = 11.82,  eta = 3:33:14\n",
      "epoch 18 of 60 (1:33:49.136572) 2.5847 piki ineda oboku siki mi se an /   i i i   b e i oi i bi oi b i ✗ average batch rate per hr = 11.51,  eta = 3:38:54\n",
      "epoch 18 of 60 (1:33:49.137082) 2.5847 pu nab u ini nwo beme, nwo mi  /   i i i   b e i oi i bi oi b i ✗ average batch rate per hr = 11.51,  eta = 3:38:54\n",
      "epoch 19 of 60 (1:36:15.664048) 2.5718 ania bu weri sime bara, aniati /  i  ii o  i ii i bi i  b i     ✗ average batch rate per hr = 11.84,  eta = 3:27:43\n",
      "epoch 19 of 60 (1:36:15.664048) 2.5718 ki mi bara mgba se wa dikiari  /  i  ii o  i ii i bi i  b i     ✗ average batch rate per hr = 11.84,  eta = 3:27:43\n",
      "epoch 19 of 60 (1:38:43.005807) 2.5539  mi bie anga juapu omie, ani b / bi ii  i i  bi  i b i   b i bi ✗ average batch rate per hr = 11.55,  eta = 3:33:01\n",
      "epoch 19 of 60 (1:38:43.005807) 2.5539 bu, josef, livai fur obo be in / bi ii  i i  bi  i b i   b i bi ✗ average batch rate per hr = 11.55,  eta = 3:33:01\n",
      "epoch 20 of 60 (1:41:20.479438) 2.5396 putekewari mi bie nweni bo be  /     i   i ii bi  bi  i bi bi b ✗ average batch rate per hr = 11.84,  eta = 3:22:40\n",
      "epoch 20 of 60 (1:41:20.479438) 2.5396  bu sosa boe kobirima mi ini o /     i   i ii bi  bi  i bi bi b ✗ average batch rate per hr = 11.84,  eta = 3:22:40\n",
      "epoch 20 of 60 (1:43:51.199357) 2.5377 ma nwose inimgba ogono sarame  / i bi  i b i i e b   i bi i i b ✗ average batch rate per hr = 11.55,  eta = 3:27:42\n",
      "epoch 20 of 60 (1:43:51.199858) 2.5377 ni na ini kara rifan oru mi, o / i bi  i b i i e b   i bi i i b ✗ average batch rate per hr = 11.55,  eta = 3:27:42\n",
      "epoch 21 of 60 (1:46:26.612498) 2.5240 uma ye mi now bereni okime bie /  i bi ii ii  be i i b i i be   ✗ average batch rate per hr = 11.84,  eta = 3:17:40\n",
      "epoch 21 of 60 (1:46:26.612498) 2.5240 engi mi biepakabo siki, tamuno /  i bi ii ii  be i i b i i be   ✗ average batch rate per hr = 11.84,  eta = 3:17:40\n",
      "epoch 21 of 60 (1:48:53.837909) 2.5151 piri yee  ani tamuno be obu pi /   i ii  bi i bi i i be b e bi  ✗ average batch rate per hr = 11.57,  eta = 3:22:14\n",
      "epoch 21 of 60 (1:48:53.837909) 2.5151  eneogboin biri bu tekebia ere /   i ii  bi i bi i i be b e bi  ✗ average batch rate per hr = 11.57,  eta = 3:22:14\n",
      "epoch 22 of 60 (1:51:20.013588) 2.5225 gbolu mgba se bereni me bebe   /  e   bi e bi be i i bi be e bo ✗ average batch rate per hr = 11.86,  eta = 3:12:18\n",
      "epoch 22 of 60 (1:51:20.013588) 2.5225 aka tomoni berenime se vinpiki /  e   bi e bi be i i bi be e bo ✗ average batch rate per hr = 11.86,  eta = 3:12:18\n",
      "epoch 22 of 60 (1:53:46.104653) 2.5162 e mama apu nwo chinme, piki gb /  bi i b i bi  bi  ii  bi i bie ✗ average batch rate per hr = 11.60,  eta = 3:16:30\n",
      "epoch 22 of 60 (1:53:46.105155) 2.5162 , bara se ini dede sa saki okw /  bi i b i bi  bi  ii  bi i bie ✗ average batch rate per hr = 11.60,  eta = 3:16:30\n",
      "epoch 23 of 60 (1:56:14.701685) 2.5301 uo ma gose ani ini stivin be b /   bi bi i b i b i bi    ibe be ✗ average batch rate per hr = 11.87,  eta = 3:07:00\n",
      "epoch 23 of 60 (1:56:14.702161) 2.5301 pisidia se mi bie sime antiok  /   bi bi i b i b i bi    ibe be ✗ average batch rate per hr = 11.87,  eta = 3:07:00\n",
      "epoch 23 of 60 (1:58:40.496207) 2.5385 ewari mi bie smun se kponji si /    i ii be  bii ibi bii i  bi  ✗ average batch rate per hr = 11.63,  eta = 3:10:54\n",
      "epoch 23 of 60 (1:58:40.496684) 2.5385 ikoniom na bie sime bereniapu  /    i ii be  bii ibi bii i  bi  ✗ average batch rate per hr = 11.63,  eta = 3:10:54\n",
      "epoch 24 of 60 (2:01:07.977139) 2.5117 oro o sobie ene seni o pirime, /  i b bi e  b i bi i b bi i i   ✗ average batch rate per hr = 11.89,  eta = 3:01:41\n",
      "epoch 24 of 60 (2:01:07.977643) 2.5117 e bipiberelapu ma na piki owua /  i b bi e  b i bi i b bi i i   ✗ average batch rate per hr = 11.89,  eta = 3:01:41\n",
      "epoch 24 of 60 (2:03:34.463172) 2.5282 e, se o chochi ma miekuromame  /   bi b bii    bi bi  i i a a b ✗ average batch rate per hr = 11.65,  eta = 3:05:21\n",
      "epoch 24 of 60 (2:03:34.463673) 2.5282 e, na ini ibioku se ori okie o /   bi b bii    bi bi  i i a a b ✗ average batch rate per hr = 11.65,  eta = 3:05:21\n",
      "epoch 25 of 60 (2:06:00.849193) 2.4799  mume  aninyo sime apu ma kubi / bi i bi i i  bi i b i bi bi e  ✗ average batch rate per hr = 11.90,  eta = 2:56:25\n",
      "epoch 25 of 60 (2:06:00.850208) 2.4799  biki oria ogono koro se ori a / bi i bi i i  bi i b i bi bi e  ✗ average batch rate per hr = 11.90,  eta = 2:56:25\n",
      "epoch 25 of 60 (2:08:27.443434) 2.4840 ila be na o duko ye naa siki,  /    bi bi b bi i be bi  bi i  b ✗ average batch rate per hr = 11.68,  eta = 2:59:50\n",
      "epoch 25 of 60 (2:08:27.444437) 2.4840 rime, bumiefiafiama mi okibia  /    bi bi b bi i be bi  bi i  b ✗ average batch rate per hr = 11.68,  eta = 2:59:50\n",
      "epoch 26 of 60 (2:10:53.358272) 2.5251 obirima sime ogono wari mi bie /  e i i bi i b i i bi i bi be   ✗ average batch rate per hr = 11.92,  eta = 2:51:09\n",
      "epoch 26 of 60 (2:10:53.358773) 2.5251  sadusiapu ma na saki kamsome  /  e i i bi i b i i bi i bi be   ✗ average batch rate per hr = 11.92,  eta = 2:51:09\n",
      "epoch 26 of 60 (2:13:18.589154) 2.4812 a fiye goyegoye now fi ma, pik /  bi i bi e   e bi  bi bi  bi i ✗ average batch rate per hr = 11.70,  eta = 2:54:19\n",
      "epoch 26 of 60 (2:13:18.589154) 2.4812 boro siki feliks be na taro dr /  bi i bi e   e bi  bi bi  bi i ✗ average batch rate per hr = 11.70,  eta = 2:54:19\n",
      "epoch 27 of 60 (2:15:44.791460) 2.4816  pol be ori na kobirima mi bie / bi  be b i bi bi e i i bi be   ✗ average batch rate per hr = 11.93,  eta = 2:45:54\n",
      "epoch 27 of 60 (2:15:44.791460) 2.4816 mi bie na, piki jerusalem mi b / bi  be b i bi bi e i i bi be   ✗ average batch rate per hr = 11.93,  eta = 2:45:54\n",
      "epoch 27 of 60 (2:18:10.878388) 2.4705 o, lisias be bo la siki u o to /   bi i  ibe be bi bi i b b bi  ✗ average batch rate per hr = 11.72,  eta = 2:48:53\n",
      "epoch 27 of 60 (2:18:10.878388) 2.4705 e kuro mi dukuno mi fomu kanme /   bi i  ibe be bi bi i b b bi  ✗ average batch rate per hr = 11.72,  eta = 2:48:53\n",
      "epoch 27 of 60 (2:20:36.429005) 2.4687 amuno be piribia, miese ini ib /  a i be bi i e   bi  i b i b i ✗ average batch rate per hr = 11.52,  eta = 2:51:51\n",
      "epoch 27 of 60 (2:20:36.429507) 2.4687 ni i gose boma se tamuno be pi /  a i be bi i e   bi  i b i b i ✗ average batch rate per hr = 11.52,  eta = 2:51:51\n",
      "epoch 28 of 60 (2:23:04.182769) 2.4714 yee, ani gbori siki mi bu, ini /     b i bie i bi i bi be  b i  ✗ average batch rate per hr = 11.74,  eta = 2:43:30\n",
      "epoch 28 of 60 (2:23:04.182769) 2.4714 ime ye    min mi ani die-au na /     b i bie i bi i bi be  b i  ✗ average batch rate per hr = 11.74,  eta = 2:43:30\n",
      "epoch 28 of 60 (2:25:30.456958) 2.4619 siria na salisia na nyo ma nwo /   i  bi bi   i  bi bie bi bi   ✗ average batch rate per hr = 11.55,  eta = 2:46:17\n",
      "epoch 28 of 60 (2:25:30.456958) 2.4619 a oju ma biari ye bereton o pi /   i  bi bi   i  bi bie bi bi   ✗ average batch rate per hr = 11.55,  eta = 2:46:17\n",
      "epoch 29 of 60 (2:28:05.816995) 2.4388 ia bo be bobia siki labia, ani /   bi be be e  bi i bi e   b i  ✗ average batch rate per hr = 11.75,  eta = 2:38:18\n",
      "epoch 29 of 60 (2:28:05.818013) 2.4388 a sekiri na bie sime ye goyego /   bi be be e  bi i bi e   b i  ✗ average batch rate per hr = 11.75,  eta = 2:38:18\n",
      "epoch 29 of 60 (2:30:32.577339) 2.4489 e kraist mgbesechi bere duko m /  bii  i bi e e    be i bi i bi ✗ average batch rate per hr = 11.56,  eta = 2:40:55\n",
      "epoch 29 of 60 (2:30:32.577339) 2.4489 iatibi ari omine gose torusior /  bii  i bi e e    be i bi i bi ✗ average batch rate per hr = 11.56,  eta = 2:40:55\n",
      "epoch 30 of 60 (2:32:58.059182) 2.4569 u diki. ani o se wa piriye mi  /  bi i  b i b bi bi bi i e bi b ✗ average batch rate per hr = 11.77,  eta = 2:32:58\n",
      "epoch 30 of 60 (2:32:58.059691) 2.4569 w some, okuma o bukuroma nyana /  bi i  b i b bi bi bi i e bi b ✗ average batch rate per hr = 11.77,  eta = 2:32:58\n",
      "epoch 30 of 60 (2:35:24.004549) 2.4919 aka yeton mi weri sime bara bu /  i be   ibi bi i bi i be i be  ✗ average batch rate per hr = 11.58,  eta = 2:35:24\n",
      "epoch 30 of 60 (2:35:24.005055) 2.4919 i. se ori aniduko peleme, na o /  i be   ibi bi i bi i be i be  ✗ average batch rate per hr = 11.58,  eta = 2:35:24\n",
      "epoch 31 of 60 (2:37:50.251543) 2.4734 , piki sise fii bu, se kraist  /  bi i bi i bi  be  bi bii  i b ✗ average batch rate per hr = 11.78,  eta = 2:27:39\n",
      "epoch 31 of 60 (2:37:50.251543) 2.4734 aist bee. a fieariye, ani na,  /  bi i bi i bi  be  bi bii  i b ✗ average batch rate per hr = 11.78,  eta = 2:27:39\n",
      "epoch 31 of 60 (2:40:17.124770) 2.4513 bia erechi. iri na oguapu ma b / e  b i     b i bi b i  i bi be ✗ average batch rate per hr = 11.60,  eta = 2:29:56\n",
      "epoch 31 of 60 (2:40:17.124770) 2.4513 umaye mi ani minaminaka baka f / e  b i     b i bi b i  i bi be ✗ average batch rate per hr = 11.60,  eta = 2:29:56\n",
      "epoch 32 of 60 (2:42:43.105688) 2.4505 bu bipiogboso nyana se ani gos / e be i  ie e bie i bi b i bi i ✗ average batch rate per hr = 11.80,  eta = 2:22:22\n",
      "epoch 32 of 60 (2:42:43.106189) 2.4505  bu boro mine oki o bu piribia / e be i  ie e bie i bi b i bi i ✗ average batch rate per hr = 11.80,  eta = 2:22:22\n",
      "epoch 32 of 60 (2:45:09.959646) 2.4347 upele na bupeleka na mie kirim /  i   bi be i   i bi bi  bi i i ✗ average batch rate per hr = 11.62,  eta = 2:24:31\n",
      "epoch 32 of 60 (2:45:09.960156) 2.4347 injibia. chuku saki bu ngisi,  /  i   bi be i   i bi bi  bi i i ✗ average batch rate per hr = 11.62,  eta = 2:24:31\n",
      "epoch 33 of 60 (2:47:35.279409) 2.4817 e, so bie bu. tomonikiri mi ma /   bi be  be  bi i i i i bi bi  ✗ average batch rate per hr = 11.81,  eta = 2:17:07\n",
      "epoch 33 of 60 (2:47:35.279409) 2.4817 oki pakuma enekubu mi bie bobi /   bi be  be  bi i i i i bi bi  ✗ average batch rate per hr = 11.81,  eta = 2:17:07\n",
      "epoch 33 of 60 (2:50:01.811430) 2.4312  bu. iri iderima tein tamunoa  / be  b i b i i i bi  ibi a i  b ✗ average batch rate per hr = 11.64,  eta = 2:19:06\n",
      "epoch 33 of 60 (2:50:01.812423) 2.4312  siki, a dikibugerere nyanaka  / be  b i b i i i bi  ibi a i  b ✗ average batch rate per hr = 11.64,  eta = 2:19:06\n",
      "epoch 34 of 60 (2:52:27.728792) 2.4347 a nyanabo be bubelebiaye now d /  bie i e be be e   e  e bi obi ✗ average batch rate per hr = 11.83,  eta = 2:11:52\n",
      "epoch 34 of 60 (2:52:27.729293) 2.4347  bara  okoko mi na yeberenibip /  bie i e be be e   e  e bi obi ✗ average batch rate per hr = 11.83,  eta = 2:11:52\n",
      "epoch 34 of 60 (2:54:54.652798) 2.4308 n mi tomonibo bara bub o i la  / ibi bi i i e be i be eb b bi b ✗ average batch rate per hr = 11.66,  eta = 2:13:45\n",
      "epoch 34 of 60 (2:54:54.652798) 2.4308 kuro mi bu, minea bereni bu bo / ibi bi i i e be i be eb b bi b ✗ average batch rate per hr = 11.66,  eta = 2:13:45\n",
      "epoch 35 of 60 (2:57:19.904568) 2.4435 e chin ogono sololo bo goyegoy /  bii ib i i bi     be bi e   e ✗ average batch rate per hr = 11.84,  eta = 2:06:39\n",
      "epoch 35 of 60 (2:57:19.905553) 2.4435 be aniatibi omine belemasam mi /  bii ib i i bi     be bi e   e ✗ average batch rate per hr = 11.84,  eta = 2:06:39\n",
      "epoch 35 of 60 (2:59:45.924045) 2.4110 roma oloko mi duko ye mie bu b / i a b i i bi bi i be bi  be be ✗ average batch rate per hr = 11.68,  eta = 2:08:24\n",
      "epoch 35 of 60 (2:59:45.925048) 2.4110 boro bereni anye ma nwo diaoki / i a b i i bi bi i be bi  be be ✗ average batch rate per hr = 11.68,  eta = 2:08:24\n",
      "epoch 36 of 60 (3:02:11.809137) 2.4312  apu ma seimabe na, dikibugere / b i bi bi  i e bi  bi i e   i  ✗ average batch rate per hr = 11.86,  eta = 2:01:27\n",
      "epoch 36 of 60 (3:02:11.809137) 2.4312 a  aniatibi enekubu mi ani ibi / b i bi bi  i e bi  bi i e   i  ✗ average batch rate per hr = 11.86,  eta = 2:01:27\n",
      "epoch 36 of 60 (3:04:38.753535) 2.4262 a mioku tamuo be o teme be bu  /  bi  i bi a  be b bi a be be b ✗ average batch rate per hr = 11.70,  eta = 2:03:05\n",
      "epoch 36 of 60 (3:04:38.753535) 2.4262 mi go apu ma pirime na ini ber /  bi  i bi a  be b bi a be be b ✗ average batch rate per hr = 11.70,  eta = 2:03:05\n",
      "epoch 37 of 60 (3:07:04.856966) 2.4096 ipokika apu ma ogono gbanabia  /  i i i b i bi b i i bie i e  b ✗ average batch rate per hr = 11.87,  eta = 1:56:17\n",
      "epoch 37 of 60 (3:07:04.856966) 2.4096 ni, \t  ini i dadiki piki i ele /  i i i b i bi b i i bie i e  b ✗ average batch rate per hr = 11.87,  eta = 1:56:17\n",
      "epoch 37 of 60 (3:09:31.333594) 2.4131 okoma duaboroma mi piripiri bo /  i a bi  e i a bi bi i i i be  ✗ average batch rate per hr = 11.71,  eta = 1:57:48\n",
      "epoch 37 of 60 (3:09:31.334082) 2.4131 u nemime na, omine ibitein bel /  i a bi  e i a bi bi i i i be  ✗ average batch rate per hr = 11.71,  eta = 1:57:48\n",
      "epoch 38 of 60 (3:11:57.015303) 2.4116 bara bu, mimgba o nemikase tam / e i be  bi i e b bi a i i bi a ✗ average batch rate per hr = 11.88,  eta = 1:51:07\n",
      "epoch 38 of 60 (3:11:57.015303) 2.4116 oroma oboku mi ibioku mi ani i / e i be  bi i e b bi a i i bi a ✗ average batch rate per hr = 11.88,  eta = 1:51:07\n",
      "epoch 38 of 60 (3:14:23.576134) 2.4128 kuma, tamuno be ani belemame.  / i a  bi a i be b i be   a a  b ✗ average batch rate per hr = 11.73,  eta = 1:52:32\n",
      "epoch 38 of 60 (3:14:23.576635) 2.4128 ton-a nama se edeni ogono mie  / i a  bi a i be b i be   a a  b ✗ average batch rate per hr = 11.73,  eta = 1:52:32\n",
      "epoch 39 of 60 (3:16:49.885967) 2.4222  mi-e. ori oboku ikowari mi na / bi    b i b e i b i o i bi bi  ✗ average batch rate per hr = 11.89,  eta = 1:45:59\n",
      "epoch 39 of 60 (3:16:49.886468) 2.4222 sam.    bereni mi miese ebraha / bi    b i b e i b i o i bi bi  ✗ average batch rate per hr = 11.89,  eta = 1:45:59\n",
      "epoch 39 of 60 (3:19:16.951361) 2.4009 tomoni ma be sima nwose inia b /   a i bi be be i bi  e b i  be ✗ average batch rate per hr = 11.74,  eta = 1:47:18\n",
      "epoch 39 of 60 (3:19:16.951361) 2.4009 bi, se okwein ari bo be poki d /   a i bi be be i bi  e b i  be ✗ average batch rate per hr = 11.74,  eta = 1:47:18\n",
      "epoch 40 of 60 (3:21:43.191455) 2.4043 se mi be koroma nwo konke, ini / e bi be bi i a bi  bi ii  b i  ✗ average batch rate per hr = 11.90,  eta = 1:40:51\n",
      "epoch 40 of 60 (3:21:43.192468) 2.4043 ria firinwengiapu ma nwose fin / e bi be bi i a bi  bi ii  b i  ✗ average batch rate per hr = 11.90,  eta = 1:40:51\n",
      "epoch 40 of 60 (3:24:11.562671) 2.4049 pu ma. nweni mamgba be berekon / i bi  bio i bi a e be be i i i ✗ average batch rate per hr = 11.75,  eta = 1:42:05\n",
      "epoch 40 of 60 (3:24:11.563675) 2.4049 ma ene mi gbolomaye ini nwo du / i bi  bio i bi a e be be i i i ✗ average batch rate per hr = 11.75,  eta = 1:42:05\n",
      "epoch 41 of 60 (3:26:40.919087) 2.3991  piribia  ori piki toku so i p / bi i e  bi i bi i bi i bi b bi ✗ average batch rate per hr = 11.90,  eta = 1:35:46\n",
      "epoch 41 of 60 (3:26:40.919087) 2.3991 k be pirime. mi melkizedek be  / bi i e  bi i bi i bi i bi b bi ✗ average batch rate per hr = 11.90,  eta = 1:35:46\n",
      "epoch 41 of 60 (3:29:14.392932) 2.3723  bara mi, ani, ani paka okue.  / be i bi  b i  b i bi i b i   b ✗ average batch rate per hr = 11.76,  eta = 1:36:57\n",
      "epoch 41 of 60 (3:29:14.392932) 2.3723 bamieapu ma enebenebene so mun / be i bi  b i  b i bi i b i   b ✗ average batch rate per hr = 11.76,  eta = 1:36:57\n",
      "epoch 42 of 60 (3:31:40.879409) 2.4251 ana boe, piki gele tein tamuno /  i be   bi i bi   bi  ibi a i  ✗ average batch rate per hr = 11.90,  eta = 1:30:43\n",
      "epoch 42 of 60 (3:31:40.879409) 2.4251 oku mi. aninakaraka, o ye wa j /  i be   bi i bi   bi  ibi a i  ✗ average batch rate per hr = 11.90,  eta = 1:30:43\n",
      "epoch 42 of 60 (3:34:10.356997) 2.3973 e mi be ogonoju cherubim nwo c /  bi be b i i i bii i e abi  bi ✗ average batch rate per hr = 11.77,  eta = 1:31:47\n",
      "epoch 42 of 60 (3:34:10.357510) 2.3973 umapu ma nyana bome, ini fibus /  bi be b i i i bii i e abi  bi ✗ average batch rate per hr = 11.77,  eta = 1:31:47\n",
      "epoch 43 of 60 (3:36:37.048409) 2.3599 use dikiariye mi nwo olo, ania /  e bi i  i e bi bio b i  b i   ✗ average batch rate per hr = 11.91,  eta = 1:25:38\n",
      "epoch 43 of 60 (3:36:37.048911) 2.3599 nabo bee nwobe ye mi dukopakum /  e bi i  i e bi bio b i  b i   ✗ average batch rate per hr = 11.91,  eta = 1:25:38\n",
      "epoch 43 of 60 (3:39:03.901517) 2.3670 e nwo mieme se tamuno be beren /  bio bi  a be bi a i be be i i ✗ average batch rate per hr = 11.78,  eta = 1:26:36\n",
      "epoch 43 of 60 (3:39:03.901517) 2.3670 wo okibo oria boma mi dieokibi /  bio bi  a be bi a i be be i i ✗ average batch rate per hr = 11.78,  eta = 1:26:36\n",
      "epoch 44 of 60 (3:41:30.308163) 2.3815 kuma ikowari mi obuju o damamu / i a b i o i bi b e   b bi a a  ✗ average batch rate per hr = 11.92,  eta = 1:20:32\n",
      "epoch 44 of 60 (3:41:30.309171) 2.3815 e oku mise,   ori agbamiebo os / i a b i o i bi b e   b bi a a  ✗ average batch rate per hr = 11.92,  eta = 1:20:32\n",
      "epoch 44 of 60 (3:44:00.148873) 2.3818 kala siki bu o fi tomoni mamgb / i   bi i be b bi bi a i bi a e ✗ average batch rate per hr = 11.79,  eta = 1:21:27\n",
      "epoch 44 of 60 (3:44:00.149841) 2.3818 be jinye, tamuno be yeberenime / i   bi i be b bi bi a i bi a e ✗ average batch rate per hr = 11.79,  eta = 1:21:27\n",
      "epoch 45 of 60 (3:46:31.908620) 2.3455 iki, tomoni ma be si ma, abga  /  i  bi a i bi be be bi  b e  b ✗ average batch rate per hr = 11.92,  eta = 1:15:30\n",
      "epoch 45 of 60 (3:46:31.908620) 2.3455 i oloko diri mi na piki tomoni /  i  bi a i bi be be bi  b e  b ✗ average batch rate per hr = 11.92,  eta = 1:15:30\n",
      "epoch 45 of 60 (3:49:01.622572) 2.3786 a, melkizedek be oru mi bu  an /   bi  i     ibe b i bi be bo i ✗ average batch rate per hr = 11.79,  eta = 1:16:20\n",
      "epoch 45 of 60 (3:49:01.622572) 2.3786 raka ini ori oki iwo tamuno be /   bi  i     ibe b i bi be bo i ✗ average batch rate per hr = 11.79,  eta = 1:16:20\n",
      "epoch 46 of 60 (3:51:30.222020) 2.3692  bebe o ton wari kiri okibia   / be e b bi ibo i bi i b i e  bi ✗ average batch rate per hr = 11.92,  eta = 1:10:27\n",
      "epoch 46 of 60 (3:51:30.222521) 2.3692 me, biebeleke  obuko, ani sime / be e b bi ibo i bi i b i e  bi ✗ average batch rate per hr = 11.92,  eta = 1:10:27\n",
      "epoch 46 of 60 (3:54:01.306477) 2.3728 ein ken be nengi yee  oria ber /   ibi ibe bi i  be  boii  be i ✗ average batch rate per hr = 11.79,  eta = 1:11:13\n",
      "epoch 46 of 60 (3:54:01.307480) 2.3728 ama bebe tomonikiri mi kelema  /   ibi ibe bi i  be  boii  be i ✗ average batch rate per hr = 11.79,  eta = 1:11:13\n",
      "epoch 47 of 60 (3:56:31.107216) 2.3768 naga dabo be piri se dumo gbei / i   bi e be bi i bi bi a bie   ✗ average batch rate per hr = 11.92,  eta = 1:05:25\n",
      "epoch 47 of 60 (3:56:31.107216) 2.3768 le mi soni simeoku-e ani boka  / i   bi e be bi i bi bi a bie   ✗ average batch rate per hr = 11.92,  eta = 1:05:25\n",
      "epoch 47 of 60 (3:59:00.916091) 2.3497 berejine okwein .- bereni na f / e i   i b io  ibi be i i bi bi ✗ average batch rate per hr = 11.80,  eta = 1:06:06\n",
      "epoch 47 of 60 (3:59:00.917107) 2.3497     diepakumaye    pita be bar / e i   i b io  ibi be i i bi bi ✗ average batch rate per hr = 11.80,  eta = 1:06:06\n",
      "epoch 48 of 60 (4:01:30.424004) 2.3737 o miebia oku, ani bu wa piki i /  bi  e  b i  b i be bo bi i b  ✗ average batch rate per hr = 11.93,  eta = 1:00:22\n",
      "epoch 48 of 60 (4:01:30.424504) 2.3737 i ani boro sedabo be-o, bereto /  bi  e  b i  b i be bo bi i b  ✗ average batch rate per hr = 11.93,  eta = 1:00:22\n",
      "epoch 48 of 60 (4:04:00.568548) 2.3738 vinpiki bome bebe, o min       /   i  i be a be e  b bi iboiiii ✗ average batch rate per hr = 11.80,  eta = 1:01:00\n",
      "epoch 48 of 60 (4:04:00.568548) 2.3738 .  okuma o biebele na kraist b /   i  i be a be e  b bi iboiiii ✗ average batch rate per hr = 11.80,  eta = 1:01:00\n",
      "epoch 49 of 60 (4:06:29.593282) 2.3418 e oru bere nemi apu ogboku omi /  b i be i bi a b i b ie i b a  ✗ average batch rate per hr = 11.93,  eta = 0:55:20\n",
      "epoch 49 of 60 (4:06:29.593282) 2.3418 niabu gbaname bebe, aniatibi k /  b i be i bi a b i b ie i b a  ✗ average batch rate per hr = 11.93,  eta = 0:55:20\n",
      "epoch 49 of 60 (4:08:59.263011) 2.3700 oro mun tamuno be labia oku ji /  i bi ibi a   be bi e  b i bi  ✗ average batch rate per hr = 11.81,  eta = 0:55:53\n",
      "epoch 49 of 60 (4:08:59.264013) 2.3700 amgba na amen.                 /  i bi ibi a   be bi e  b i bi  ✗ average batch rate per hr = 11.81,  eta = 0:55:53\n",
      "epoch 50 of 60 (4:11:28.045367) 2.3330 ubeleme  mine na minabuna min  /  e   a boi i bi bi i e   bi ib ✗ average batch rate per hr = 11.93,  eta = 0:50:17\n",
      "epoch 50 of 60 (4:11:28.045367) 2.3330  mine ma mioku tamuno awo-e, o /  e   a boi i bi bi i e   bi ib ✗ average batch rate per hr = 11.93,  eta = 0:50:17\n",
      "epoch 50 of 60 (4:13:57.613240) 2.3324 u nemi, miese bara layelaye mi /  bi a  bi  e be i bi e   e bi  ✗ average batch rate per hr = 11.81,  eta = 0:50:47\n",
      "epoch 50 of 60 (4:13:57.613240) 2.3324 oteinme ori piki onyechie seni /  bi a  bi  e be i bi e   e bi  ✗ average batch rate per hr = 11.81,  eta = 0:50:47\n",
      "epoch 51 of 60 (4:16:26.460574) 2.3441  na karakaraye mi mie bo goyeg / bi bi i i i e bi bi  be bi e   ✗ average batch rate per hr = 11.93,  eta = 0:45:15\n",
      "epoch 51 of 60 (4:16:26.460574) 2.3441  tetema -  belema piri -    __ / bi bi i i i e bi bi  be bi e   ✗ average batch rate per hr = 11.93,  eta = 0:45:15\n",
      "epoch 51 of 60 (4:18:55.757215) 2.3450 na onyechie min mi pirime na k / i b ie     bi ibi bi i i bi bi ✗ average batch rate per hr = 11.82,  eta = 0:45:41\n",
      "epoch 51 of 60 (4:18:55.757732) 2.3450 ma nwose ini be, dumo bu inia  / i b ie     bi ibi bi i i bi bi ✗ average batch rate per hr = 11.82,  eta = 0:45:41\n",
      "epoch 52 of 60 (4:21:24.191385) 2.3527  mi kien-apu ma kubiekuromame  / bi bi  i  i bi bi e  i i a a b ✗ average batch rate per hr = 11.94,  eta = 0:40:12\n",
      "epoch 52 of 60 (4:21:24.192400) 2.3527 ene ma soni nwo lame. omine so / bi bi  i  i bi bi e  i i a a b ✗ average batch rate per hr = 11.94,  eta = 0:40:12\n",
      "epoch 52 of 60 (4:23:54.332548) 2.3258 tomonibo nwo be na o bereni ny /   a i e bio be bi b be i i bie ✗ average batch rate per hr = 11.82,  eta = 0:40:36\n",
      "epoch 52 of 60 (4:23:54.333049) 2.3258  piki duaboroma nyanabia oku.  /   a i e bio be bi b be i i bie ✗ average batch rate per hr = 11.82,  eta = 0:40:36\n",
      "epoch 53 of 60 (4:26:24.102920) 2.3378 ki o basam, o piki omine be ku / i b be e a b bi i b i i be bi  ✗ average batch rate per hr = 11.94,  eta = 0:35:11\n",
      "epoch 53 of 60 (4:26:24.103934) 2.3378 uno be bereton tomonikiri bie  / i b be e a b bi i b i i be bi  ✗ average batch rate per hr = 11.94,  eta = 0:35:11\n",
      "epoch 53 of 60 (4:29:00.423292) 2.3174 iki fiafia se mi na ani gose s /  i bi  i  bi bi bi b i bi e be ✗ average batch rate per hr = 11.82,  eta = 0:35:31\n",
      "epoch 53 of 60 (4:29:00.423793) 2.3174 ist be bu o dawonemisa ye ma b /  i bi  i  bi bi bi b i bi e be ✗ average batch rate per hr = 11.82,  eta = 0:35:31\n",
      "epoch 54 of 60 (4:31:29.821083) 2.3258  ma na piki kuro mamgba piriar / bi bi bi i bi i bi a e bi i  i ✗ average batch rate per hr = 11.93,  eta = 0:30:09\n",
      "epoch 54 of 60 (4:31:29.821584) 2.3258 e aye ma ari doki fabia, piki  / bi bi bi i bi i bi a e bi i  i ✗ average batch rate per hr = 11.93,  eta = 0:30:09\n",
      "epoch 54 of 60 (4:33:57.044799) 2.3530 kirima nyana mi bie tamuno kik / i i i bie i ba be  bi a   bi i ✗ average batch rate per hr = 11.83,  eta = 0:30:26\n",
      "epoch 54 of 60 (4:33:57.044799) 2.3530 nwo be ye goseke aninakaraka,  / i i i bie i ba be  bi a   bi i ✗ average batch rate per hr = 11.83,  eta = 0:30:26\n",
      "epoch 55 of 60 (4:36:22.565318) 2.3112 , o bereni ye mie bu, tonapu o /  b be i i be bi  be  bi i i b  ✗ average batch rate per hr = 11.94,  eta = 0:25:07\n",
      "epoch 55 of 60 (4:36:22.565318) 2.3112 me na o tamuno be belemame, ok /  b be i i be bi  be  bi i i b  ✗ average batch rate per hr = 11.94,  eta = 0:25:07\n",
      "epoch 55 of 60 (4:38:48.369921) 2.3266 , ini mine na simesam. okuma i /  b i bi i bi bi i e a b i a bn ✗ average batch rate per hr = 11.84,  eta = 0:25:20\n",
      "epoch 55 of 60 (4:38:48.369921) 2.3266  ye mi nwo da. ibiye mie bo go /  b i bi i bi bi i e a b i a bn ✗ average batch rate per hr = 11.84,  eta = 0:25:20\n",
      "epoch 55 of 60 (4:41:14.683792) 2.3398 alafa so belema mi bie oforie  /  a i be be   a ba be  b i i  b ✗ average batch rate per hr = 11.73,  eta = 0:25:34\n",
      "epoch 55 of 60 (4:41:14.683792) 2.3398 n apu ma toroko mun alakon apu /  a i be be   a ba be  b i i  b ✗ average batch rate per hr = 11.73,  eta = 0:25:34\n",
      "epoch 56 of 60 (4:43:41.474039) 2.3225 iki minea bu o duko balafama y /  i bi i  be b bi i be a i a be ✗ average batch rate per hr = 11.84,  eta = 0:20:15\n",
      "epoch 56 of 60 (4:43:41.474039) 2.3225  mie gbein ibiye mi nemi se an /  i bi i  be b bi i be a i a be ✗ average batch rate per hr = 11.84,  eta = 0:20:15\n",
      "epoch 56 of 60 (4:46:07.269205) 2.3417  sosam  o poki ibu yela minapu / bi e abo bi i bne be   bi i i  ✗ average batch rate per hr = 11.74,  eta = 0:20:26\n",
      "epoch 56 of 60 (4:46:07.269205) 2.3417 siki ngisi tamuno be o seleme, / bi e abo bi i bne be   bi i i  ✗ average batch rate per hr = 11.74,  eta = 0:20:26\n",
      "epoch 57 of 60 (4:48:34.317656) 2.3274 o simeoku ani i simeokue, boka /  bi i  i b i bnbi i  i   be i  ✗ average batch rate per hr = 11.85,  eta = 0:15:11\n",
      "epoch 57 of 60 (4:48:34.317656) 2.3274 n si ye gose torusiori mi neng /  bi i  i b i bnbi i  i   be i  ✗ average batch rate per hr = 11.85,  eta = 0:15:11\n",
      "epoch 57 of 60 (4:50:59.343778) 2.3434 dapu ma bara o daokisa yebusok /   i ba be i b bi  i i be e e i ✗ average batch rate per hr = 11.75,  eta = 0:15:18\n",
      "epoch 57 of 60 (4:50:59.343778) 2.3434 o nyana yela bereni mi nyanasa /   i ba be i b bi  i i be e e i ✗ average batch rate per hr = 11.75,  eta = 0:15:18\n",
      "epoch 58 of 60 (4:53:26.582592) 2.3247  mi mie  ominea koroma ma o nw / bi bi  boii i  bi i a ba b bio ✗ average batch rate per hr = 11.86,  eta = 0:10:07\n",
      "epoch 58 of 60 (4:53:26.582592) 2.3247 eton piri dumokon apu ma bo ok / bi bi  boii i  bi i a ba b bio ✗ average batch rate per hr = 11.86,  eta = 0:10:07\n",
      "epoch 58 of 60 (4:55:54.212968) 2.3250 wengiabo chinbipi mi   iruo ye / o i   e bii ie i bi biini  be  ✗ average batch rate per hr = 11.76,  eta = 0:10:12\n",
      "epoch 58 of 60 (4:55:54.212968) 2.3250 mime  a diri gien omine asemen / o i   e bii ie i bi biini  be  ✗ average batch rate per hr = 11.76,  eta = 0:10:12\n",
      "epoch 59 of 60 (4:58:20.366730) 2.3168 oroma bu  a bime bolo siki ngi /  i a be bonbe i be e be i bi   ✗ average batch rate per hr = 11.87,  eta = 0:05:03\n",
      "epoch 59 of 60 (4:58:20.366730) 2.3168 be omine na gboribu omie, piki /  i a be bonbe i be e be i bi   ✗ average batch rate per hr = 11.87,  eta = 0:05:03\n",
      "epoch 59 of 60 (5:00:45.629299) 2.2968  siki ngisi sime bo be omine o / be i bi  i bi i be be b i i b  ✗ average batch rate per hr = 11.77,  eta = 0:05:05\n",
      "epoch 59 of 60 (5:00:45.629299) 2.2968  mi bie omie now be ye mi   in / be i bi  i bi i be be b i i b  ✗ average batch rate per hr = 11.77,  eta = 0:05:05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 250\n",
    "plot_every = 100\n",
    "\n",
    "vloss = []\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "iter=0\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=nb_epoch):\n",
    "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    category =  [lin2txt(l) for l in y_]\n",
    "    lines = [lin2txt(l) for l in x]\n",
    "    category_tensor=mb2t(y_)\n",
    "    line_tensor=mb2t(x)\n",
    "    output, loss = train(torch.tensor(y_,dtype=torch.long), line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess = [lin2txt([ch.argmax(dim=0) for ch in line]) for line in output]\n",
    "        for i in range(2):\n",
    "            elapsed_time = time.time() - start\n",
    "            tss = str(datetime.timedelta(seconds=elapsed_time)) # time since start string\n",
    "            if epoch > 0:\n",
    "                speed = epoch/elapsed_time\n",
    "                eta = (nb_epoch-epoch)/speed\n",
    "                sspeed = speed*60*60\n",
    "                seta = str(datetime.timedelta(seconds=int(eta)))\n",
    "                stats = f'average epoch rate per hr = %3.2f,  eta = {seta}'%(sspeed)\n",
    "            else:\n",
    "                stats ='calculating stats..'\n",
    "            correct = '✓' if guess[i] == category[i] else '✗ %s' % stats \n",
    "            print('epoch %d of %d (%s) %.4f %s / %s %s' % (epoch, nb_epoch, tss, loss, lines[i], guess[0], correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0 and len(valitext) > 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, BATCHSIZE, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        line_tensor = mb2t(vali_x)\n",
    "        output, loss = train(torch.tensor(vali_y, dtype=torch.long), line_tensor)\n",
    "        vloss.append(loss)\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9bbc6e847988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(vloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is=torch.Size([1, 98]), hs=torch.Size([1, 128])\n",
      "torch.Size([1, 98])\n"
     ]
    }
   ],
   "source": [
    "#input = lineToTensor('Albert')\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "output, next_hidden = rnn(a[0][0], hidden)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# training loop\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10):\n",
    "\n",
    "    # train on one minibatch\n",
    "    # feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n",
    "    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n",
    "\n",
    "    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n",
    "    if step % _50_BATCHES == 0:\n",
    "        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n",
    "        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n",
    "        summary_writer.add_summary(smm, step)\n",
    "\n",
    "    # run a validation step every 50 batches\n",
    "    # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n",
    "    # so we cut it up and batch the pieces (slightly inaccurate)\n",
    "    # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n",
    "    if step % _50_BATCHES == 0 and len(valitext) > 0:\n",
    "        VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n",
    "        bsize = len(valitext) // VALI_SEQLEN\n",
    "        txt.print_validation_header(len(codetext), bookranges)\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n",
    "        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n",
    "                     batchsize: bsize}\n",
    "        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_validation_stats(ls, acc)\n",
    "        # save validation data for Tensorboard\n",
    "        validation_writer.add_summary(smm, step)\n",
    "\n",
    "    # display a short text generated with the current weights and biases (every 150 batches)\n",
    "    if step // 3 % _50_BATCHES == 0:\n",
    "        txt.print_text_generation_header()\n",
    "        ry = np.array([[txt.convert_from_alphabet(ord(\"K\"))]])\n",
    "        rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n",
    "        for k in range(1000):\n",
    "            ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n",
    "            rc = txt.sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n",
    "            print(chr(txt.convert_to_alphabet(rc)), end=\"\")\n",
    "            ry = np.array([[rc]])\n",
    "        txt.print_text_generation_footer()\n",
    "\n",
    "    # save a checkpoint (every 500 batches)\n",
    "    if step // 10 % _50_BATCHES == 0:\n",
    "        saved_file = saver.save(sess, 'checkpoints/rnn_train_' + timestamp, global_step=step)\n",
    "        print(\"Saved file: \" + saved_file)\n",
    "\n",
    "    # display progress bar\n",
    "    progress.step(reset=step % _50_BATCHES == 0)\n",
    "\n",
    "    # loop state around\n",
    "    istate = ostate\n",
    "    step += BATCHSIZE * SEQLEN"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
