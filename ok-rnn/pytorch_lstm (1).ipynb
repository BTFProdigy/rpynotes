{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "pytorch_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNRPmXnaYG5T",
        "outputId": "5b920d23-8567-4bce-ba55-1b31b3c7165a"
      },
      "source": [
        "!git clone https://github.com/kpu/kenlm.git\r\n",
        "!mkdir -p kenlm/build\r\n",
        "!cd kenlm/build && cmake .. && make -j 4\r\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip\r\n",
        "!wget --no-check-certificate -c -O txts.zip \"https://onedrive.live.com/download?cid=EB20651D09521520&resid=EB20651D09521520%21171763&authkey=AAAmYzTmYQLp4lU\"\r\n",
        "!unzip txts.zip\r\n",
        "!git clone https://github.com/u1273400/rpynotes.git\r\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kenlm'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 13681 (delta 46), reused 60 (delta 24), pack-reused 13582\u001b[K\n",
            "Receiving objects: 100% (13681/13681), 5.54 MiB | 20.05 MiB/s, done.\n",
            "Resolving deltas: 100% (7852/7852), done.\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   program_options\n",
            "--   system\n",
            "--   thread\n",
            "--   unit_test_framework\n",
            "--   chrono\n",
            "--   date_time\n",
            "--   atomic\n",
            "-- Check if compiler accepts -pthread\n",
            "-- Check if compiler accepts -pthread - yes\n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/include (found version \"5.2.2\") \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_util\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 38%] Built target kenlm_util\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_filter\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target probing_hash_table_benchmark\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 52%] Built target kenlm_filter\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 62%] Built target probing_hash_table_benchmark\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "\u001b[35m\u001b[1mScanning dependencies of target build_binary\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target query\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_benchmark\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fragment\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 77%] Built target fragment\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_builder\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 82%] Built target query\n",
            "\u001b[35m\u001b[1mScanning dependencies of target phrase_table_vocab\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 87%] Built target phrase_table_vocab\n",
            "\u001b[35m\u001b[1mScanning dependencies of target filter\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 92%] Built target kenlm_benchmark\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 93%] Built target kenlm_builder\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lmplz\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target count_ngrams\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 97%] Built target filter\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n",
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[K     - 2.4MB 20.6MB/s\n",
            "\u001b[?25hBuilding wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp36-cp36m-linux_x86_64.whl size=2333253 sha256=6b2585bc576f20849051a05513f081b86d99199dcfacbde5c7dd0d4691c059b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cqwg6mtu/wheels/2d/32/73/e3093c9d11dc8abf79c156a4db1a1c5631428059d4f9ff2cba\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.0.0\n",
            "--2020-12-18 13:00:27--  https://onedrive.live.com/download?cid=EB20651D09521520&resid=EB20651D09521520%21171763&authkey=AAAmYzTmYQLp4lU\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ekou5w.am.files.1drv.com/y4mFmCDF2eSUJuYCvNUpabiswg6Bq0CFo7DmxseoJjuPA07u2aaLh7LoUjGeS_BMpnYggmJODrJ0dPjFj4KHgJczrV9WXuFdO9Yt_I_mSZj1MZ-aamqpcHBlO0ljQ1fAQ5STkUlOpryrfCdOkOEybapashdYdQuopGF4M2r4cf4AEXCg_vz3apXdrTYlkI6Waggp1oKmkqL1dvDUnwzA51n_Q/txts.zip?download&psid=1 [following]\n",
            "--2020-12-18 13:00:28--  https://ekou5w.am.files.1drv.com/y4mFmCDF2eSUJuYCvNUpabiswg6Bq0CFo7DmxseoJjuPA07u2aaLh7LoUjGeS_BMpnYggmJODrJ0dPjFj4KHgJczrV9WXuFdO9Yt_I_mSZj1MZ-aamqpcHBlO0ljQ1fAQ5STkUlOpryrfCdOkOEybapashdYdQuopGF4M2r4cf4AEXCg_vz3apXdrTYlkI6Waggp1oKmkqL1dvDUnwzA51n_Q/txts.zip?download&psid=1\n",
            "Resolving ekou5w.am.files.1drv.com (ekou5w.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to ekou5w.am.files.1drv.com (ekou5w.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1015212 (991K) [application/zip]\n",
            "Saving to: ‘txts.zip’\n",
            "\n",
            "txts.zip            100%[===================>] 991.42K  1.48MB/s    in 0.7s    \n",
            "\n",
            "2020-12-18 13:00:29 (1.48 MB/s) - ‘txts.zip’ saved [1015212/1015212]\n",
            "\n",
            "Archive:  txts.zip\n",
            "   creating: txts/\n",
            "  inflating: txts/acts_new.txt       \n",
            "  inflating: txts/gal_eph_new.txt    \n",
            "  inflating: txts/heb_new.txt        \n",
            "  inflating: txts/jam_jud_new.txt    \n",
            "  inflating: txts/john_new.txt       \n",
            "  inflating: txts/jud_rev_new.txt    \n",
            "  inflating: txts/luke_8_john_new.txt  \n",
            "  inflating: txts/mark01_new.txt     \n",
            "  inflating: txts/matt02_new.txt     \n",
            "  inflating: txts/matt_new.txt       \n",
            "  inflating: txts/phil_col_new.txt   \n",
            "  inflating: txts/thes_tim_new.txt   \n",
            "  inflating: txts/tit_phl_new.txt    \n",
            "Cloning into 'rpynotes'...\n",
            "remote: Enumerating objects: 749, done.\u001b[K\n",
            "remote: Counting objects: 100% (749/749), done.\u001b[K\n",
            "remote: Compressing objects: 100% (566/566), done.\u001b[K\n",
            "remote: Total 2560 (delta 307), reused 551 (delta 157), pack-reused 1811\u001b[K\n",
            "Receiving objects: 100% (2560/2560), 199.52 MiB | 32.38 MiB/s, done.\n",
            "Resolving deltas: 100% (910/910), done.\n",
            "Checking out files: 100% (1110/1110), done.\n",
            "Fri Dec 18 13:00:40 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJpKKi4JbYYh"
      },
      "source": [
        "#!apt-cache madison nvidia-cuda-dev\r\n",
        "!cp rpynotes/ok-rnn/txts -r rpynotes/ok-rnn/txteval\r\n",
        "!mkdir -p rpynotes/ok-rnn/txteval/val\r\n",
        "!mv rpynotes/ok-rnn/txteval/gal_eph_new.txt rpynotes/ok-rnn/txteval/val\r\n",
        "!cat rpynotes/ok-rnn/txteval/*.txt > rpynotes/ok-rnn/txteval/val/all.txt\r\n",
        "with open('rpynotes/ok-rnn/txteval/val/all.txt', encoding='utf-16') as f:\r\n",
        "  s=f.read()\r\n",
        "s=s.replace('.','\\n')\r\n",
        "with open('rpynotes/ok-rnn/txteval/val/all.txt', 'w') as f:\r\n",
        "  f.write(f'{s}')\r\n",
        "!kenlm/build/bin/lmplz -o 5 <rpynotes/ok-rnn/txteval/val/all.txt > rpynotes/ok-rnn/txteval/val/text.arpa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Fy-h58elwQ",
        "outputId": "2b72fe64-111b-4ab1-d9da-2b4f293d6524"
      },
      "source": [
        "!kenlm/build/bin/lmplz --discount_fallback -o 5 <rpynotes/ok-rnn/txteval/val/all.txt > rpynotes/ok-rnn/txteval/val/text.arpa"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/rpynotes/ok-rnn/txteval/val/all.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 1923129344 bytes == 0x560078bbe000 @  0x7fc3fbc4f1e7 0x560076f257a2 0x560076ec051e 0x560076e9f2eb 0x560076e8b066 0x7fc3f9de8bf7 0x560076e8cbaa\n",
            "tcmalloc: large alloc 8974589952 bytes == 0x5600eb5c8000 @  0x7fc3fbc4f1e7 0x560076f257a2 0x560076f147ca 0x560076f15208 0x560076e9f308 0x560076e8b066 0x7fc3f9de8bf7 0x560076e8cbaa\n",
            "****************************************************************************************************\n",
            "Unigram tokens 599916 types 11414\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:136968 2:1065633600 3:1998063104 4:3196900864 5:4662147072\n",
            "tcmalloc: large alloc 4662149120 bytes == 0x560078bbe000 @  0x7fc3fbc4f1e7 0x560076f257a2 0x560076f147ca 0x560076f15208 0x560076e9f8d7 0x560076e8b066 0x7fc3f9de8bf7 0x560076e8cbaa\n",
            "tcmalloc: large alloc 1998069760 bytes == 0x5601ce25a000 @  0x7fc3fbc4f1e7 0x560076f257a2 0x560076f147ca 0x560076f15208 0x560076e9fcdd 0x560076e8b066 0x7fc3f9de8bf7 0x560076e8cbaa\n",
            "tcmalloc: large alloc 3196903424 bytes == 0x560302d4a000 @  0x7fc3fbc4f1e7 0x560076f257a2 0x560076f147ca 0x560076f15208 0x560076e9fcdd 0x560076e8b066 0x7fc3f9de8bf7 0x560076e8cbaa\n",
            "Substituting fallback discounts for order 4: D1=0.5 D2=1 D3+=1.5\n",
            "Statistics:\n",
            "1 11414 D1=0.661239 D2=1.23023 D3+=1.28198\n",
            "2 75254 D1=0.719085 D2=1.26128 D3+=1.3872\n",
            "3 156591 D1=0.785233 D2=1.52296 D3+=1.2623\n",
            "4 197890 D1=0.831452 D2=1.75754 D3+=0.721526\n",
            "5 212530 D1=0.5 D2=1 D3+=1.5\n",
            "Memory estimate for binary LM:\n",
            "type       kB\n",
            "probing 14097 assuming -p 1.5\n",
            "probing 16660 assuming -r models -p 1.5\n",
            "trie     6418 without quantization\n",
            "trie     3363 assuming -q 8 -b 8 quantization \n",
            "trie     5857 assuming -a 22 array pointer compression\n",
            "trie     2802 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:136968 2:1204064 3:3131820 4:4749360 5:5950840\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:136968 2:1204064 3:3131820 4:4749360 5:5950840\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:14214456 kB\tVmRSS:1933916 kB\tRSSMax:1934072 kB\tuser:0.759233\tsys:0.909095\tCPU:1.66837\treal:1.5182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUdyasg3cD26"
      },
      "source": [
        "!cp rpynotes/ok-rnn/my_txtutils.py .\r\n",
        "from functools import reduce\r\n",
        "import my_txtutils as txt\r\n",
        "#import tensorflow.compat.v1 as tf\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "nb_epoch=10\r\n",
        "SEQLEN = 30\r\n",
        "BATCHSIZE = 200\r\n",
        "ALPHASIZE = txt.ALPHASIZE\r\n",
        "INTERNALSIZE = 256\r\n",
        "NLAYERS = 3\r\n",
        "learning_rate = 0.001  # fixed learning rate\r\n",
        "dropout_pkeep = 0.8    # some dropout"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLHH6UbcbTME",
        "outputId": "4ad6f365-6ba5-43ae-9e25-2bb353672acb"
      },
      "source": [
        "import kenlm\r\n",
        "with open('rpynotes/ok-rnn/txts/gal_eph_new.txt', encoding='utf-16') as f:\r\n",
        "  s=f.read()\r\n",
        "model = kenlm.Model('rpynotes/ok-rnn/txteval/val/text.arpa')\r\n",
        "print(model.perplexity(s))\r\n",
        "\r\n",
        "def cppx(scores, corpus):\r\n",
        "    nw=len(corpus.split())\r\n",
        "    nc=len(corpus)\r\n",
        "    c=math.exp(sum(scores)/nc)\r\n",
        "    print(c,nw,nc)\r\n",
        "    return 2**(c*nc/nw)\r\n",
        "\r\n",
        "def cppx2(scores, corpus):\r\n",
        "    nw=len(corpus.split())\r\n",
        "    nc=len(corpus)\r\n",
        "    ip=np.prod(1/np.array(scores))\r\n",
        "    c=ip**(1/nc)\r\n",
        "    print(scores)\r\n",
        "    print(c,nw,nc)\r\n",
        "    return 2**(c*nc/nw)\r\n",
        "  \r\n",
        "#cppx(get_scores(s),s)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "238.71607887366767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzaxS9zMX714"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "kcFEKWenX72I"
      },
      "source": [
        "#@title\n",
        "# encoding: UTF-8\n",
        "# Copyright 2017 Google.com\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import sys\n",
        "\n",
        "\n",
        "# Specification of the supported alphabet (subset of ASCII-7)\n",
        "# 10 line feed LF\n",
        "# 32-64 numbers and punctuation\n",
        "# 65-90 upper-case letters\n",
        "# 91-97 more punctuation\n",
        "# 97-122 lower-case letters\n",
        "# 123-126 more punctuation\n",
        "def convert_from_alphabet(a):\n",
        "    \"\"\"Encode a character\n",
        "    :param a: one character\n",
        "    :return: the encoded value\n",
        "    \"\"\"\n",
        "    if a == 9:\n",
        "        return 1\n",
        "    if a == 10:\n",
        "        return 127 - 30  # LF\n",
        "    elif 32 <= a <= 126:\n",
        "        return a - 30\n",
        "    else:\n",
        "        return 0  # unknown\n",
        "\n",
        "\n",
        "# encoded values:\n",
        "# unknown = 0\n",
        "# tab = 1\n",
        "# space = 2\n",
        "# all chars from 32 to 126 = c-30\n",
        "# LF mapped to 127-30\n",
        "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
        "    \"\"\"Decode a code point\n",
        "    :param c: code point\n",
        "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
        "    :return: decoded character\n",
        "    \"\"\"\n",
        "    if c == 1:\n",
        "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
        "    if c == 127 - 30:\n",
        "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
        "    if 32 <= c + 30 <= 126:\n",
        "        return c + 30\n",
        "    else:\n",
        "        return 0  # unknown\n",
        "\n",
        "\n",
        "def encode_text(s):\n",
        "    \"\"\"Encode a string.\n",
        "    :param s: a text string\n",
        "    :return: encoded list of code points\n",
        "    \"\"\"\n",
        "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
        "\n",
        "\n",
        "def decode_to_text(c, avoid_tab_and_lf=False):\n",
        "    \"\"\"Decode an encoded string.\n",
        "    :param c: encoded list of code points\n",
        "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
        "\n",
        "\n",
        "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
        "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
        "    according to the provided probabilities. If topn is specified, only the\n",
        "    topn highest probabilities are taken into account.\n",
        "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
        "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
        "    :return: a random integer\n",
        "    \"\"\"\n",
        "    p = np.squeeze(probabilities)\n",
        "    p[np.argsort(p)[:-topn]] = 0\n",
        "    p = p / np.sum(p)\n",
        "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
        "\n",
        "\n",
        "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
        "    \"\"\"\n",
        "    Divides the data into batches of sequences so that all the sequences in one batch\n",
        "    continue in the next batch. This is a generator that will keep returning batches\n",
        "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
        "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
        "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
        "    :param raw_data: the training text\n",
        "    :param batch_size: the size of a training minibatch\n",
        "    :param sequence_size: the unroll size of the RNN\n",
        "    :param nb_epochs: number of epochs to train on\n",
        "    :return:\n",
        "        x: one batch of training sequences\n",
        "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
        "        epoch: the current epoch number (starting at 0)\n",
        "    \"\"\"\n",
        "    data = np.array(raw_data)\n",
        "    data_len = data.shape[0]\n",
        "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
        "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
        "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
        "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
        "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
        "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        for batch in range(nb_batches):\n",
        "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
        "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
        "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
        "            y = np.roll(y, -epoch, axis=0)\n",
        "            yield x, y, epoch\n",
        "\n",
        "\n",
        "def find_book(index, bookranges):\n",
        "    return next(\n",
        "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
        "\n",
        "\n",
        "def find_book_index(index, bookranges):\n",
        "    return next(\n",
        "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
        "\n",
        "\n",
        "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
        "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
        "    print()\n",
        "    # epoch_size in number of batches\n",
        "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
        "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
        "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
        "    for k in range(batch_size):\n",
        "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
        "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
        "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
        "        bookname = find_book(index_in_epoch, bookranges)\n",
        "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
        "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
        "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
        "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
        "        print(print_string.format(decx, decy, loss_string))\n",
        "        index += sequence_len\n",
        "    # box formatting characters:\n",
        "    # │ \\u2502\n",
        "    # ─ \\u2500\n",
        "    # └ \\u2514\n",
        "    # ┘ \\u2518\n",
        "    # ┴ \\u2534\n",
        "    # ┌ \\u250C\n",
        "    # ┐ \\u2510\n",
        "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
        "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
        "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
        "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
        "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
        "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
        "    print(footer)\n",
        "    # print statistics\n",
        "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
        "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
        "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
        "    print()\n",
        "    print(\"TRAINING STATS: {}\".format(stats))\n",
        "\n",
        "\n",
        "class Progress:\n",
        "    \"\"\"Text mode progress bar.\n",
        "    Usage:\n",
        "            p = Progress(30)\n",
        "            p.step()\n",
        "            p.step()\n",
        "            p.step(start=True) # to restart form 0%\n",
        "    The progress bar displays a new header at each restart.\"\"\"\n",
        "    def __init__(self, maxi, size=100, msg=\"\"):\n",
        "        \"\"\"\n",
        "        :param maxi: the number of steps required to reach 100%\n",
        "        :param size: the number of characters taken on the screen by the progress bar\n",
        "        :param msg: the message displayed in the header of the progress bat\n",
        "        \"\"\"\n",
        "        self.maxi = maxi\n",
        "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
        "        self.header_printed = False\n",
        "        self.msg = msg\n",
        "        self.size = size\n",
        "\n",
        "    def step(self, reset=False):\n",
        "        if reset:\n",
        "            self.__init__(self.maxi, self.size, self.msg)\n",
        "        if not self.header_printed:\n",
        "            self.__print_header()\n",
        "        next(self.p)\n",
        "\n",
        "    def __print_header(self):\n",
        "        print()\n",
        "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
        "        print(format_string.format(self.msg))\n",
        "        self.header_printed = True\n",
        "\n",
        "    def __start_progress(self, maxi):\n",
        "        def print_progress():\n",
        "            # Bresenham's algorithm. Yields the number of dots printed.\n",
        "            # This will always print 100 dots in max invocations.\n",
        "            dx = maxi\n",
        "            dy = self.size\n",
        "            d = dy - dx\n",
        "            for x in range(maxi):\n",
        "                k = 0\n",
        "                while d >= 0:\n",
        "                    print('=', end=\"\", flush=True)\n",
        "                    k += 1\n",
        "                    d -= dx\n",
        "                d += dy\n",
        "                yield k\n",
        "\n",
        "        return print_progress\n",
        "\n",
        "\n",
        "def read_data_files(directory, validation=True):\n",
        "    \"\"\"Read data files according to the specified glob pattern\n",
        "    Optionnaly set aside the last file as validation data.\n",
        "    No validation data is returned if there are 5 files or less.\n",
        "    :param directory: for example \"data/*.txt\"\n",
        "    :param validation: if True (default), sets the last file aside as validation data\n",
        "    :return: training data, validation data, list of loaded file names with ranges\n",
        "     If validation is\n",
        "    \"\"\"\n",
        "    codetext = []\n",
        "    bookranges = []\n",
        "    shakelist = glob.glob(directory, recursive=True)\n",
        "    for shakefile in shakelist:\n",
        "        shaketext = open(shakefile, \"r\")\n",
        "        print(\"Loading file \" + shakefile)\n",
        "        start = len(codetext)\n",
        "        codetext.extend(encode_text(shaketext.read()))\n",
        "        end = len(codetext)\n",
        "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
        "        shaketext.close()\n",
        "\n",
        "    if len(bookranges) == 0:\n",
        "        sys.exit(\"No training data has been found. Aborting.\")\n",
        "\n",
        "    # For validation, use roughly 90K of text,\n",
        "    # but no more than 10% of the entire text\n",
        "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
        "\n",
        "    # 10% of the text is how many files ?\n",
        "    total_len = len(codetext)\n",
        "    validation_len = 0\n",
        "    nb_books1 = 0\n",
        "    for book in reversed(bookranges):\n",
        "        validation_len += book[\"end\"]-book[\"start\"]\n",
        "        nb_books1 += 1\n",
        "        if validation_len > total_len // 10:\n",
        "            break\n",
        "\n",
        "    # 90K of text is how many books ?\n",
        "    validation_len = 0\n",
        "    nb_books2 = 0\n",
        "    for book in reversed(bookranges):\n",
        "        validation_len += book[\"end\"]-book[\"start\"]\n",
        "        nb_books2 += 1\n",
        "        if validation_len > 90*1024:\n",
        "            break\n",
        "\n",
        "    # 20% of the books is how many books ?\n",
        "    nb_books3 = len(bookranges) // 5\n",
        "\n",
        "    # pick the smallest\n",
        "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
        "\n",
        "    if nb_books == 0 or not validation:\n",
        "        cutoff = len(codetext)\n",
        "    else:\n",
        "        cutoff = bookranges[-nb_books][\"start\"]\n",
        "    valitext = codetext[cutoff:]\n",
        "    codetext = codetext[:cutoff]\n",
        "    return codetext, valitext, bookranges\n",
        "\n",
        "\n",
        "def print_data_stats(datalen, valilen, epoch_size):\n",
        "    datalen_mb = datalen/1024.0/1024.0\n",
        "    valilen_kb = valilen/1024.0\n",
        "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
        "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
        "\n",
        "\n",
        "def print_validation_header(validation_start, bookranges):\n",
        "    bookindex = find_book_index(validation_start, bookranges)\n",
        "    books = ''\n",
        "    for i in range(bookindex, len(bookranges)):\n",
        "        books += bookranges[i][\"name\"]\n",
        "        if i < len(bookranges)-1:\n",
        "            books += \", \"\n",
        "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
        "\n",
        "\n",
        "def print_validation_stats(loss, accuracy):\n",
        "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
        "                                                                                                           accuracy))\n",
        "\n",
        "\n",
        "def print_text_generation_header():\n",
        "    print()\n",
        "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
        "\n",
        "\n",
        "def print_text_generation_footer():\n",
        "    print()\n",
        "    print(\"└{:─^111}┘\".format('End of generation'))\n",
        "\n",
        "\n",
        "def frequency_limiter(n, multiple=1, modulo=0):\n",
        "    def limit(i):\n",
        "        return i % (multiple * n) == modulo*multiple\n",
        "    return limit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSQjUaZbX72X",
        "outputId": "a510d327-f7d9-4828-ba5b-91c1497aae39"
      },
      "source": [
        "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
        "shakedir = \"txts/*.txt\"\n",
        "#shakedir = \"../tensorflow/**/*.py\"\n",
        "codetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading file txts/jam_jud_new.txt\n",
            "Loading file txts/acts_new.txt\n",
            "Loading file txts/gal_eph_new.txt\n",
            "Loading file txts/thes_tim_new.txt\n",
            "Loading file txts/jud_rev_new.txt\n",
            "Loading file txts/luke_8_john_new.txt\n",
            "Loading file txts/tit_phl_new.txt\n",
            "Loading file txts/phil_col_new.txt\n",
            "Loading file txts/heb_new.txt\n",
            "Loading file txts/matt_new.txt\n",
            "Loading file txts/matt02_new.txt\n",
            "Loading file txts/mark01_new.txt\n",
            "Loading file txts/john_new.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERlAnhVkX72h",
        "outputId": "da56affa-7bf7-4be3-e7b4-6b3c3a693d55"
      },
      "source": [
        "# display some stats on the data\n",
        "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
        "txt.print_data_stats(len(codetext), len(valitext), epoch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training text size is 2.83MB with 283.09KB set aside for validation. There will be 493 batches per epoch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou34FsIhX72k",
        "outputId": "55a106d9-112c-4540-c6b9-1c41d8ccb32b"
      },
      "source": [
        "# model\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, layers):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.gru = nn.GRU(input_size, hidden_size, layers)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.fc_layer = nn.Linear(hidden_size, ALPHASIZE)        \n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # combined = torch.cat((input, hidden), 1)\n",
        "        output, hidden = self.gru(input, hidden)\n",
        "        #print(f'gos={output.size()}, is={line_tensor.size()}')\n",
        "        output = self.fc_layer(output)\n",
        "        #print(f'fc={output.size()}, ls={line_tensor.size()}')\n",
        "        output = self.softmax(output)\n",
        "        #print(f'sm={output.size()}, cs[2:]={category_tensor.size()}')\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(NLAYERS, BATCHSIZE, self.hidden_size, device=device)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gru = GRU(ALPHASIZE, INTERNALSIZE, NLAYERS)\n",
        "gru.to(device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRU(\n",
              "  (gru): GRU(98, 256, num_layers=3)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              "  (fc_layer): Linear(in_features=256, out_features=98, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AxIxrljX72m"
      },
      "source": [
        "# loss fn\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "#from ok_seq2seq import EncoderRNN \\\n",
        "#                        ,DecoderRNN \\\n",
        "#                        ,AttnDecoderRNN \\\n",
        "#                        ,evaluateRandomly \\\n",
        "#                        ,teacher_forcing_ratio \n",
        "# Parameters\n",
        "# input_size – The number of expected features in the input x\n",
        "# hidden_size – The number of features in the hidden state h\n",
        "# num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, \n",
        "#     with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\n",
        "# bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "# batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
        "# dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
        "# bidirectional – If True, becomes a bidirectional GRU. Default: False\n",
        "\n",
        "# rnn = nn.GRU(10, 20, 2)\n",
        "# input = torch.randn(5, 3, 10)\n",
        "# h0 = torch.randn(2, 3, 20)\n",
        "# output, hn = rnn(input, h0)\n",
        "\n",
        "# input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. \n",
        "#     The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details.\n",
        "\n",
        "# h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. \n",
        "#     Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1\n",
        "    \n",
        "# output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. \n",
        "#     If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. \n",
        "#     For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\n",
        "#     Similarly, the directions can be separated in the packed case.\n",
        "\n",
        "# h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
        "#     Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).\n",
        "    \n",
        "# Shape:\n",
        "# Input: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K)(N,C,d \n",
        "# Target: (N)(N) where each value is 0 \\leq \\text{targets}[i] \\leq C-10≤targets[i]≤C−1 , or (N, d_1, d_2, ..., d_K)(N,d \n",
        "# Output: scalar. If reduction is 'none', then the same size as the target: (N)(N) , or (N, d_1, d_2, ..., d_K)(N,d "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw6jZeiJX72o"
      },
      "source": [
        "# training fn\n",
        "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
        "\n",
        "def train(category_tensor, line_tensor):\n",
        "    hidden = gru.initHidden()\n",
        "    gru.zero_grad()\n",
        "    \n",
        "    output, hidden = gru(line_tensor, hidden)\n",
        "    input=output.transpose(0,1).transpose(1,2)\n",
        "    loss = criterion(input, category_tensor) # N (batch),C\n",
        "    loss.backward()\n",
        "\n",
        "    # Add parameters' gradients to their values, multiplied by learning rate\n",
        "    for p in gru.parameters():\n",
        "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "    return output.transpose(0,1), loss.item()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJUBYp_fX72q"
      },
      "source": [
        "def mb2t(rows):\n",
        "    rows=rows.transpose()\n",
        "    tensor = torch.zeros(rows.shape[0], rows.shape[1], ALPHASIZE, device=device)\n",
        "    for i, row in enumerate(rows):\n",
        "        for j, letter_code in enumerate(row):\n",
        "            tensor[i][j][letter_code] = 1\n",
        "    return tensor\n",
        "\n",
        "def lin2txt(lt):\n",
        "    return ''.join([chr(txt.convert_to_alphabet(c))  if c != 0 else '' for c in lt])\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq8_RvByX72q"
      },
      "source": [
        "# init train\n",
        "\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "# product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
        "\n",
        "\n",
        "print_every = 250\n",
        "plot_every = 500\n",
        "\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "iter=0\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rIVgyYOtX72t",
        "outputId": "d06fca6d-a689-4712-d532-2f740c0d227e"
      },
      "source": [
        "# training\n",
        "start = time.time()\n",
        "plt.figure()\n",
        "vloss=[]\n",
        "VALI_SEQLEN = 30\n",
        "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=nb_epoch):\n",
        "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    category =  [lin2txt(l) for l in y_]\n",
        "    lines = [lin2txt(l) for l in x]\n",
        "    line_tensor=mb2t(x)\n",
        "    output, loss = train(torch.tensor(y_, device=device, dtype=torch.long), line_tensor)\n",
        "    current_loss += loss\n",
        "\n",
        "    # Print iter number, loss, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess = [lin2txt([ch.argmax(dim=0) for ch in line]) for line in output]\n",
        "        for i in range(2):\n",
        "            correct = '✓' if guess[i] == category[i] else '✗ %s' % category[i] \n",
        "            print(f'epoch {epoch} of  {nb_epoch} ({timeSince(start)}) {loss} {lines[i]}({len(lines[i])}) / {guess[i]}({len(guess[i])}) {correct}' )\n",
        " \n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0 and len(valitext) > 0:\n",
        "        all_losses.append(current_loss / plot_every)\n",
        "        plt.plot(all_losses)\n",
        "        current_loss = 0\n",
        "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, BATCHSIZE, VALI_SEQLEN, 1))  # all data in 1 batch\n",
        "        line_tensor = mb2t(vali_x)\n",
        "        output, loss = train(torch.tensor(vali_y, device=device, dtype=torch.long), line_tensor)\n",
        "        vloss.append(loss)\n",
        "        plt.plot(vloss)      \n",
        "    iter += 1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 of  10 (0m 2s) 5.298305988311768 jems jems be barabo firifirima(30) / =,===,=DD>>%nnn\t\t______(23) ✗ ems jems be barabo firifirimad\n",
            "epoch 0 of  10 (0m 2s) 5.298305988311768 o yekarifi ma o ye bereni ari (30) / ???AA``>&_&]&&..`AAA00AAc>>>(28) ✗  yekarifi ma o ye bereni ari s\n",
            "epoch 0 of  10 (0m 57s) 5.298327922821045 e miebo bara nwose ori okime. (30) / ,,AE6nnn>>>>>{{{AAnnn??]]EO(27) ✗  miebo bara nwose ori okime. a\n",
            "epoch 0 of  10 (0m 57s) 5.298327922821045 egoye inibo bietonye inibo gba(30) / ,lL``AAA~__nnnt__||TAA\n",
            "\n",
            "__n?t5(30) ✗ goye inibo bietonye inibo gbar\n",
            "epoch 1 of  10 (1m 51s) 5.29824686050415  sime i piri eke.    ominea bi(30) / .g',]]OOOO,,,,,rrKKKKiic99(26) ✗ sime i piri eke.    ominea bie\n",
            "epoch 1 of  10 (1m 51s) 5.29824686050415 mi konka siki. anierechi ini d(30) / N]]]]??[[@{''OOO&&&&666FFZZ]]](30) ✗ i konka siki. anierechi ini du\n",
            "epoch 1 of  10 (2m 45s) 5.298155784606934    se fiafiadiri mi jin anga b(30) / ....AAA22i__****]]]]]]~~.~559(29) ✗   se fiafiadiri mi jin anga bu\n",
            "epoch 1 of  10 (2m 45s) 5.298155784606934 am nwo be ye mi.  min bara mi (30) / &L..{T?;;0;AAAAA\n",
            "KKK\n",
            "\t9>>>>>.(29) ✗ m nwo be ye mi.  min bara mi b\n",
            "epoch 2 of  10 (3m 39s) 5.2981648445129395 oni  piriye-e ani inia ojumini(30) / ??o\n",
            "\n",
            "nw>&_OOFFA0cc'&&&&&???))|(30) ✗ ni  piriye-e ani inia ojumini \n",
            "epoch 2 of  10 (3m 39s) 5.2981648445129395 amuno be ene mi koruari siki o(30) / &+e**??99gAAAA\n",
            "\n",
            "??====&'''''](29) ✗ muno be ene mi koruari siki o \n",
            "epoch 2 of  10 (4m 32s) 5.298130989074707  se ominea bie korombia, omine(30) / .g,AAA||c9990cc4====+aa^^]]](28) ✗ se ominea bie korombia, omine \n",
            "epoch 2 of  10 (4m 32s) 5.298130989074707  kokomaye mi bie iruoye ofori-(30) / .c????=++```.\t\t]]]d'O`````??`=(30) ✗ kokomaye mi bie iruoye ofori-e\n",
            "epoch 3 of  10 (5m 26s) 5.298118591308594  ani wa sime kuroari ye-e iwo (30) / ..>>>>>>mmh,,,====H##AAAAdd?(28) ✗ ani wa sime kuroari ye-e iwo s\n",
            "epoch 3 of  10 (5m 26s) 5.298118591308594 iatibi tamuno be tatari wa bel(30) / &&&____]OLJJJJJ9gg922S>>>>>>DR(30) ✗ atibi tamuno be tatari wa bele\n",
            "epoch 3 of  10 (6m 19s) 5.298104763031006 o goyegoye piki dabo be sikime(30) / ?n???`````AAvvOOWW44nnn00{''OE(30) ✗  goyegoye piki dabo be sikime \n",
            "epoch 3 of  10 (6m 19s) 5.298104763031006  kraist be bereni minapu ma bo(30) / nr[>>&Ymm/DDDRH\n",
            "\n",
            "\n",
            "*&eGJJJ::(27) ✗ kraist be bereni minapu ma bo \n",
            "epoch 4 of  10 (7m 12s) 5.298000812530518 ase ori orisa bo be o nwo bele(30) / &+/,AAnnnnn'++ennn00nnn????;RR(30) ✗ se ori orisa bo be o nwo belem\n",
            "epoch 4 of  10 (7m 12s) 5.298000812530518  minapu tamuno be min tomoniki(30) / n..]&&eGJJJJJJJJ9gg\n",
            "\n",
            "]]]=|]](28) ✗ minapu tamuno be min tomonikir\n",
            "epoch 4 of  10 (8m 5s) 5.297973155975342 nwo da. ibiye mie bo goyegoye (30) / k33???+BBB__44EOE,66nn??`````A(30) ✗ wo da. ibiye mie bo goyegoye s\n",
            "epoch 4 of  10 (8m 5s) 5.297973155975342 e, boka siki nwose sime se was(30) / ,llDp:::{{'''\n",
            "oo{ggg0{(22) ✗ , boka siki nwose sime se wasa\n",
            "epoch 5 of  10 (8m 57s) 5.297977924346924  osike tapu o punuma se o fono(30) / nnnn',,E22IJJJJ:::J:J{{{ggA???(30) ✗ osike tapu o punuma se o fono \n",
            "epoch 5 of  10 (8m 57s) 5.297977924346924 yebusoka papa dumo mi bie o pe(30) / `AT`3``+v::::::y:?```J]]]]]nng(30) ✗ ebusoka papa dumo mi bie o pek\n",
            "epoch 5 of  10 (9m 49s) 5.297939300537109  bara mi kurake.  aniokuma, nw(30) / nnD>>>>>.ccCG II\\Zrcc>???aaJ.{(30) ✗ bara mi kurake.  aniokuma, nwo\n",
            "epoch 5 of  10 (9m 49s) 5.297939300537109 ma o nwo mie fiafiama, miese i(30) / NL..Jn????i]]iiii___&all]]E,,,(30) ✗ a o nwo mie fiafiama, miese in\n",
            "epoch 6 of  10 (10m 40s) 5.29793119430542 u sime bo be. tomonikiri m ma (30) / vggv`n0\"AABB``|||||]]]]..(25) ✗  sime bo be. tomonikiri m ma t\n",
            "epoch 6 of  10 (10m 40s) 5.29793119430542 iruo ye dieapu ma . kraist be (30) / &&O  ``AAAAIIIIM:J:J::>>&&mmqR(30) ✗ ruo ye dieapu ma . kraist be k\n",
            "epoch 6 of  10 (11m 32s) 5.297878742218018 , ibi ye mie gose torusiori mi(30) / l<.\t_\t\t,,A.,zz8ggg8888eO= ||](29) ✗  ibi ye mie gose torusiori mi \n",
            "epoch 6 of  10 (11m 32s) 5.297878742218018 iki ngisi, tonbu sime ikiankor(30) / &]]]]]]]]OOOOooo({{e]]O&   (27) ✗ ki ngisi, tonbu sime ikiankoro\n",
            "epoch 7 of  10 (12m 25s) 5.297846794128418 jizos kraist be karakara mi bu(30) / ==LLLvvSS +OYe/gDD2>>>>>>yyyn9(30) ✗ izos kraist be karakara mi bu \n",
            "epoch 7 of  10 (12m 25s) 5.297846794128418 omine o nemime. a diri gien om(30) / ?`]]]]]cccA]E,,cAKiHHjj]]]]](28) ✗ mine o nemime. a diri gien omi\n",
            "epoch 7 of  10 (13m 16s) 5.297806739807129 e, ojumini mi dikipukosolia ye(30) / ,ll:::)))||]]]]]]]]_   ==++JJJ(30) ✗ , ojumini mi dikipukosolia ye \n",
            "epoch 7 of  10 (13m 16s) 5.297806739807129 be wa no belemame now se yeke,(30) / TI0000>>nn;RRRREEcc1?{{gAAAAA(29) ✗ e wa no belemame now se yeke, \n",
            "epoch 8 of  10 (14m 7s) 5.297790050506592 aki siki ngisni sime bo be omi(30) / &l&&m''']]]]]**]]']]]nnn0n``(28) ✗ ki siki ngisni sime bo be omin\n",
            "epoch 8 of  10 (14m 7s) 5.297790050506592  mi i piki oju bu so oko kuro.(30) / n...]\t\t]]]]]o?JJ?Dg{gg??????==(30) ✗ mi i piki oju bu so oko kuro. \n",
            "epoch 8 of  10 (14m 58s) 5.29775857925415 jinbojinbo belemame nwo bebe, (30) / ===_oooooooooRRREEEc111;000II(29) ✗ inbojinbo belemame nwo bebe, t\n",
            "epoch 8 of  10 (14m 58s) 5.29775857925415 omaka bo beke, okuma ani se mi(30) / ?`LL !!ynnnpIIIpp:``JJ.>>m,,A(29) ✗ maka bo beke, okuma ani se mie\n",
            "epoch 9 of  10 (15m 48s) 5.297724723815918  oforie bo naa na iya awo ma k(30) / nn???i||nnnn>>>>>>>>>>>>yB`yv(29) ✗ oforie bo naa na iya awo ma ko\n",
            "epoch 9 of  10 (15m 48s) 5.297724723815918 na tuburu, anisiki ori o diki (30) /  3>9999}eeeD{{{&& ]]] nnnn??4](30) ✗ a tuburu, anisiki ori o diki m\n",
            "epoch 9 of  10 (16m 39s) 5.297654151916504 obori se obu chinabe  fiafia p(30) / ???` nn,An`ggg/4+T9900\tiii&&&(29) ✗ bori se obu chinabe  fiafia pi\n",
            "epoch 9 of  10 (16m 39s) 5.297654151916504 ni weri bobia bara na nwo doki(30) /   \n",
            "\n",
            "*> C\tnnoann9>>>>>>>>>?????(30) ✗ i weri bobia bara na nwo dokim\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa9XoPS1X72v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}