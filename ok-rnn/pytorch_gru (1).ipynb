{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "pytorch_gru.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXon3UzDNMJP"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn3J_6wsCPu_",
        "outputId": "60976379-4e33-4c35-9b7c-aa2571a0b41d"
      },
      "source": [
        "!!cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cat: /usr/local/cuda/include/cudnn.h: No such file or directory']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eg98sgCNaGx",
        "outputId": "5cae79cb-235f-47a3-88ed-5e6e1b826eb7"
      },
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip\r\n",
        "!wget --no-check-certificate -c -O txts.zip \"https://onedrive.live.com/download?cid=EB20651D09521520&resid=EB20651D09521520%21171763&authkey=AAAmYzTmYQLp4lU\"\r\n",
        "!unzip txts.zip\r\n",
        "!git clone https://github.com/u1273400/rpynotes.git\r\n",
        "!cp rpynotes/ok-rnn/my_txtutils.py .\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[K     | 2.5MB 6.7MB/s\n",
            "\u001b[?25hBuilding wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp36-cp36m-linux_x86_64.whl size=2333092 sha256=c7a402983ca5a550a310ba93b3ea3148ed62168f47d6c3996fa7b6a5224a2c5f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-001xf8e_/wheels/2d/32/73/e3093c9d11dc8abf79c156a4db1a1c5631428059d4f9ff2cba\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.0.0\n",
            "--2020-12-19 22:07:16--  https://onedrive.live.com/download?cid=EB20651D09521520&resid=EB20651D09521520%21171763&authkey=AAAmYzTmYQLp4lU\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ekou5w.am.files.1drv.com/y4ms7o4He00PteVlja42LG1D9levdYZaDcAxzCPZn6swkj5GPxox9zxniXGxWYSfIJH-vqJz2br7TIdN1qGCOuxjQTVgNcjEKtU2p_vyEoYhAbIwVdDJtu-B9p4JxP53qyDP45zkJyo04AYIAocHssj8Ko4xvxPLo-eiTEh_smW1zltUxpLtio-PmqZoICHRk75f9cUQ8YsMfWkIJie7TZsfA/txts.zip?download&psid=1 [following]\n",
            "--2020-12-19 22:07:18--  https://ekou5w.am.files.1drv.com/y4ms7o4He00PteVlja42LG1D9levdYZaDcAxzCPZn6swkj5GPxox9zxniXGxWYSfIJH-vqJz2br7TIdN1qGCOuxjQTVgNcjEKtU2p_vyEoYhAbIwVdDJtu-B9p4JxP53qyDP45zkJyo04AYIAocHssj8Ko4xvxPLo-eiTEh_smW1zltUxpLtio-PmqZoICHRk75f9cUQ8YsMfWkIJie7TZsfA/txts.zip?download&psid=1\n",
            "Resolving ekou5w.am.files.1drv.com (ekou5w.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to ekou5w.am.files.1drv.com (ekou5w.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1015212 (991K) [application/zip]\n",
            "Saving to: ‘txts.zip’\n",
            "\n",
            "txts.zip            100%[===================>] 991.42K   922KB/s    in 1.1s    \n",
            "\n",
            "2020-12-19 22:07:19 (922 KB/s) - ‘txts.zip’ saved [1015212/1015212]\n",
            "\n",
            "Archive:  txts.zip\n",
            "   creating: txts/\n",
            "  inflating: txts/acts_new.txt       \n",
            "  inflating: txts/gal_eph_new.txt    \n",
            "  inflating: txts/heb_new.txt        \n",
            "  inflating: txts/jam_jud_new.txt    \n",
            "  inflating: txts/john_new.txt       \n",
            "  inflating: txts/jud_rev_new.txt    \n",
            "  inflating: txts/luke_8_john_new.txt  \n",
            "  inflating: txts/mark01_new.txt     \n",
            "  inflating: txts/matt02_new.txt     \n",
            "  inflating: txts/matt_new.txt       \n",
            "  inflating: txts/phil_col_new.txt   \n",
            "  inflating: txts/thes_tim_new.txt   \n",
            "  inflating: txts/tit_phl_new.txt    \n",
            "Cloning into 'rpynotes'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 2668 (delta 50), reused 71 (delta 30), pack-reused 2566\u001b[K\n",
            "Receiving objects: 100% (2668/2668), 205.41 MiB | 34.49 MiB/s, done.\n",
            "Resolving deltas: 100% (1014/1014), done.\n",
            "Checking out files: 100% (1108/1108), done.\n",
            "cp: missing destination file operand after 'rpynotes/ok-rnn/my_txtutils.py'\n",
            "Try 'cp --help' for more information.\n",
            "Sat Dec 19 22:07:28 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQjt3TzHNMJZ"
      },
      "source": [
        "# encoding: UTF-8\n",
        "# Copyright 2017 Google.com\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from tensorflow.contrib import layers\n",
        "#from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import my_txtutils as txt\n",
        "import json\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq4pWpihNMJb"
      },
      "source": [
        "\n",
        "# model parameters\n",
        "#\n",
        "# Usage:\n",
        "#   Training only:\n",
        "#         Leave all the parameters as they are\n",
        "#         Disable validation to run a bit faster (set validation=False below)\n",
        "#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n",
        "#   Training and experimentation (default):\n",
        "#         Keep validation enabled\n",
        "#         You can now play with the parameters anf follow the effects in Tensorboard\n",
        "#         A good choice of parameters ensures that the testing and validation curves stay close\n",
        "#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n",
        "#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n",
        "#\n",
        "nb_epoch=60\n",
        "SEQLEN = 30\n",
        "BATCHSIZE = 200\n",
        "VALI_SEQLEN = SEQLEN\n",
        "ALPHASIZE = txt.ALPHASIZE\n",
        "INTERNALSIZE = 512\n",
        "NLAYERS = 3\n",
        "learning_rate = 0.001  # fixed learning rate\n",
        "dropout_pkeep = 0.8    # some dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "cellView": "form",
        "id": "u1osiVsZNMJc"
      },
      "source": [
        "#@title\n",
        "# encoding: UTF-8\n",
        "# Copyright 2017 Google.com\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import sys\n",
        "\n",
        "# size of the alphabet that we work with\n",
        "ALPHASIZE = 98\n",
        "\n",
        "\n",
        "# Specification of the supported alphabet (subset of ASCII-7)\n",
        "# 10 line feed LF\n",
        "# 32-64 numbers and punctuation\n",
        "# 65-90 upper-case letters\n",
        "# 91-97 more punctuation\n",
        "# 97-122 lower-case letters\n",
        "# 123-126 more punctuation\n",
        "def convert_from_alphabet(a):\n",
        "    \"\"\"Encode a character\n",
        "    :param a: one character\n",
        "    :return: the encoded value\n",
        "    \"\"\"\n",
        "    if a == 9:\n",
        "        return 1\n",
        "    if a == 10:\n",
        "        return 127 - 30  # LF\n",
        "    elif 32 <= a <= 126:\n",
        "        return a - 30\n",
        "    else:\n",
        "        return 0  # unknown\n",
        "\n",
        "\n",
        "# encoded values:\n",
        "# unknown = 0\n",
        "# tab = 1\n",
        "# space = 2\n",
        "# all chars from 32 to 126 = c-30\n",
        "# LF mapped to 127-30\n",
        "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
        "    \"\"\"Decode a code point\n",
        "    :param c: code point\n",
        "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
        "    :return: decoded character\n",
        "    \"\"\"\n",
        "    if c == 1:\n",
        "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
        "    if c == 127 - 30:\n",
        "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
        "    if 32 <= c + 30 <= 126:\n",
        "        return c + 30\n",
        "    else:\n",
        "        return 0  # unknown\n",
        "\n",
        "\n",
        "def encode_text(s):\n",
        "    \"\"\"Encode a string.\n",
        "    :param s: a text string\n",
        "    :return: encoded list of code points\n",
        "    \"\"\"\n",
        "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
        "\n",
        "\n",
        "def decode_to_text(c, avoid_tab_and_lf=False):\n",
        "    \"\"\"Decode an encoded string.\n",
        "    :param c: encoded list of code points\n",
        "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
        "\n",
        "\n",
        "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
        "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
        "    according to the provided probabilities. If topn is specified, only the\n",
        "    topn highest probabilities are taken into account.\n",
        "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
        "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
        "    :return: a random integer\n",
        "    \"\"\"\n",
        "    p = np.squeeze(probabilities)\n",
        "    p[np.argsort(p)[:-topn]] = 0\n",
        "    p = p / np.sum(p)\n",
        "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
        "\n",
        "\n",
        "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
        "    \"\"\"\n",
        "    Divides the data into batches of sequences so that all the sequences in one batch\n",
        "    continue in the next batch. This is a generator that will keep returning batches\n",
        "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
        "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
        "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
        "    :param raw_data: the training text\n",
        "    :param batch_size: the size of a training minibatch\n",
        "    :param sequence_size: the unroll size of the RNN\n",
        "    :param nb_epochs: number of epochs to train on\n",
        "    :return:\n",
        "        x: one batch of training sequences\n",
        "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
        "        epoch: the current epoch number (starting at 0)\n",
        "    \"\"\"\n",
        "    data = np.array(raw_data)\n",
        "    data_len = data.shape[0]\n",
        "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
        "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
        "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
        "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
        "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
        "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        for batch in range(nb_batches):\n",
        "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
        "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
        "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
        "            y = np.roll(y, -epoch, axis=0)\n",
        "            yield x, y, epoch\n",
        "\n",
        "\n",
        "def find_book(index, bookranges):\n",
        "    return next(\n",
        "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
        "\n",
        "\n",
        "def find_book_index(index, bookranges):\n",
        "    return next(\n",
        "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
        "\n",
        "\n",
        "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
        "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
        "    print()\n",
        "    # epoch_size in number of batches\n",
        "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
        "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
        "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
        "    for k in range(batch_size):\n",
        "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
        "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
        "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
        "        bookname = find_book(index_in_epoch, bookranges)\n",
        "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
        "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
        "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
        "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
        "        print(print_string.format(decx, decy, loss_string))\n",
        "        index += sequence_len\n",
        "    # box formatting characters:\n",
        "    # │ \\u2502\n",
        "    # ─ \\u2500\n",
        "    # └ \\u2514\n",
        "    # ┘ \\u2518\n",
        "    # ┴ \\u2534\n",
        "    # ┌ \\u250C\n",
        "    # ┐ \\u2510\n",
        "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
        "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
        "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
        "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
        "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
        "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
        "    print(footer)\n",
        "    # print statistics\n",
        "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
        "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
        "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
        "    print()\n",
        "    print(\"TRAINING STATS: {}\".format(stats))\n",
        "\n",
        "\n",
        "class Progress:\n",
        "    \"\"\"Text mode progress bar.\n",
        "    Usage:\n",
        "            p = Progress(30)\n",
        "            p.step()\n",
        "            p.step()\n",
        "            p.step(start=True) # to restart form 0%\n",
        "    The progress bar displays a new header at each restart.\"\"\"\n",
        "    def __init__(self, maxi, size=100, msg=\"\"):\n",
        "        \"\"\"\n",
        "        :param maxi: the number of steps required to reach 100%\n",
        "        :param size: the number of characters taken on the screen by the progress bar\n",
        "        :param msg: the message displayed in the header of the progress bat\n",
        "        \"\"\"\n",
        "        self.maxi = maxi\n",
        "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
        "        self.header_printed = False\n",
        "        self.msg = msg\n",
        "        self.size = size\n",
        "\n",
        "    def step(self, reset=False):\n",
        "        if reset:\n",
        "            self.__init__(self.maxi, self.size, self.msg)\n",
        "        if not self.header_printed:\n",
        "            self.__print_header()\n",
        "        next(self.p)\n",
        "\n",
        "    def __print_header(self):\n",
        "        print()\n",
        "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
        "        print(format_string.format(self.msg))\n",
        "        self.header_printed = True\n",
        "\n",
        "    def __start_progress(self, maxi):\n",
        "        def print_progress():\n",
        "            # Bresenham's algorithm. Yields the number of dots printed.\n",
        "            # This will always print 100 dots in max invocations.\n",
        "            dx = maxi\n",
        "            dy = self.size\n",
        "            d = dy - dx\n",
        "            for x in range(maxi):\n",
        "                k = 0\n",
        "                while d >= 0:\n",
        "                    print('=', end=\"\", flush=True)\n",
        "                    k += 1\n",
        "                    d -= dx\n",
        "                d += dy\n",
        "                yield k\n",
        "\n",
        "        return print_progress\n",
        "\n",
        "\n",
        "def read_data_files(directory, validation=True):\n",
        "    \"\"\"Read data files according to the specified glob pattern\n",
        "    Optionnaly set aside the last file as validation data.\n",
        "    No validation data is returned if there are 5 files or less.\n",
        "    :param directory: for example \"data/*.txt\"\n",
        "    :param validation: if True (default), sets the last file aside as validation data\n",
        "    :return: training data, validation data, list of loaded file names with ranges\n",
        "     If validation is\n",
        "    \"\"\"\n",
        "    codetext = []\n",
        "    bookranges = []\n",
        "    shakelist = glob.glob(directory, recursive=True)\n",
        "    for shakefile in shakelist:\n",
        "        shaketext = open(shakefile, \"r\")\n",
        "        print(\"Loading file \" + shakefile)\n",
        "        start = len(codetext)\n",
        "        codetext.extend(encode_text(shaketext.read()))\n",
        "        end = len(codetext)\n",
        "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
        "        shaketext.close()\n",
        "\n",
        "    if len(bookranges) == 0:\n",
        "        sys.exit(\"No training data has been found. Aborting.\")\n",
        "\n",
        "    # For validation, use roughly 90K of text,\n",
        "    # but no more than 10% of the entire text\n",
        "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
        "\n",
        "    # 10% of the text is how many files ?\n",
        "    total_len = len(codetext)\n",
        "    validation_len = 0\n",
        "    nb_books1 = 0\n",
        "    for book in reversed(bookranges):\n",
        "        validation_len += book[\"end\"]-book[\"start\"]\n",
        "        nb_books1 += 1\n",
        "        if validation_len > total_len // 10:\n",
        "            break\n",
        "\n",
        "    # 90K of text is how many books ?\n",
        "    validation_len = 0\n",
        "    nb_books2 = 0\n",
        "    for book in reversed(bookranges):\n",
        "        validation_len += book[\"end\"]-book[\"start\"]\n",
        "        nb_books2 += 1\n",
        "        if validation_len > 90*1024:\n",
        "            break\n",
        "\n",
        "    # 20% of the books is how many books ?\n",
        "    nb_books3 = len(bookranges) // 5\n",
        "\n",
        "    # pick the smallest\n",
        "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
        "\n",
        "    if nb_books == 0 or not validation:\n",
        "        cutoff = len(codetext)\n",
        "    else:\n",
        "        cutoff = bookranges[-nb_books][\"start\"]\n",
        "    valitext = codetext[cutoff:]\n",
        "    codetext = codetext[:cutoff]\n",
        "    return codetext, valitext, bookranges\n",
        "\n",
        "\n",
        "def print_data_stats(datalen, valilen, epoch_size):\n",
        "    datalen_mb = datalen/1024.0/1024.0\n",
        "    valilen_kb = valilen/1024.0\n",
        "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
        "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
        "\n",
        "\n",
        "def print_validation_header(validation_start, bookranges):\n",
        "    bookindex = find_book_index(validation_start, bookranges)\n",
        "    books = ''\n",
        "    for i in range(bookindex, len(bookranges)):\n",
        "        books += bookranges[i][\"name\"]\n",
        "        if i < len(bookranges)-1:\n",
        "            books += \", \"\n",
        "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
        "\n",
        "\n",
        "def print_validation_stats(loss, accuracy):\n",
        "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
        "                                                                                                           accuracy))\n",
        "\n",
        "\n",
        "def print_text_generation_header():\n",
        "    print()\n",
        "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
        "\n",
        "\n",
        "def print_text_generation_footer():\n",
        "    print()\n",
        "    print(\"└{:─^111}┘\".format('End of generation'))\n",
        "\n",
        "\n",
        "def frequency_limiter(n, multiple=1, modulo=0):\n",
        "    def limit(i):\n",
        "        return i % (multiple * n) == modulo*multiple\n",
        "    return limit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTbnELiONMJl",
        "outputId": "54c3a0fe-efd6-4d8b-f5d0-0e66bf1c64e9"
      },
      "source": [
        "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
        "shakedir = \"txts/*.txt\"\n",
        "#shakedir = \"../tensorflow/**/*.py\"\n",
        "codetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading file txts/jam_jud_new.txt\n",
            "Loading file txts/acts_new.txt\n",
            "Loading file txts/gal_eph_new.txt\n",
            "Loading file txts/thes_tim_new.txt\n",
            "Loading file txts/jud_rev_new.txt\n",
            "Loading file txts/luke_8_john_new.txt\n",
            "Loading file txts/tit_phl_new.txt\n",
            "Loading file txts/phil_col_new.txt\n",
            "Loading file txts/heb_new.txt\n",
            "Loading file txts/matt_new.txt\n",
            "Loading file txts/matt02_new.txt\n",
            "Loading file txts/mark01_new.txt\n",
            "Loading file txts/john_new.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnQShJG7NMJq",
        "outputId": "54c54d3e-5e11-4d7e-c888-19218a52cdea"
      },
      "source": [
        "# display some stats on the data\n",
        "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
        "txt.print_data_stats(len(codetext), len(valitext), epoch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training text size is 2.83MB with 283.09KB set aside for validation. There will be 493 batches per epoch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfHf56rYNMJr",
        "outputId": "394de08f-72d9-4796-d394-f12f58340af0"
      },
      "source": [
        "# model\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.GRUCell(input_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(input,hidden)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(BATCHSIZE, self.hidden_size, device=device)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('using gpu..' if torch.cuda.is_available() else 'using cpu..')\n",
        "rnn = RNN(ALPHASIZE, INTERNALSIZE, ALPHASIZE)\n",
        "rnn.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using gpu..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (i2h): GRUCell(98, 512)\n",
              "  (i2o): Linear(in_features=610, out_features=98, bias=True)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "P79-gHUTNMJr"
      },
      "source": [
        "# loss fn\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "#from ok_seq2seq import EncoderRNN \\\n",
        "#                        ,DecoderRNN \\\n",
        "#                        ,AttnDecoderRNN \\\n",
        "#                        ,evaluateRandomly \\\n",
        "#                        ,teacher_forcing_ratio \n",
        "\n",
        "#torch.nn.GRUCell(input_size: int, hidden_size: int, bias: bool = True)\n",
        "# Parameters\n",
        "# input_size – The number of expected features in the input x\n",
        "\n",
        "# hidden_size – The number of features in the hidden state h\n",
        "\n",
        "# bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "\n",
        "# Inputs: input, hidden\n",
        "# input of shape (batch, input_size): tensor containing input features\n",
        "\n",
        "# hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\n",
        "\n",
        "# Outputs: h’\n",
        "# h’ of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5FKhhWDNMJs"
      },
      "source": [
        "# training fn\n",
        "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
        "\n",
        "def train(category_tensor, line_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "    \n",
        "    lint = []\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = rnn(line_tensor[i], hidden)\n",
        "        lint.append(output)\n",
        "    input = torch.stack(lint).transpose(0,1).transpose(1,2)\n",
        "#     print(f'is={input.size()}, cs={category_tensor.size()}')\n",
        "#     print(f'is[1:]={input.size()[1:]}, cs[1:]={category_tensor.size()[1:]}')\n",
        "#     print(f'is[2:]={input.size()[2:]}, cs[2:]={category_tensor.size()[2:]}')\n",
        "    loss = criterion(input, category_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    # Add parameters' gradients to their values, multiplied by learning rate\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "    return torch.stack(lint).transpose(0,1), loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM4NTqI5NMJu"
      },
      "source": [
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def one_hot(chcode):\n",
        "    tensor = torch.zeros(1, ALPHASIZE)\n",
        "    tensor[0][chcode] = 1\n",
        "    return tensor\n",
        "\n",
        "def mb2t(rows):\n",
        "    rows=rows.transpose()\n",
        "    tensor = torch.zeros(rows.shape[0], rows.shape[1], ALPHASIZE, device=device)\n",
        "    for i, row in enumerate(rows):\n",
        "        for j, letter_code in enumerate(row):\n",
        "            tensor[i][j][letter_code] = 1\n",
        "    return tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngo8CDGYNMJv"
      },
      "source": [
        "# \n",
        "def lin2txt(lt):\n",
        "    return ''.join([chr(txt.convert_to_alphabet(c))  if c != 0 else '' for c in lt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XSdtXUCNMJv",
        "outputId": "9ba61bfa-036d-43fa-bda9-95d83391d156"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 250\n",
        "plot_every = 100\n",
        "\n",
        "vloss = []\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "iter=0\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=nb_epoch):\n",
        "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    category =  [lin2txt(l) for l in y_]\n",
        "    lines = [lin2txt(l) for l in x]\n",
        "    category_tensor=mb2t(y_)\n",
        "    line_tensor=mb2t(x)\n",
        "    output, loss = train(torch.tensor(y_, device=device, dtype=torch.long), line_tensor)\n",
        "    current_loss += loss\n",
        "\n",
        "    # Print iter number, loss, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess = [lin2txt([ch.argmax(dim=0) for ch in line]) for line in output]\n",
        "        for i in range(2):\n",
        "            correct = '✓' if guess[i] == category[i] else '✗ %s' % category[i] \n",
        "            print('epoch %d of  %d (%s) %.4f %s / %s %s' % (epoch, nb_epoch, timeSince(start), loss, lines[i], guess[0], correct))\n",
        "\n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0 and len(valitext) > 0:\n",
        "        all_losses.append(current_loss / plot_every)\n",
        "        current_loss = 0\n",
        "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, BATCHSIZE, VALI_SEQLEN, 1))  # all data in 1 batch\n",
        "        line_tensor = mb2t(vali_x)\n",
        "        output, loss = train(torch.tensor(vali_y, device=device, dtype=torch.long), line_tensor)\n",
        "        vloss.append(loss)\n",
        "        with open('vloss.json', 'w') as f:\n",
        "          json.dump(vloss,f)\n",
        "    iter += 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 of  60 (0m 2s) 4.5849 jems jems be barabo firifirima / Ws(S,W,(S,(,((,,,([,Wm,(c1,((, ✗ ems jems be barabo firifirimad\n",
            "epoch 0 of  60 (0m 2s) 4.5849 o yekarifi ma o ye bereni ari  / Ws(S,W,(S,(,((,,,([,Wm,(c1,((, ✗  yekarifi ma o ye bereni ari s\n",
            "epoch 0 of  60 (1m 33s) 4.3815 e miebo bara nwose ori okime.  /                                ✗  miebo bara nwose ori okime. a\n",
            "epoch 0 of  60 (1m 33s) 4.3815 egoye inibo bietonye inibo gba /                                ✗ goye inibo bietonye inibo gbar\n",
            "epoch 1 of  60 (3m 4s) 4.1073  sime i piri eke.    ominea bi /                                ✗ sime i piri eke.    ominea bie\n",
            "epoch 1 of  60 (3m 4s) 4.1073 mi konka siki. anierechi ini d /                                ✗ i konka siki. anierechi ini du\n",
            "epoch 1 of  60 (4m 36s) 3.6348    se fiafiadiri mi jin anga b /                                ✗   se fiafiadiri mi jin anga bu\n",
            "epoch 1 of  60 (4m 36s) 3.6348 am nwo be ye mi.  min bara mi  /                                ✗ m nwo be ye mi.  min bara mi b\n",
            "epoch 2 of  60 (6m 7s) 3.1587 oni  piriye-e ani inia ojumini /                                ✗ ni  piriye-e ani inia ojumini \n",
            "epoch 2 of  60 (6m 7s) 3.1587 amuno be ene mi koruari siki o /                                ✗ muno be ene mi koruari siki o \n",
            "epoch 2 of  60 (7m 38s) 3.0083  se ominea bie korombia, omine /                                ✗ se ominea bie korombia, omine \n",
            "epoch 2 of  60 (7m 38s) 3.0083  kokomaye mi bie iruoye ofori- /                                ✗ kokomaye mi bie iruoye ofori-e\n",
            "epoch 3 of  60 (9m 9s) 2.9195  ani wa sime kuroari ye-e iwo  /                                ✗ ani wa sime kuroari ye-e iwo s\n",
            "epoch 3 of  60 (9m 9s) 2.9195 iatibi tamuno be tatari wa bel /                                ✗ atibi tamuno be tatari wa bele\n",
            "epoch 3 of  60 (10m 40s) 2.8556 o goyegoye piki dabo be sikime /                                ✗  goyegoye piki dabo be sikime \n",
            "epoch 3 of  60 (10m 40s) 2.8556  kraist be bereni minapu ma bo /                                ✗ kraist be bereni minapu ma bo \n",
            "epoch 4 of  60 (12m 12s) 2.8327 ase ori orisa bo be o nwo bele /                                ✗ se ori orisa bo be o nwo belem\n",
            "epoch 4 of  60 (12m 12s) 2.8327  minapu tamuno be min tomoniki /                                ✗ minapu tamuno be min tomonikir\n",
            "epoch 4 of  60 (13m 43s) 2.8042 nwo da. ibiye mie bo goyegoye  /                                ✗ wo da. ibiye mie bo goyegoye s\n",
            "epoch 4 of  60 (13m 43s) 2.8042 e, boka siki nwose sime se was /                                ✗ , boka siki nwose sime se wasa\n",
            "epoch 5 of  60 (15m 14s) 2.8336  osike tapu o punuma se o fono /                                ✗ osike tapu o punuma se o fono \n",
            "epoch 5 of  60 (15m 14s) 2.8336 yebusoka papa dumo mi bie o pe /                                ✗ ebusoka papa dumo mi bie o pek\n",
            "epoch 5 of  60 (16m 46s) 2.7917  bara mi kurake.  aniokuma, nw / b                              ✗ bara mi kurake.  aniokuma, nwo\n",
            "epoch 5 of  60 (16m 46s) 2.7917 ma o nwo mie fiafiama, miese i / b                              ✗ a o nwo mie fiafiama, miese in\n",
            "epoch 6 of  60 (18m 17s) 2.7558 u sime bo be. tomonikiri m ma  /                                ✗  sime bo be. tomonikiri m ma t\n",
            "epoch 6 of  60 (18m 17s) 2.7558 iruo ye dieapu ma . kraist be  /                                ✗ ruo ye dieapu ma . kraist be k\n",
            "epoch 6 of  60 (19m 49s) 2.7680 , ibi ye mie gose torusiori mi /                                ✗  ibi ye mie gose torusiori mi \n",
            "epoch 6 of  60 (19m 49s) 2.7680 iki ngisi, tonbu sime ikiankor /                                ✗ ki ngisi, tonbu sime ikiankoro\n",
            "epoch 7 of  60 (21m 20s) 2.7667 jizos kraist be karakara mi bu /                                ✗ izos kraist be karakara mi bu \n",
            "epoch 7 of  60 (21m 20s) 2.7667 omine o nemime. a diri gien om /                                ✗ mine o nemime. a diri gien omi\n",
            "epoch 7 of  60 (22m 52s) 2.7079 e, ojumini mi dikipukosolia ye /                                ✗ , ojumini mi dikipukosolia ye \n",
            "epoch 7 of  60 (22m 52s) 2.7079 be wa no belemame now se yeke, /                                ✗ e wa no belemame now se yeke, \n",
            "epoch 8 of  60 (24m 23s) 2.7286 aki siki ngisni sime bo be omi /                                ✗ ki siki ngisni sime bo be omin\n",
            "epoch 8 of  60 (24m 23s) 2.7286  mi i piki oju bu so oko kuro. /                                ✗ mi i piki oju bu so oko kuro. \n",
            "epoch 8 of  60 (25m 55s) 2.7040 jinbojinbo belemame nwo bebe,  /                         i      ✗ inbojinbo belemame nwo bebe, t\n",
            "epoch 8 of  60 (25m 55s) 2.7040 omaka bo beke, okuma ani se mi /                         i      ✗ maka bo beke, okuma ani se mie\n",
            "epoch 9 of  60 (27m 26s) 2.6898  oforie bo naa na iya awo ma k / b          i   i          i    ✗ oforie bo naa na iya awo ma ko\n",
            "epoch 9 of  60 (27m 26s) 2.6898 na tuburu, anisiki ori o diki  / b          i   i          i    ✗ a tuburu, anisiki ori o diki m\n",
            "epoch 9 of  60 (28m 57s) 2.6891 obori se obu chinabe  fiafia p /                      i      i  ✗ bori se obu chinabe  fiafia pi\n",
            "epoch 9 of  60 (28m 57s) 2.6891 ni weri bobia bara na nwo doki /                      i      i  ✗ i weri bobia bara na nwo dokim\n",
            "epoch 10 of  60 (30m 27s) 2.6720 ku owu omine ominea deri mi pi /         i     i       i  i     ✗ u owu omine ominea deri mi pik\n",
            "epoch 10 of  60 (30m 27s) 2.6720 jinbo belema, se o piki ibiye  /         i     i       i  i     ✗ inbo belema, se o piki ibiye m\n",
            "epoch 10 of  60 (31m 58s) 2.6723 maa ikiankoroapu ma dieme na i / a          i     i     i  i i  ✗ aa ikiankoroapu ma dieme na in\n",
            "epoch 10 of  60 (31m 58s) 2.6723 ma aniabie chie   babilon mi b / a          i     i     i  i i  ✗ a aniabie chie   babilon mi bi\n",
            "epoch 11 of  60 (33m 29s) 2.6741 ai tamuno be bereni o piriye m /      i    i  i i   i ii i   ii ✗ i tamuno be bereni o piriye mi\n",
            "epoch 11 of  60 (33m 29s) 2.6741 amuno be ani gbuntein kanakana /      i    i  i i   i ii i   ii ✗ muno be ani gbuntein kanakana \n",
            "epoch 11 of  60 (34m 59s) 2.6610 i  ye deinma sime omine kraist /   i      ii    i   i i  ii     ✗   ye deinma sime omine kraist \n",
            "epoch 11 of  60 (34m 59s) 2.6610  wa se ibiokuma nimibia yee na /   i      ii    i   i i  ii     ✗ wa se ibiokuma nimibia yee na \n",
            "epoch 12 of  60 (36m 30s) 2.6651 apu ma pirime  ini bu nyana be /     i    i i  i i ii ii  i ii  ✗ pu ma pirime  ini bu nyana ber\n",
            "epoch 12 of  60 (36m 30s) 2.6651 yana teme be ani kraist be na  /     i    i i  i i ii ii  i ii  ✗ ana teme be ani kraist be na t\n",
            "epoch 12 of  60 (38m 1s) 2.6327  iwo yeke, okuma oloko mi anie / b           i i     i ii i i   ✗ iwo yeke, okuma oloko mi anie,\n",
            "epoch 12 of  60 (38m 1s) 2.6327  soni se o piri ma  aniatibi d / b           i i     i ii i i   ✗ soni se o piri ma  aniatibi de\n",
            "epoch 13 of  60 (39m 31s) 2.6316 atibi o bie sime tamuno be kur /        ii  ii i i  i i ii ii i ✗ tibi o bie sime tamuno be kuro\n",
            "epoch 13 of  60 (39m 31s) 2.6316 zos be na kobirima sime siki i /        ii  ii i i  i i ii ii i ✗ os be na kobirima sime siki in\n",
            "epoch 13 of  60 (41m 2s) 2.6280 omine na iria se toru se omine /  i    i i i  ii i  i i  i i i  ✗ mine na iria se toru se omine \n",
            "epoch 13 of  60 (41m 2s) 2.6280 muno be fi bu be gbori jizos b /  i    i i i  ii i  i i  i i i  ✗ uno be fi bu be gbori jizos be\n",
            "epoch 14 of  60 (42m 32s) 2.6088 ime o pirike omine ari miebia  /  i   ii i i i i i i i ii  i  i ✗ me o pirike omine ari miebia s\n",
            "epoch 14 of  60 (42m 32s) 2.6088 ka se be ikelibo be bu mimgba  /  i   ii i i i i i i i ii  i  i ✗ a se be ikelibo be bu mimgba n\n",
            "epoch 14 of  60 (44m 3s) 2.6042  mi o mioku ori, piki naa ye m / bi   ii  i i i  ii i ii  i  ii ✗ mi o mioku ori, piki naa ye mi\n",
            "epoch 14 of  60 (44m 3s) 2.6042 gasiki koruabe okuma wa bko ma / bi   ii  i i i  ii i ii  i  ii ✗ asiki koruabe okuma wa bko ma \n",
            "epoch 15 of  60 (45m 33s) 2.6011  ominea toroko gwosa bara bu c / b i i  i  i i i     ii i ii i  ✗ ominea toroko gwosa bara bu ch\n",
            "epoch 15 of  60 (45m 33s) 2.6011 un mi bu josef be mie se oria  / b i i  i  i i i     ii i ii i  ✗ n mi bu josef be mie se oria m\n",
            "epoch 15 of  60 (47m 5s) 2.5988 bori owubo so bome now sei ini / e i i   i ii ii i ii  ii  i i  ✗ ori owubo so bome now sei ini \n",
            "epoch 15 of  60 (47m 5s) 2.5988 sameria mi bie. ori obu se opu / e i i   i ii ii i ii  ii  i i  ✗ ameria mi bie. ori obu se opub\n",
            "epoch 16 of  60 (48m 35s) 2.5841  kienbipi laye sonofa kini ine / bi  i    ii   ii i   bi i i i  ✗ kienbipi laye sonofa kini inei\n",
            "epoch 16 of  60 (48m 35s) 2.5841  sol be babia oku now baime ok / bi  i    ii   ii i   bi i i i  ✗ sol be babia oku now baime oku\n",
            "epoch 16 of  60 (50m 6s) 2.5827 u irimame ani miese ini toru s /  b i i i i i ii    i i ii i bi ✗  irimame ani miese ini toru se\n",
            "epoch 16 of  60 (50m 6s) 2.5827 diki balafa se karakaraye mie  /  b i i i i i ii    i i ii i bi ✗ iki balafa se karakaraye mie b\n",
            "epoch 17 of  60 (51m 37s) 2.5755 ma mi be onu mi bu finji sime  / a bi ii i i bi ii bi i  ii i b ✗ a mi be onu mi bu finji sime n\n",
            "epoch 17 of  60 (51m 37s) 2.5755 pita be nungo da book mi nwo g / a bi ii i i bi ii bi i  ii i b ✗ ita be nungo da book mi nwo gb\n",
            "epoch 17 of  60 (53m 8s) 2.5600 mabe, tomoni mamgba nyanabo be / a    i  i i ii i i bi  i i bi  ✗ abe, tomoni mamgba nyanabo be.\n",
            "epoch 17 of  60 (53m 8s) 2.5600 obi, miese ikiankoroapu ma duk / a    i  i i ii i i bi  i i bi  ✗ bi, miese ikiankoroapu ma duko\n",
            "epoch 18 of  60 (54m 39s) 2.5694 na pita be o nungo chie sime w / i bi   ii i ii i  b    bi i b  ✗ a pita be o nungo chie sime wa\n",
            "epoch 18 of  60 (54m 39s) 2.5694 bugerere bu duaboroma nyana ok / i bi   ii i ii i  b    bi i b  ✗ ugerere bu duaboroma nyana oku\n",
            "epoch 18 of  60 (56m 11s) 2.5528  nwo duko belema o piri so pol / bi  ii i bi   i b bi i ii bi   ✗ nwo duko belema o piri so pol \n",
            "epoch 18 of  60 (56m 11s) 2.5528 bini ma pol be na sailas be na / bi  ii i bi   i b bi i ii bi   ✗ ini ma pol be na sailas be na \n",
            "epoch 19 of  60 (57m 41s) 2.5373 buamaye ma na nwo naa siki ini / e  i   ii bi bi  bi  bi i b i  ✗ uamaye ma na nwo naa siki ini \n",
            "epoch 19 of  60 (57m 41s) 2.5373 moni ma o nemi ka sikima o nwo / e  i   ii bi bi  bi  bi i b i  ✗ oni ma o nemi ka sikima o nwo \n",
            "epoch 19 of  60 (59m 13s) 2.5525  bu ikoliwari dikidikibo be in / bi i i     i bi i   i i bi b i ✗ bu ikoliwari dikidikibo be ini\n",
            "epoch 19 of  60 (59m 13s) 2.5525 unbia.  mun ani la so siki, o / bi i i     i bi i   i i bi b i ✗ nbia.  mun ani la so siki, o \n",
            "epoch 20 of  60 (60m 44s) 2.5690 erisam, ani o selepakuma bosa  /  i   i b i i bi   i i a be   b ✗ risam, ani o selepakuma bosa b\n",
            "epoch 20 of  60 (60m 44s) 2.5690 tibi gose.  se moku ari o se t /  i   i b i i bi   i i a be   b ✗ ibi gose.  se moku ari o se ta\n",
            "epoch 20 of  60 (62m 16s) 2.5292 ki mi bu mi opu bufuka efisos  / i bi ii bi b i bi   i b i i ib ✗ i mi bu mi opu bufuka efisos b\n",
            "epoch 20 of  60 (62m 16s) 2.5292 e ini se ikoli seinme. gele ag / i bi ii bi b i bi   i b i i ib ✗  ini se ikoli seinme. gele agb\n",
            "epoch 21 of  60 (63m 48s) 2.5506 ikanika goli, anikanika buchua /  i i i ii    b i i i i bi      ✗ kanika goli, anikanika buchuay\n",
            "epoch 21 of  60 (63m 48s) 2.5506  piri anisiki owuniberetonbo b /  i i i ii    b i i i i bi      ✗ piri anisiki owuniberetonbo be\n",
            "epoch 21 of  60 (65m 19s) 2.5434 ikasi se ini kun bo bufuka se  /  i   ii i i bi ibi bi   i bi b ✗ kasi se ini kun bo bufuka se i\n",
            "epoch 21 of  60 (65m 19s) 2.5434 i pirime na romapu ma ini tomo /  i   ii i i bi ibi bi   i bi b ✗  pirime na romapu ma ini tomon\n",
            "epoch 22 of  60 (66m 50s) 2.5714 ku-e, se ari iya owuni ma na m / i    bi b i b i b   i bi bi bi ✗ u-e, se ari iya owuni ma na mu\n",
            "epoch 22 of  60 (66m 50s) 2.5714 ima borome, se opu bufuka bub  / i    bi b i b i b   i bi bi bi ✗ ma borome, se opu bufuka bub o\n",
            "epoch 22 of  60 (68m 22s) 2.5392 numaye ma bie. ini minyo bo si / i i   bi bi   b i bi i  bi bi  ✗ umaye ma bie. ini minyo bo sik\n",
            "epoch 22 of  60 (68m 22s) 2.5392 in boro siki pol be juapu ma b / i i   bi bi   b i bi i  bi bi  ✗ n boro siki pol be juapu ma be\n",
            "epoch 23 of  60 (69m 53s) 2.5245 sebubama ene miemieye mi boros /   e i a b i bi  i    bi bi i   ✗ ebubama ene miemieye mi borosa\n",
            "epoch 23 of  60 (69m 53s) 2.5245 i basabas nwose chin bo be, in /   e i a b i bi  i    bi bi i   ✗  basabas nwose chin bo be, ini\n",
            "epoch 23 of  60 (71m 24s) 2.5001 juadapu ma bara wa daoki amami /      i bi bi i bi bi  i b i a  ✗ uadapu ma bara wa daoki amamie\n",
            "epoch 23 of  60 (71m 24s) 2.5001  oku nwo ori siki, ori ini tek /      i bi bi i bi bi  i b i a  ✗ oku nwo ori siki, ori ini teke\n",
            "epoch 24 of  60 (72m 55s) 2.5005 mi nwo okibia bo, okunwengibo  / a bi  b i i  bi  b i i  i  i b ✗ i nwo okibia bo, okunwengibo b\n",
            "epoch 24 of  60 (72m 55s) 2.5005 jizos be na tuburubia erechi,  / a bi  b i i  bi  b i i  i  i b ✗ izos be na tuburubia erechi, i\n",
            "epoch 24 of  60 (74m 26s) 2.5257 gisi oforie, okuma a nyana ye  /   i i i i   b i a b bi  i be b ✗ isi oforie, okuma a nyana ye a\n",
            "epoch 24 of  60 (74m 26s) 2.5257 i mi bie na tomonia wari ma bi /   i i i i   b i a b bi  i be b ✗  mi bie na tomonia wari ma bie\n",
            "epoch 25 of  60 (75m 57s) 2.4944 u elekima chua, se bereni mine /  b i i i bii   bi be i i bi i  ✗  elekima chua, se bereni mine \n",
            "epoch 25 of  60 (75m 57s) 2.4944 ni oki pakumabo yee, bugbiripu /  b i i i bii   bi be i i bi i  ✗ i oki pakumabo yee, bugbiripum\n",
            "epoch 25 of  60 (77m 28s) 2.5089 orome, grik okwein konari juap /  i i  bii ib i   ibi i i bi  i ✗ rome, grik okwein konari juapu\n",
            "epoch 25 of  60 (77m 28s) 2.5089 ku, jinbo ani duko belema i pi /  i i  bii ib i   ibi i i bi  i ✗ u, jinbo ani duko belema i pir\n",
            "epoch 26 of  60 (78m 58s) 2.5081  se o firimbia, ani o nwo i fi / bi b ii i ie   b i b bi  b bi  ✗ se o firimbia, ani o nwo i fir\n",
            "epoch 26 of  60 (78m 58s) 2.5081 nilios be na gbori owubo sizar / bi b ii i ie   b i b bi  b bi  ✗ ilios be na gbori owubo sizari\n",
            "epoch 26 of  60 (80m 29s) 2.4966 huka ma kirisinari siki gboinm /   i bi bi i i i i bi i bie  ia ✗ uka ma kirisinari siki gboinma\n",
            "epoch 26 of  60 (80m 29s) 2.4966 hiesime enjelb be nwo ori oku  /   i bi bi i i i i bi i bie  ia ✗ iesime enjelb be nwo ori oku n\n",
            "epoch 27 of  60 (82m 1s) 2.4949 muno piripirime. o bakama bara / a i bi i i i i  b be i a be i  ✗ uno piripirime. o bakama barac\n",
            "epoch 27 of  60 (82m 1s) 2.4949 u ani o gamunuma majik bo be d / a i bi i i i i  b be i a be i  ✗  ani o gamunuma majik bo be di\n",
            "epoch 27 of  60 (83m 32s) 2.4788  okwein siki, fiafia teme be k / b i   ibi i  bi  i  bi i be bi ✗ okwein siki, fiafia teme be ko\n",
            "epoch 27 of  60 (83m 32s) 2.4788  tomoni toru nwose koro wa dam / b i   ibi i  bi  i  bi i be bi ✗ tomoni toru nwose koro wa dama\n",
            "epoch 28 of  60 (85m 3s) 2.4673 ku nyanabo be bara mi korobo i / i bi  i e be be i bi bi i e b  ✗ u nyanabo be bara mi korobo iy\n",
            "epoch 28 of  60 (85m 3s) 2.4673 o ini se firima apu ma damamum / i bi  i e be be i bi bi i e b  ✗  ini se firima apu ma damamume\n",
            "epoch 28 of  60 (86m 35s) 2.4826  na ngwengwe na nwose ama mi b / bi bii  i   bi bi  i b a bi be ✗ na ngwengwe na nwose ama mi be\n",
            "epoch 28 of  60 (86m 35s) 2.4826  oforifori bara se ateliogbo n / bi bii  i   bi bi  i b a bi be ✗ oforifori bara se ateliogbo nw\n",
            "epoch 29 of  60 (88m 6s) 2.4785 iki ani dukome. pol be na bana /  i b i bi i a  bi  be bi be i  ✗ ki ani dukome. pol be na banab\n",
            "epoch 29 of  60 (88m 6s) 2.4785 i mi bie so se juapu ma na jua /  i b i bi i a  bi  be bi be i  ✗  mi bie so se juapu ma na juap\n",
            "epoch 29 of  60 (89m 37s) 2.4647 ma piribia oku. okuma ini ini  / a bi i i  b i  b i a b i b i b ✗ a piribia oku. okuma ini ini d\n",
            "epoch 29 of  60 (89m 37s) 2.4647 ri irima tolu boro se tomoni m / a bi i i  b i  b i a b i b i b ✗ i irima tolu boro se tomoni ma\n",
            "epoch 30 of  60 (91m 8s) 2.4637 iki o damabobia. ani ori efiso /  i b ii a e e   b i b i b i i  ✗ ki o damabobia. ani ori efisos\n",
            "epoch 30 of  60 (91m 8s) 2.4637 i nwo oribia, minea bie juapu  /  i b ii a e e   b i b i b i i  ✗  nwo oribia, minea bie juapu b\n",
            "epoch 30 of  60 (92m 39s) 2.4695 obia. biria bie bo pairos ya,  /  e   bi i  be  be bi  i ibe  b ✗ bia. biria bie bo pairos ya, s\n",
            "epoch 30 of  60 (92m 39s) 2.4695 kuma o nwo doki nemi, ani anib /  e   bi i  be  be bi  i ibe  b ✗ uma o nwo doki nemi, ani anibe\n",
            "epoch 31 of  60 (94m 10s) 2.4686 uapu ma dieabe na ini mozizi o /   i bi bi   e bi b i bi     b  ✗ apu ma dieabe na ini mozizi ol\n",
            "epoch 31 of  60 (94m 10s) 2.4686 oni na toroko anga okue. a to /   i bi bi   e bi b i bi     b  ✗ ni na toroko anga okue. a ton\n",
            "epoch 31 of  60 (95m 41s) 2.4618 ukukuma berechiri mi diki se n /  i i a be i    i bi bi i bi bi ✗ kukuma berechiri mi diki se nw\n",
            "epoch 31 of  60 (95m 41s) 2.4618 oku now dukome okunwengiapu mi /  i i a be i    i bi bi i bi bi ✗ ku now dukome okunwengiapu mie\n",
            "epoch 32 of  60 (97m 12s) 2.4576 ee, a bumiefiafiafiama miemiey /    b be a     i  i  a bi  a  e ✗ e, a bumiefiafiafiama miemieye\n",
            "epoch 32 of  60 (97m 12s) 2.4576  kala aru mi kpo diki ma peles /    b be a     i  i  a bi  a  e ✗ kala aru mi kpo diki ma pelesi\n",
            "epoch 32 of  60 (98m 42s) 2.4547 u, a nwo orime, sobie korobo b /   b bi  b i i  bi e  bi i e be ✗ , a nwo orime, sobie korobo bi\n",
            "epoch 32 of  60 (98m 42s) 2.4547  be oria tamuno firinwengi mi  /   b bi  b i i  bi e  bi i e be ✗ be oria tamuno firinwengi mi  \n",
            "epoch 33 of  60 (100m 13s) 2.4379 efi miese omine inyo nyanabia  /    bi  e b i i b i  bie i e  b ✗ fi miese omine inyo nyanabia o\n",
            "epoch 33 of  60 (100m 13s) 2.4379 a siki mamgba nyanabo be nwo o /    bi  e b i i b i  bie i e  b ✗  siki mamgba nyanabo be nwo or\n",
            "epoch 33 of  60 (101m 44s) 2.4311 e ikoliboe jerusalem bie, siza /  b i i e  bi i     abe   bi i  ✗  ikoliboe jerusalem bie, sizar\n",
            "epoch 33 of  60 (101m 44s) 2.4311 bamieapu ma na, oputekewari mi /  b i i e  bi i     abe   bi i  ✗ amieapu ma na, oputekewari mi \n",
            "epoch 34 of  60 (103m 15s) 2.4215 be soni, torusedikibiaye a nwo / e bi i  bi i     i e  e b bi   ✗ e soni, torusedikibiaye a nwo \n",
            "epoch 34 of  60 (103m 15s) 2.4215 ma na, gbotubara okunwengiapu  / e bi i  bi i     i e  e b bi   ✗ a na, gbotubara okunwengiapu m\n",
            "epoch 34 of  60 (104m 46s) 2.4326 e na fiapu fibusise dumobia  a /  bi bi  i bi e   i bi a e  bb  ✗  na fiapu fibusise dumobia  an\n",
            "epoch 34 of  60 (104m 46s) 2.4326 se mianga i tekebia  anisiki t /  bi bi  i bi e   i bi a e  bb  ✗ e mianga i tekebia  anisiki ta\n",
            "epoch 35 of  60 (106m 17s) 2.4206 oliwari onobu book ma finji ok /      i b i e be  ibi bi i  b i ✗ liwari onobu book ma finji oku\n",
            "epoch 35 of  60 (106m 17s) 2.4206 ba galagala judia na sameria n /      i b i e be  ibi bi i  b i ✗ a galagala judia na sameria na\n",
            "epoch 36 of  60 (107m 48s) 2.4381 se jekob be minea oyima dapu f / e bi i ebe bi i  b e i bi i bi ✗ e jekob be minea oyima dapu fi\n",
            "epoch 36 of  60 (107m 48s) 2.4381 okoro korome se ori ineda piki / e bi i ebe bi i  b e i bi i bi ✗ koro korome se ori ineda piki \n",
            "epoch 36 of  60 (109m 19s) 2.4121 se bereniapu ma fonofono pakum / e be i i  i bi bi i i i bi i a ✗ e bereniapu ma fonofono pakuma\n",
            "epoch 36 of  60 (109m 19s) 2.4121 i iya wari bie sime teke simem / e be i i  i bi bi i i i bi i a ✗  iya wari bie sime teke simeme\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "1W0YOx5gDPuL",
        "outputId": "862849a3-b8d4-437f-fff2-2951f22885c3"
      },
      "source": [
        "import torch\r\n",
        "PATH = './slgru_epoch36.model'\r\n",
        "torch.save(rnn.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cb8b7248f6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./slgru_epoch36.model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'rnn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOF1-xK-NMJw",
        "outputId": "c7d7adff-c3c3-4ab8-90a6-e0a2c00a8b97"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "# product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.plot(vloss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXwKxI0KNMJx",
        "outputId": "cb7fe06f-082d-4d5f-dd89-14f4d04581dd"
      },
      "source": [
        "import kenlm\n",
        "with open('rpynotes/ok-rnn/txts/gal_eph_new.txt', encoding='utf-16') as f:\n",
        "  s=f.read()\n",
        "model = kenlm.Model('rpynotes/ok-rnn/txteval/val/text.arpa')\n",
        "print(model.perplexity(s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is=torch.Size([1, 98]), hs=torch.Size([1, 128])\n",
            "torch.Size([1, 98])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}