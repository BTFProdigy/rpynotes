{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AenBlnPHOt0",
    "outputId": "bee135bf-58db-4702-e007-c8a2658cf7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package             Version\n",
      "------------------- -------\n",
      "absl-py             0.11.0 \n",
      "gast                0.4.0  \n",
      "importlib-metadata  3.3.0  \n",
      "Keras-Preprocessing 1.1.2  \n",
      "Markdown            3.3.3  \n",
      "numpy               1.19.4 \n",
      "pip                 9.0.1  \n",
      "pkg-resources       0.0.0  \n",
      "protobuf            3.14.0 \n",
      "setuptools          39.0.1 \n",
      "six                 1.15.0 \n",
      "typing-extensions   3.7.4.3\n",
      "Werkzeug            1.0.1  \n",
      "wheel               0.36.2 \n",
      "zipp                3.4.0  \n"
     ]
    }
   ],
   "source": [
    "!source venv/bin/activate && pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vPD2_LBHQ4-",
    "outputId": "56835923-9e76-4f10-9b93-935b3e44a92b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'rpynotes/ok-rnn/txteval': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -l rpynotes/ok-rnn/txteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qt5T58Q5RckE",
    "outputId": "151fc27f-b583-47d9-cb76-f27f29c51bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'kenlm'...\n",
      "remote: Enumerating objects: 99, done.\u001b[K\n",
      "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
      "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
      "remote: Total 13681 (delta 46), reused 60 (delta 24), pack-reused 13582\u001b[K\n",
      "Receiving objects: 100% (13681/13681), 5.54 MiB | 1.77 MiB/s, done.\n",
      "Resolving deltas: 100% (7852/7852), done.\n",
      "-- The C compiler identification is GNU 7.5.0\n",
      "-- The CXX compiler identification is GNU 7.5.0\n",
      "-- Check for working C compiler: /usr/bin/cc\n",
      "-- Check for working C compiler: /usr/bin/cc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++\n",
      "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Looking for pthread_create\n",
      "-- Looking for pthread_create - not found\n",
      "-- Looking for pthread_create in pthreads\n",
      "-- Looking for pthread_create in pthreads - not found\n",
      "-- Looking for pthread_create in pthread\n",
      "-- Looking for pthread_create in pthread - found\n",
      "-- Found Threads: TRUE  \n",
      "-- Boost version: 1.65.1\n",
      "-- Found the following Boost libraries:\n",
      "--   program_options\n",
      "--   system\n",
      "--   thread\n",
      "--   unit_test_framework\n",
      "--   chrono\n",
      "--   date_time\n",
      "--   atomic\n",
      "-- Check if compiler accepts -pthread\n",
      "-- Check if compiler accepts -pthread - yes\n",
      "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
      "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n",
      "-- Looking for BZ2_bzCompressInit\n",
      "-- Looking for BZ2_bzCompressInit - found\n",
      "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Found LibLZMA: /usr/include (found version \"5.2.2\") \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /content/kenlm/build\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm_util\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
      "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
      "[ 38%] Built target kenlm_util\n",
      "\u001b[35m\u001b[1mScanning dependencies of target probing_hash_table_benchmark\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm_filter\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
      "[ 53%] Built target kenlm_filter\n",
      "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
      "[ 62%] Built target probing_hash_table_benchmark\n",
      "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
      "[ 71%] Built target kenlm\n",
      "\u001b[35m\u001b[1mScanning dependencies of target build_binary\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target fragment\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target query\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm_benchmark\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
      "[ 77%] Built target fragment\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm_builder\u001b[0m\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
      "[ 80%] Built target build_binary\n",
      "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
      "[ 82%] Built target query\n",
      "\u001b[35m\u001b[1mScanning dependencies of target phrase_table_vocab\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
      "[ 87%] Built target phrase_table_vocab\n",
      "\u001b[35m\u001b[1mScanning dependencies of target filter\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
      "[ 92%] Built target kenlm_benchmark\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
      "[ 93%] Built target kenlm_builder\n",
      "\u001b[35m\u001b[1mScanning dependencies of target lmplz\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target count_ngrams\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
      "[ 96%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
      "[ 97%] Built target filter\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
      "[ 98%] Built target lmplz\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
      "[100%] Built target count_ngrams\n",
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "\u001b[?25l  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
      "\u001b[K     | 1.5MB 8.9MB/s\n",
      "\u001b[?25hBuilding wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kenlm: filename=kenlm-0.0.0-cp36-cp36m-linux_x86_64.whl size=2333074 sha256=eb28b3d7e7ce947ee49830d1d9f7d0146e12f28580aead5a4c0bedd20cdd4c2e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-l0ciryit/wheels/2d/32/73/e3093c9d11dc8abf79c156a4db1a1c5631428059d4f9ff2cba\n",
      "Successfully built kenlm\n",
      "Installing collected packages: kenlm\n",
      "Successfully installed kenlm-0.0.0\n",
      "--2020-12-18 08:39:51--  https://onedrive.live.com/download?cid=EB20651D09521520&resid=EB20651D09521520%21171763&authkey=AAAmYzTmYQLp4lU\n",
      "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
      "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ekou5w.am.files.1drv.com/y4mzlzivR6R54axSz40pVq9DGnLF8o-Rm8oZPO6DfvIbj6mz3nWAZvIt-_VxgvhrkxjTOGVFz4WChPEViQ74b3aXiOIvlfXw1PzQjjscZIVeF40N5aiKOBD_f7P8OsDopUHcin7qDfa05u1G3CAi15e6uqY6eNLgFl6SlyG5gh1SZH5lTlzGowzOr-0VTKtwPUx8zQ2TJbdsS-kqlQrDpumTw/txts.zip?download&psid=1 [following]\n",
      "--2020-12-18 08:39:53--  https://ekou5w.am.files.1drv.com/y4mzlzivR6R54axSz40pVq9DGnLF8o-Rm8oZPO6DfvIbj6mz3nWAZvIt-_VxgvhrkxjTOGVFz4WChPEViQ74b3aXiOIvlfXw1PzQjjscZIVeF40N5aiKOBD_f7P8OsDopUHcin7qDfa05u1G3CAi15e6uqY6eNLgFl6SlyG5gh1SZH5lTlzGowzOr-0VTKtwPUx8zQ2TJbdsS-kqlQrDpumTw/txts.zip?download&psid=1\n",
      "Resolving ekou5w.am.files.1drv.com (ekou5w.am.files.1drv.com)... 13.107.42.12\n",
      "Connecting to ekou5w.am.files.1drv.com (ekou5w.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1015212 (991K) [application/zip]\n",
      "Saving to: ‘txts.zip’\n",
      "\n",
      "txts.zip            100%[===================>] 991.42K  3.07MB/s    in 0.3s    \n",
      "\n",
      "2020-12-18 08:39:53 (3.07 MB/s) - ‘txts.zip’ saved [1015212/1015212]\n",
      "\n",
      "Archive:  txts.zip\n",
      "   creating: txts/\n",
      "  inflating: txts/acts_new.txt       \n",
      "  inflating: txts/gal_eph_new.txt    \n",
      "  inflating: txts/heb_new.txt        \n",
      "  inflating: txts/jam_jud_new.txt    \n",
      "  inflating: txts/john_new.txt       \n",
      "  inflating: txts/jud_rev_new.txt    \n",
      "  inflating: txts/luke_8_john_new.txt  \n",
      "  inflating: txts/mark01_new.txt     \n",
      "  inflating: txts/matt02_new.txt     \n",
      "  inflating: txts/matt_new.txt       \n",
      "  inflating: txts/phil_col_new.txt   \n",
      "  inflating: txts/thes_tim_new.txt   \n",
      "  inflating: txts/tit_phl_new.txt    \n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  python-pip-whl python3.6-venv\n",
      "The following NEW packages will be installed:\n",
      "  python-pip-whl python3-venv python3.6-venv\n",
      "0 upgraded, 3 newly installed, 0 to remove and 14 not upgraded.\n",
      "Need to get 1,660 kB of archives.\n",
      "After this operation, 1,902 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.4 [1,653 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3.6-venv amd64 3.6.9-1~18.04ubuntu1.3 [6,180 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-venv amd64 3.6.7-1~18.04 [1,208 B]\n",
      "Fetched 1,660 kB in 2s (666 kB/s)\n",
      "Selecting previously unselected package python-pip-whl.\n",
      "(Reading database ... 144865 files and directories currently installed.)\n",
      "Preparing to unpack .../python-pip-whl_9.0.1-2.3~ubuntu1.18.04.4_all.deb ...\n",
      "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
      "Selecting previously unselected package python3.6-venv.\n",
      "Preparing to unpack .../python3.6-venv_3.6.9-1~18.04ubuntu1.3_amd64.deb ...\n",
      "Unpacking python3.6-venv (3.6.9-1~18.04ubuntu1.3) ...\n",
      "Selecting previously unselected package python3-venv.\n",
      "Preparing to unpack .../python3-venv_3.6.7-1~18.04_amd64.deb ...\n",
      "Unpacking python3-venv (3.6.7-1~18.04) ...\n",
      "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
      "Setting up python3.6-venv (3.6.9-1~18.04ubuntu1.3) ...\n",
      "Setting up python3-venv (3.6.7-1~18.04) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Cloning into 'rpynotes'...\n",
      "remote: Enumerating objects: 734, done.\u001b[K\n",
      "remote: Counting objects: 100% (734/734), done.\u001b[K\n",
      "remote: Compressing objects: 100% (555/555), done.\u001b[K\n",
      "remote: Total 2545 (delta 296), reused 543 (delta 153), pack-reused 1811\u001b[K\n",
      "Receiving objects: 100% (2545/2545), 199.50 MiB | 12.52 MiB/s, done.\n",
      "Resolving deltas: 100% (899/899), done.\n",
      "Checking out files: 100% (1110/1110), done.\n",
      "Collecting tensorflow-gpu==1.13.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 345.2MB 4.2kB/s \n",
      "tcmalloc: large alloc 1073750016 bytes == 0x5588e000 @  0x7f80baa342a4 0x591e47 0x5b7f6c 0x50a25a 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 12.5MB/s \n",
      "\u001b[?25hCollecting numpy>=1.13.3 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/fc/36e52d0ae2aa502b211f1bcd2fdeec72d343d58224eabcdddc1bcb052db1/numpy-1.19.4-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.4MB 109kB/s \n",
      "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 321kB/s \n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/98/74a430566fdd9d4cc0386322e55306c8928a95da95b1da6fba08641526b5/grpcio-1.34.0.tar.gz (21.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 21.0MB 65kB/s \n",
      "\u001b[?25hCollecting wheel>=0.26 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 2.2MB/s \n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 12.0MB/s \n",
      "\u001b[?25hCollecting six>=1.10.0 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/fd/247ef25f5ec5f9acecfbc98ca3c6aaf66716cf52509aca9a93583d410493/protobuf-3.14.0-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 1.0MB/s \n",
      "\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/58/0aa6fb779dc69cfc811df3398fcbeaeefbf18561b6e36b185df0782781cc/absl_py-0.11.0-py3-none-any.whl (127kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 10.1MB/s \n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 1.8MB/s \n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/ef/24a91ca96efa0d7802dffb83ccc7a3c677027bea19ec3c9ee80be740408e/Markdown-3.3.3-py3-none-any.whl (96kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 12.5MB/s \n",
      "\u001b[?25hCollecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
      "Collecting h5py (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.0MB 318kB/s \n",
      "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\" (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/85/ac225e35048e050a6351b6f1251cdb2b6060092f2c6840aff1d6319941b1/importlib_metadata-3.3.0-py3-none-any.whl\n",
      "Collecting cached-property; python_version < \"3.8\" (from h5py->keras-applications>=1.0.6->tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\" (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Building wheels for collected packages: grpcio, termcolor\n",
      "  Running setup.py bdist_wheel for grpcio ... \u001b[?25lerror\n",
      "\u001b[31m  Failed building wheel for grpcio\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for grpcio\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25lerror\n",
      "\u001b[31m  Failed building wheel for termcolor\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for termcolor\n",
      "Failed to build grpcio termcolor\n",
      "Installing collected packages: six, numpy, keras-preprocessing, werkzeug, protobuf, absl-py, grpcio, wheel, zipp, typing-extensions, importlib-metadata, markdown, tensorboard, mock, tensorflow-estimator, gast, cached-property, h5py, keras-applications, astor, termcolor, tensorflow-gpu\n",
      "  Running setup.py install for grpcio ... \u001b[?25lerror\n",
      "\u001b[31mCommand \"/content/venv/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-_yavfrf5/grpcio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-wf9qcutk-record/install-record.txt --single-version-externally-managed --compile --install-headers /content/venv/include/site/python3.6/grpcio\" failed with error code 1 in /tmp/pip-build-_yavfrf5/grpcio/\u001b[0m\n",
      "\u001b[?25hFri Dec 18 08:47:29 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#!wget -c -o texts.zip https://drive.google.com/u/0/uc?id=1-DlSSrRYH0aYdMzPY2XLt5dxH4kJWJDa&export=download\r\n",
    "#!curl  https://drive.google.com/u/0/uc?id=1-DlSSrRYH0aYdMzPY2XLt5dxH4kJWJDa&export=download\r\n",
    "#!curl https://studygrp-my.sharepoint.com/:u:/g/personal/ialamina_studygroup_com/EfBkRpv2QPZGq5MyaEVdRdEBcMtipH4o09GmzwGouZX76Q?e=D6eyDH > txts.zip\r\n",
    "!git clone https://github.com/kpu/kenlm.git\r\n",
    "!mkdir -p kenlm/build\r\n",
    "!cd kenlm/build && cmake .. && make -j 4\r\n",
    "!pip install https://github.com/kpu/kenlm/archive/master.zip\r\n",
    "#!source venv/bin/activate && pip install https://github.com/kpu/kenlm/archive/master.zip\r\n",
    "!wget --no-check-certificate -c -O txts.zip \"https://onedrive.live.com/download?cid=EB20651D09521520&resid=EB20651D09521520%21171763&authkey=AAAmYzTmYQLp4lU\"\r\n",
    "!unzip txts.zip\r\n",
    "!apt-get install python3-venv\r\n",
    "!python3 -m venv venv\r\n",
    "!git clone https://github.com/u1273400/rpynotes.git\r\n",
    "!source venv/bin/activate && pip install tensorflow-gpu==1.13.1 && pip list\r\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyXuaMU1gQpH",
    "outputId": "90fdaeb7-f487-4ca8-a173-52817ed3b482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==1.13.1\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "tcmalloc: large alloc 1073750016 bytes == 0x54f3a000 @  0x7f0bf9d9b2a4 0x591e47 0x5b7f6c 0x50a25a 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96\n",
      "Requirement already satisfied: protobuf>=3.6.1 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.13.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.13.1)\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow-gpu==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow-gpu==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.13.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.13.1)\n",
      "Requirement already satisfied: six>=1.10.0 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.13.1)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow-gpu==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/98/74a430566fdd9d4cc0386322e55306c8928a95da95b1da6fba08641526b5/grpcio-1.34.0.tar.gz\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow-gpu==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.13.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.13.1)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-gpu==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./venv/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "Collecting h5py (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in ./venv/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "Collecting cached-property; python_version < \"3.8\" (from h5py->keras-applications>=1.0.6->tensorflow-gpu==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in ./venv/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
      "Building wheels for collected packages: grpcio, termcolor\n",
      "  Running setup.py bdist_wheel for grpcio ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/cb/48/63/53113818695508567c6a4c702fc3c8c56c21be322a03962cb1\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built grpcio termcolor\n",
      "Installing collected packages: astor, mock, tensorflow-estimator, grpcio, tensorboard, cached-property, h5py, keras-applications, termcolor, tensorflow-gpu\n",
      "Successfully installed astor-0.8.1 cached-property-1.5.2 grpcio-1.34.0 h5py-3.1.0 keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1 termcolor-1.1.0\n",
      "Package              Version\n",
      "-------------------- -------\n",
      "absl-py              0.11.0 \n",
      "astor                0.8.1  \n",
      "cached-property      1.5.2  \n",
      "gast                 0.4.0  \n",
      "grpcio               1.34.0 \n",
      "h5py                 3.1.0  \n",
      "importlib-metadata   3.3.0  \n",
      "Keras-Applications   1.0.8  \n",
      "Keras-Preprocessing  1.1.2  \n",
      "Markdown             3.3.3  \n",
      "mock                 4.0.3  \n",
      "numpy                1.19.4 \n",
      "pip                  9.0.1  \n",
      "pkg-resources        0.0.0  \n",
      "protobuf             3.14.0 \n",
      "setuptools           39.0.1 \n",
      "six                  1.15.0 \n",
      "tensorboard          1.13.1 \n",
      "tensorflow-estimator 1.13.0 \n",
      "tensorflow-gpu       1.13.1 \n",
      "termcolor            1.1.0  \n",
      "typing-extensions    3.7.4.3\n",
      "Werkzeug             1.0.1  \n",
      "wheel                0.36.2 \n",
      "zipp                 3.4.0  \n"
     ]
    }
   ],
   "source": [
    "!source venv/bin/activate && pip install tensorflow-gpu==1.13.1 && pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Di_Z2Y68sgT",
    "outputId": "5ad870e0-e6fa-4c24-f374-cd973dafd3cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /content/rpynotes/ok-rnn/txteval/val/all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "tcmalloc: large alloc 1923129344 bytes == 0x565154bce000 @  0x7f0a1b5af1e7 0x565152ec67a2 0x565152e6151e 0x565152e402eb 0x565152e2c066 0x7f0a19748bf7 0x565152e2dbaa\n",
      "tcmalloc: large alloc 8974589952 bytes == 0x5651c75d8000 @  0x7f0a1b5af1e7 0x565152ec67a2 0x565152eb57ca 0x565152eb6208 0x565152e40308 0x565152e2c066 0x7f0a19748bf7 0x565152e2dbaa\n",
      "****************************************************************************************************\n",
      "Unigram tokens 599916 types 11414\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:136968 2:1065633600 3:1998063104 4:3196900864 5:4662147072\n",
      "tcmalloc: large alloc 4662149120 bytes == 0x565154bce000 @  0x7f0a1b5af1e7 0x565152ec67a2 0x565152eb57ca 0x565152eb6208 0x565152e408d7 0x565152e2c066 0x7f0a19748bf7 0x565152e2dbaa\n",
      "tcmalloc: large alloc 1998069760 bytes == 0x5652aa26a000 @  0x7f0a1b5af1e7 0x565152ec67a2 0x565152eb57ca 0x565152eb6208 0x565152e40cdd 0x565152e2c066 0x7f0a19748bf7 0x565152e2dbaa\n",
      "tcmalloc: large alloc 3196903424 bytes == 0x5653ded5a000 @  0x7f0a1b5af1e7 0x565152ec67a2 0x565152eb57ca 0x565152eb6208 0x565152e40cdd 0x565152e2c066 0x7f0a19748bf7 0x565152e2dbaa\n",
      "/content/kenlm/lm/builder/adjust_counts.cc:60 in void lm::builder::{anonymous}::StatCollector::CalculateDiscounts(const lm::builder::DiscountConfig&) threw BadDiscountException because `discounts_[i].amount[j] < 0.0 || discounts_[i].amount[j] > j'.\n",
      "ERROR: 5-gram discount out of range for adjusted count 2: -2.1905189.  This means modified Kneser-Ney smoothing thinks something is weird about your data.  To override this error for e.g. a class-based model, rerun with --discount_fallback\n",
      "\n",
      "/bin/bash: line 1: 14281 Aborted                 (core dumped) kenlm/build/bin/lmplz -o 5 < rpynotes/ok-rnn/txteval/val/all.txt > rpynotes/ok-rnn/txteval/val/text.arpa\n"
     ]
    }
   ],
   "source": [
    "#!apt-cache madison nvidia-cuda-dev\r\n",
    "!cp rpynotes/ok-rnn/txts -r rpynotes/ok-rnn/txteval\r\n",
    "!mkdir -p rpynotes/ok-rnn/txteval/val\r\n",
    "!mv rpynotes/ok-rnn/txteval/gal_eph_new.txt rpynotes/ok-rnn/txteval/val\r\n",
    "!cat rpynotes/ok-rnn/txteval/*.txt > rpynotes/ok-rnn/txteval/val/all.txt\r\n",
    "with open('rpynotes/ok-rnn/txteval/val/all.txt', encoding='utf-16') as f:\r\n",
    "  s=f.read()\r\n",
    "s=s.replace('.','\\n')\r\n",
    "with open('rpynotes/ok-rnn/txteval/val/all.txt', 'w') as f:\r\n",
    "  f.write(f'{s}')\r\n",
    "!kenlm/build/bin/lmplz -o 5 <rpynotes/ok-rnn/txteval/val/all.txt > rpynotes/ok-rnn/txteval/val/text.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rT-W9dentydb",
    "outputId": "6f115714-2ddb-4309-ef20-04fd78dce7cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /content/rpynotes/ok-rnn/txteval/val/all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "tcmalloc: large alloc 1923129344 bytes == 0x559433058000 @  0x7fc8dc2d41e7 0x559431a6a7a2 0x559431a0551e 0x5594319e42eb 0x5594319d0066 0x7fc8da46dbf7 0x5594319d1baa\n",
      "tcmalloc: large alloc 8974589952 bytes == 0x5594a5a62000 @  0x7fc8dc2d41e7 0x559431a6a7a2 0x559431a597ca 0x559431a5a208 0x5594319e4308 0x5594319d0066 0x7fc8da46dbf7 0x5594319d1baa\n",
      "****************************************************************************************************\n",
      "Unigram tokens 599916 types 11414\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:136968 2:1065633600 3:1998063104 4:3196900864 5:4662147072\n",
      "tcmalloc: large alloc 4662149120 bytes == 0x559433058000 @  0x7fc8dc2d41e7 0x559431a6a7a2 0x559431a597ca 0x559431a5a208 0x5594319e48d7 0x5594319d0066 0x7fc8da46dbf7 0x5594319d1baa\n",
      "tcmalloc: large alloc 1998069760 bytes == 0x5595886f4000 @  0x7fc8dc2d41e7 0x559431a6a7a2 0x559431a597ca 0x559431a5a208 0x5594319e4cdd 0x5594319d0066 0x7fc8da46dbf7 0x5594319d1baa\n",
      "tcmalloc: large alloc 3196903424 bytes == 0x5596bd1e4000 @  0x7fc8dc2d41e7 0x559431a6a7a2 0x559431a597ca 0x559431a5a208 0x5594319e4cdd 0x5594319d0066 0x7fc8da46dbf7 0x5594319d1baa\n",
      "Substituting fallback discounts for order 4: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 11414 D1=0.661239 D2=1.23023 D3+=1.28198\n",
      "2 75254 D1=0.719085 D2=1.26128 D3+=1.3872\n",
      "3 156591 D1=0.785233 D2=1.52296 D3+=1.2623\n",
      "4 197890 D1=0.831452 D2=1.75754 D3+=0.721526\n",
      "5 212530 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 14097 assuming -p 1.5\n",
      "probing 16660 assuming -r models -p 1.5\n",
      "trie     6418 without quantization\n",
      "trie     3363 assuming -q 8 -b 8 quantization \n",
      "trie     5857 assuming -a 22 array pointer compression\n",
      "trie     2802 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:136968 2:1204064 3:3131820 4:4749360 5:5950840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:136968 2:1204064 3:3131820 4:4749360 5:5950840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:14214452 kB\tVmRSS:1933872 kB\tRSSMax:1934028 kB\tuser:0.738002\tsys:0.907315\tCPU:1.64535\treal:1.50184\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz --discount_fallback -o 5 <rpynotes/ok-rnn/txteval/val/all.txt > rpynotes/ok-rnn/txteval/val/text.arpa\r\n",
    "#!cat rpynotes/ok-rnn/txteval/all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MNwsSaWRRVID",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "98390f2c-9156-403c-ad1a-4774f52a3086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "!cp rpynotes/ok-rnn/my_txtutils.py .\n",
    "from functools import reduce\n",
    "import my_txtutils\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import math\n",
    "SEQLEN = 30\n",
    "BATCHSIZE = 200\n",
    "ALPHASIZE = my_txtutils.ALPHASIZE\n",
    "INTERNALSIZE = 512\n",
    "NLAYERS = 3\n",
    "learning_rate = 0.001  # fixed learning rate\n",
    "dropout_pkeep = 0.8    # some dropout\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dJDVy3L6Lro",
    "outputId": "e4bf2307-16a0-4434-a4a5-46817ead9019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238.71607887366767\n"
     ]
    }
   ],
   "source": [
    "import kenlm\r\n",
    "with open('rpynotes/ok-rnn/txts/gal_eph_new.txt', encoding='utf-16') as f:\r\n",
    "  s=f.read()\r\n",
    "model = kenlm.Model('rpynotes/ok-rnn/txteval/val/text.arpa')\r\n",
    "print(model.perplexity(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "nlMlHz3494JZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.ticker as ticker\r\n",
    "# product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "q7cQOQIGLHcd"
   },
   "outputs": [],
   "source": [
    "# my_txtutils.convert_from_alphabet(ord(\"L\")) rnn_train_1608267268-40000000\r\n",
    "author='rpynotes/ok-rnn/checkpoints/rnn_train_1608286151-55000000'\r\n",
    "def get_scores(corpora, use_log=False):\r\n",
    "  probs=[]\r\n",
    "  with tf.Session() as sess:\r\n",
    "      # new_saver = tf.train.import_meta_graph('checkpoints/rnn_train_1512567262-0.meta')\r\n",
    "      new_saver = tf.train.import_meta_graph(author+'.meta')\r\n",
    "      new_saver.restore(sess, author)\r\n",
    "      x = my_txtutils.convert_from_alphabet(ord(corpora[0]))\r\n",
    "      x = np.array([[x]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\r\n",
    "\r\n",
    "      # initial values\r\n",
    "      y = x\r\n",
    "      h = np.zeros([1, INTERNALSIZE * NLAYERS], dtype=np.float32)  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\r\n",
    "      for i in range(1,len(corpora)):\r\n",
    "          yo, h = sess.run(['Yo:0', 'H:0'], feed_dict={'X:0': y, 'pkeep:0': 1., 'Hin:0': h, 'batchsize:0': 1})\r\n",
    "          #!nvidia-smi\r\n",
    "          next_ord =  my_txtutils.convert_from_alphabet(ord(corpora[i]))\r\n",
    "          # If sampling is bedone from the topn most likely characters, the generated text\r\n",
    "          # is more credible and more \"english\". If topn is not set, it defaults to the full\r\n",
    "          # distribution (ALPHASIZE)\r\n",
    "          #print(yo.shape)\r\n",
    "          if use_log:\r\n",
    "            probs.append(math.log(yo[0][next_ord]))\r\n",
    "          else:\r\n",
    "            probs.append(yo[0][next_ord])\r\n",
    "          # Recommended: topn = 10 for intermediate checkpoints, topn=2 or 3 for fully trained checkpoints\r\n",
    "\r\n",
    "          # c = my_txtutils.sample_from_probabilities(yo, topn=2)\r\n",
    "          y = np.array([[next_ord]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\r\n",
    "          # c = chr(my_txtutils.convert_to_alphabet(c))\r\n",
    "      return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "z7u3IIMcGNXt"
   },
   "outputs": [],
   "source": [
    "def cppx(scores, corpus):\r\n",
    "    nw=len(corpus.split())\r\n",
    "    nc=len(corpus)\r\n",
    "    c=math.exp(sum(scores)/nc)\r\n",
    "    print(c,nw,nc)\r\n",
    "    return 2**(c*nc/nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDuJQ1nzlrj5",
    "outputId": "094ee95b-2117-459e-8f17-564cc00c2bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041666666666666664"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\r\n",
    "np.prod(1/np.array([1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GcdgJmnIlsXx"
   },
   "outputs": [],
   "source": [
    "def cppx2(scores, corpus):\r\n",
    "    nw=len(corpus.split())\r\n",
    "    nc=len(corpus)\r\n",
    "    ip=np.prod(1/np.array(scores))\r\n",
    "    c=ip**(1/nc)\r\n",
    "    print(ip)\r\n",
    "    print(c,nw,nc)\r\n",
    "    return 2**(c*nc/nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nn4sUvwDK0Wr",
    "outputId": "be103d63-09cf-4ddc-b29f-434d917ba1d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rpynotes/ok-rnn/checkpoints/rnn_train_1608286151-55000000\n",
      "inf\n",
      "inf 25951 136706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('rpynotes/ok-rnn/txts/gal_eph_new.txt', encoding='utf-16') as f:\r\n",
    "  s=f. read()\r\n",
    "scores = get_scores(s)\r\n",
    "cppx2(scores,s)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EhBRfJNz1eAi",
    "outputId": "58f1b766-d870-40b7-d966-8f194720ff2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.818921113100376 25951 136706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.889765523477507"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score(s):\r\n",
    "    return sum(prob for prob, _, _ in model.full_scores(s))   \r\n",
    "\r\n",
    "nw=len(s.split())\r\n",
    "nc=len(s)\r\n",
    "ip=np.sum(-np.log(scores))/nc\r\n",
    "print(ip,nw,nc)\r\n",
    "2**(ip*nc/nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "1XQk3HyoRVH7",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# import tensorflow as tf\n",
    "from tensorflow.compat.v1 import layers\n",
    "\n",
    "from tensorflow.keras import layers as rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import my_txtutils as txt\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Szl6kEiXdSZz",
    "outputId": "4aa29160-4c24-4550-ac0d-d0915142d560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/content/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/content/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/content/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/content/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/content/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Loading file txts/jam_jud_new.txt\n",
      "Loading file txts/acts_new.txt\n",
      "Loading file txts/gal_eph_new.txt\n",
      "Loading file txts/thes_tim_new.txt\n",
      "Loading file txts/jud_rev_new.txt\n",
      "Loading file txts/luke_8_john_new.txt\n",
      "Loading file txts/tit_phl_new.txt\n",
      "Loading file txts/phil_col_new.txt\n",
      "Loading file txts/heb_new.txt\n",
      "Loading file txts/matt_new.txt\n",
      "Loading file txts/matt02_new.txt\n",
      "Loading file txts/mark01_new.txt\n",
      "Loading file txts/john_new.txt\n",
      "Training text size is 2.83MB with 283.09KB set aside for validation. There will be 296 batches per epoch\n",
      "Traceback (most recent call last):\n",
      "  File \"rnn_train.py\", line 61, in <module>\n",
      "    lr = tf.placeholder(tf.float32, name='lr')  # learning rate\n",
      "  File \"/content/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2074, in placeholder\n",
      "    raise RuntimeError(\"tf.placeholder() is not compatible with \"\n",
      "RuntimeError: tf.placeholder() is not compatible with eager execution.\n"
     ]
    }
   ],
   "source": [
    "!cd rpynotes/ok-rnn/ && source ../../venv/bin/activate && python rnn_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUEDK0rg47zJ"
   },
   "outputs": [],
   "source": [
    "plt.figure()\r\n",
    "plt.plot(all_losses)\r\n",
    "plt.plot(vloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "_csE-nDVRVIE",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# size of the alphabet that we work with\n",
    "ALPHASIZE = 98\n",
    "\n",
    "\n",
    "# Specification of the supported alphabet (subset of ASCII-7)\n",
    "# 10 line feed LF\n",
    "# 32-64 numbers and punctuation\n",
    "# 65-90 upper-case letters\n",
    "# 91-97 more punctuation\n",
    "# 97-122 lower-case letters\n",
    "# 123-126 more punctuation\n",
    "def convert_from_alphabet(a):\n",
    "    \"\"\"Encode a character\n",
    "    :param a: one character\n",
    "    :return: the encoded value\n",
    "    \"\"\"\n",
    "    if a == 9:\n",
    "        return 1\n",
    "    if a == 10:\n",
    "        return 127 - 30  # LF\n",
    "    elif 32 <= a <= 126:\n",
    "        return a - 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "# encoded values:\n",
    "# unknown = 0\n",
    "# tab = 1\n",
    "# space = 2\n",
    "# all chars from 32 to 126 = c-30\n",
    "# LF mapped to 127-30\n",
    "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode a code point\n",
    "    :param c: code point\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return: decoded character\n",
    "    \"\"\"\n",
    "    if c == 1:\n",
    "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
    "    if c == 127 - 30:\n",
    "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
    "    if 32 <= c + 30 <= 126:\n",
    "        return c + 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "def encode_text(s):\n",
    "    \"\"\"Encode a string.\n",
    "    :param s: a text string\n",
    "    :return: encoded list of code points\n",
    "    \"\"\"\n",
    "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
    "\n",
    "\n",
    "def decode_to_text(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode an encoded string.\n",
    "    :param c: encoded list of code points\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
    "\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(probabilities)\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
    "\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "\n",
    "\n",
    "def find_book(index, bookranges):\n",
    "    return next(\n",
    "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def find_book_index(index, bookranges):\n",
    "    return next(\n",
    "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
    "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
    "    print()\n",
    "    # epoch_size in number of batches\n",
    "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
    "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
    "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "    for k in range(batch_size):\n",
    "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
    "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
    "        bookname = find_book(index_in_epoch, bookranges)\n",
    "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
    "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
    "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
    "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
    "        print(print_string.format(decx, decy, loss_string))\n",
    "        index += sequence_len\n",
    "    # box formatting characters:\n",
    "    # │ \\u2502\n",
    "    # ─ \\u2500\n",
    "    # └ \\u2514\n",
    "    # ┘ \\u2518\n",
    "    # ┴ \\u2534\n",
    "    # ┌ \\u250C\n",
    "    # ┐ \\u2510\n",
    "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
    "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
    "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
    "    print(footer)\n",
    "    # print statistics\n",
    "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
    "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
    "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
    "    print()\n",
    "    print(\"TRAINING STATS: {}\".format(stats))\n",
    "\n",
    "\n",
    "class Progress:\n",
    "    \"\"\"Text mode progress bar.\n",
    "    Usage:\n",
    "            p = Progress(30)\n",
    "            p.step()\n",
    "            p.step()\n",
    "            p.step(start=True) # to restart form 0%\n",
    "    The progress bar displays a new header at each restart.\"\"\"\n",
    "    def __init__(self, maxi, size=100, msg=\"\"):\n",
    "        \"\"\"\n",
    "        :param maxi: the number of steps required to reach 100%\n",
    "        :param size: the number of characters taken on the screen by the progress bar\n",
    "        :param msg: the message displayed in the header of the progress bat\n",
    "        \"\"\"\n",
    "        self.maxi = maxi\n",
    "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
    "        self.header_printed = False\n",
    "        self.msg = msg\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, reset=False):\n",
    "        if reset:\n",
    "            self.__init__(self.maxi, self.size, self.msg)\n",
    "        if not self.header_printed:\n",
    "            self.__print_header()\n",
    "        next(self.p)\n",
    "\n",
    "    def __print_header(self):\n",
    "        print()\n",
    "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
    "        print(format_string.format(self.msg))\n",
    "        self.header_printed = True\n",
    "\n",
    "    def __start_progress(self, maxi):\n",
    "        def print_progress():\n",
    "            # Bresenham's algorithm. Yields the number of dots printed.\n",
    "            # This will always print 100 dots in max invocations.\n",
    "            dx = maxi\n",
    "            dy = self.size\n",
    "            d = dy - dx\n",
    "            for x in range(maxi):\n",
    "                k = 0\n",
    "                while d >= 0:\n",
    "                    print('=', end=\"\", flush=True)\n",
    "                    k += 1\n",
    "                    d -= dx\n",
    "                d += dy\n",
    "                yield k\n",
    "\n",
    "        return print_progress\n",
    "\n",
    "\n",
    "def read_data_files(directory, validation=True):\n",
    "    \"\"\"Read data files according to the specified glob pattern\n",
    "    Optionnaly set aside the last file as validation data.\n",
    "    No validation data is returned if there are 5 files or less.\n",
    "    :param directory: for example \"data/*.txt\"\n",
    "    :param validation: if True (default), sets the last file aside as validation data\n",
    "    :return: training data, validation data, list of loaded file names with ranges\n",
    "     If validation is\n",
    "    \"\"\"\n",
    "    codetext = []\n",
    "    bookranges = []\n",
    "    shakelist = glob.glob(directory, recursive=True)\n",
    "    for shakefile in shakelist:\n",
    "        shaketext = open(shakefile, \"r\")\n",
    "        print(\"Loading file \" + shakefile)\n",
    "        start = len(codetext)\n",
    "        codetext.extend(encode_text(shaketext.read()))\n",
    "        end = len(codetext)\n",
    "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
    "        shaketext.close()\n",
    "\n",
    "    if len(bookranges) == 0:\n",
    "        sys.exit(\"No training data has been found. Aborting.\")\n",
    "\n",
    "    # For validation, use roughly 90K of text,\n",
    "    # but no more than 10% of the entire text\n",
    "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "    # 10% of the text is how many files ?\n",
    "    total_len = len(codetext)\n",
    "    validation_len = 0\n",
    "    nb_books1 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books1 += 1\n",
    "        if validation_len > total_len // 10:\n",
    "            break\n",
    "\n",
    "    # 90K of text is how many books ?\n",
    "    validation_len = 0\n",
    "    nb_books2 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books2 += 1\n",
    "        if validation_len > 90*1024:\n",
    "            break\n",
    "\n",
    "    # 20% of the books is how many books ?\n",
    "    nb_books3 = len(bookranges) // 5\n",
    "\n",
    "    # pick the smallest\n",
    "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
    "\n",
    "    if nb_books == 0 or not validation:\n",
    "        cutoff = len(codetext)\n",
    "    else:\n",
    "        cutoff = bookranges[-nb_books][\"start\"]\n",
    "    valitext = codetext[cutoff:]\n",
    "    codetext = codetext[:cutoff]\n",
    "    return codetext, valitext, bookranges\n",
    "\n",
    "\n",
    "def print_data_stats(datalen, valilen, epoch_size):\n",
    "    datalen_mb = datalen/1024.0/1024.0\n",
    "    valilen_kb = valilen/1024.0\n",
    "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
    "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
    "\n",
    "\n",
    "def print_validation_header(validation_start, bookranges):\n",
    "    bookindex = find_book_index(validation_start, bookranges)\n",
    "    books = ''\n",
    "    for i in range(bookindex, len(bookranges)):\n",
    "        books += bookranges[i][\"name\"]\n",
    "        if i < len(bookranges)-1:\n",
    "            books += \", \"\n",
    "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
    "\n",
    "\n",
    "def print_validation_stats(loss, accuracy):\n",
    "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
    "                                                                                                           accuracy))\n",
    "\n",
    "\n",
    "def print_text_generation_header():\n",
    "    print()\n",
    "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
    "\n",
    "\n",
    "def print_text_generation_footer():\n",
    "    print()\n",
    "    print(\"└{:─^111}┘\".format('End of generation'))\n",
    "\n",
    "\n",
    "def frequency_limiter(n, multiple=1, modulo=0):\n",
    "    def limit(i):\n",
    "        return i % (multiple * n) == modulo*multiple\n",
    "    return limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "35M78bzyRVIK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
    "shakedir = \"txts/*.txt\"\n",
    "#shakedir = \"../tensorflow/**/*.py\"\n",
    "codetext, valitext, bookranges = read_data_files(shakedir, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Adzq9ynZRVIP"
   },
   "outputs": [],
   "source": [
    "# display some stats on the data\n",
    "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
    "print_data_stats(len(codetext), len(valitext), epoch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "l1EHlKFyRVIQ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# the model (see FAQ in README.md)\n",
    "#\n",
    "lr = tf.placeholder(tf.float32, name='lr')  # learning rate\n",
    "pkeep = tf.placeholder(tf.float32, name='pkeep')  # dropout parameter\n",
    "batchsize = tf.placeholder(tf.int32, name='batchsize')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "AN4GZdTVRVIQ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "X = tf.placeholder(tf.uint8, [None, None], name='X')    # [ BATCHSIZE, SEQLEN ]\n",
    "Xo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n",
    "# expected outputs = same sequence shifted by 1 since we are trying to predict the next character\n",
    "Y_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  # [ BATCHSIZE, SEQLEN ]\n",
    "Yo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n",
    "# input state\n",
    "Hin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
    "\n",
    "# using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times\n",
    "# dynamic_rnn infers SEQLEN from the size of the inputs Xo\n",
    "\n",
    "# How to properly apply dropout in RNNs: see README.md\n",
    "cells = [rnn.GRUCell(INTERNALSIZE) for _ in range(NLAYERS)]\n",
    "# \"naive dropout\" implementation\n",
    "dropcells = [rnn.DropoutWrapper(cell,input_keep_prob=pkeep) for cell in cells]\n",
    "multicell = rnn.MultiRNNCell(dropcells, state_is_tuple=False)\n",
    "multicell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)  # dropout for the softmax layer\n",
    "\n",
    "Yr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)\n",
    "# Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]\n",
    "# H:  [ BATCHSIZE, INTERNALSIZE*NLAYERS ] # this is the last state in the sequence\n",
    "\n",
    "H = tf.identity(H, name='H')  # just to give it a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC42BYLmRVIR"
   },
   "outputs": [],
   "source": [
    "# Softmax layer implementation:\n",
    "# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n",
    "# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n",
    "\n",
    "Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\n",
    "Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\n",
    "loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\n",
    "Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\n",
    "Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "XW1cFi-fRVIR",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# stats for display\n",
    "seqloss = tf.reduce_mean(loss, 1)\n",
    "batchloss = tf.reduce_mean(seqloss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\n",
    "loss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\n",
    "acc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\n",
    "summaries = tf.summary.merge([loss_summary, acc_summary])\n",
    "\n",
    "# Init Tensorboard stuff. This will save Tensorboard information into a different\n",
    "# folder at each run named 'log/<timestamp>/'. Two sets of data are saved so that\n",
    "# you can compare training and validation curves visually in Tensorboard.\n",
    "timestamp = str(math.trunc(time.time()))\n",
    "summary_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-training\")\n",
    "validation_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-validation\")\n",
    "\n",
    "# Init for saving models. They will be saved into a directory named 'checkpoints'.\n",
    "# Only the last checkpoint is kept.\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")\n",
    "saver = tf.train.Saver(max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "T9mni8WPRVIR",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for display: init the progress bar\n",
    "DISPLAY_FREQ = 50\n",
    "_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n",
    "progress = txt.Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n",
    "\n",
    "# init\n",
    "istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "NcUqzGKsRVIS",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10):\n",
    "\n",
    "    # train on one minibatch\n",
    "    feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n",
    "    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n",
    "\n",
    "    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n",
    "    if step % _50_BATCHES == 0:\n",
    "        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n",
    "        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n",
    "        summary_writer.add_summary(smm, step)\n",
    "\n",
    "    # run a validation step every 50 batches\n",
    "    # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n",
    "    # so we cut it up and batch the pieces (slightly inaccurate)\n",
    "    # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n",
    "    if step % _50_BATCHES == 0 and len(valitext) > 0:\n",
    "        VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n",
    "        bsize = len(valitext) // VALI_SEQLEN\n",
    "        txt.print_validation_header(len(codetext), bookranges)\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n",
    "        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n",
    "                     batchsize: bsize}\n",
    "        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_validation_stats(ls, acc)\n",
    "        # save validation data for Tensorboard\n",
    "        validation_writer.add_summary(smm, step)\n",
    "\n",
    "    # display a short text generated with the current weights and biases (every 150 batches)\n",
    "    if step // 3 % _50_BATCHES == 0:\n",
    "        txt.print_text_generation_header()\n",
    "        ry = np.array([[txt.convert_from_alphabet(ord(\"K\"))]])\n",
    "        rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n",
    "        for k in range(1000):\n",
    "            ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n",
    "            rc = txt.sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n",
    "            print(chr(txt.convert_to_alphabet(rc)), end=\"\")\n",
    "            ry = np.array([[rc]])\n",
    "        txt.print_text_generation_footer()\n",
    "\n",
    "    # save a checkpoint (every 500 batches)\n",
    "    if step // 10 % _50_BATCHES == 0:\n",
    "        saved_file = saver.save(sess, 'checkpoints/rnn_train_' + timestamp, global_step=step)\n",
    "        print(\"Saved file: \" + saved_file)\n",
    "\n",
    "    # display progress bar\n",
    "    progress.step(reset=step % _50_BATCHES == 0)\n",
    "\n",
    "    # loop state around\n",
    "    istate = ostate\n",
    "    step += BATCHSIZE * SEQLEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAsLeYjPRVIU"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "ok_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
