{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import my_txtutils\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acts_new.txt', 'gal_eph_new.txt', 'heb_new.txt', 'jam_jud_new.txt', 'john_new.txt', 'jud_rev_new.txt', 'luke_8_john_new.txt', 'mark01_new.txt', 'matt02_new.txt', 'matt_new.txt', 'phil_col_new.txt', 'thes_tim_new.txt', 'tit_phl_new.txt']\n"
     ]
    }
   ],
   "source": [
    "mypath='./new_txts/'\n",
    "txts='./txts/'\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(txts) if isfile(join(txts, f))]\n",
    "print(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#large files\n",
    "def large_f():\n",
    "    #filenames = ['file1.txt', 'file2.txt', ...]\n",
    "    with open('path/to/output/file', 'w') as outfile:\n",
    "        for fname in onlyfiles:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#small files\n",
    "def small_f():\n",
    "    #filenames = ['file1.txt', 'file2.txt', ...]\n",
    "    with open(mypath+'full.txt', 'w') as outfile:\n",
    "        for fname in onlyfiles:\n",
    "            print('writing file',fname)\n",
    "            with open(txts+fname) as infile:\n",
    "                outfile.write(infile.read())\n",
    "    return open(mypath+\"full.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file acts_new.txt\n",
      "writing file gal_eph_new.txt\n",
      "writing file heb_new.txt\n",
      "writing file jam_jud_new.txt\n",
      "writing file john_new.txt\n",
      "writing file jud_rev_new.txt\n",
      "writing file luke_8_john_new.txt\n",
      "writing file mark01_new.txt\n",
      "writing file matt02_new.txt\n",
      "writing file matt_new.txt\n",
      "writing file phil_col_new.txt\n",
      "writing file thes_tim_new.txt\n",
      "writing file tit_phl_new.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "649190"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=small_f()\n",
    "words=re.sub(' +', ' ',txt.replace('\\n',' ')).split(' ')\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6507438"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0721 11:44:55.502967 4466668992 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n",
      "[[0.02731734 0.00911933 0.02987786 0.00783274 0.00781383 0.00791675\n",
      "  0.00792072 0.00797469 0.00883834 0.00889832 0.00916244 0.00986102\n",
      "  0.00820671 0.00829697 0.01128333 0.01053004 0.01098963 0.00840346\n",
      "  0.00767191 0.00813099 0.00805331 0.00803352 0.00793198 0.00814621\n",
      "  0.00811185 0.00792489 0.00794537 0.00821406 0.00825582 0.00800292\n",
      "  0.00787085 0.00767554 0.0081084  0.0079169  0.00810332 0.00786511\n",
      "  0.00813212 0.00800238 0.00778155 0.0079629  0.00791606 0.00817212\n",
      "  0.00830134 0.0081915  0.0080427  0.00812944 0.00783252 0.00802331\n",
      "  0.00802122 0.00825228 0.0078577  0.00826596 0.00796773 0.00799554\n",
      "  0.00809396 0.00801248 0.00795189 0.00826468 0.00790384 0.00775188\n",
      "  0.00811648 0.00898477 0.00874621 0.0090804  0.00801829 0.00848367\n",
      "  0.00828231 0.0197167  0.01689533 0.01080387 0.01321569 0.0178067\n",
      "  0.01208614 0.01350665 0.0111219  0.02097857 0.01208391 0.01571456\n",
      "  0.01318455 0.01616486 0.01680871 0.01884054 0.01438852 0.00845549\n",
      "  0.01440457 0.01513813 0.0132025  0.01473367 0.00941686 0.01300005\n",
      "  0.00800919 0.01342627 0.01028628 0.00791034 0.00808766 0.00777887\n",
      "  0.00800569 0.00808159]]\n"
     ]
    }
   ],
   "source": [
    "# these must match what was saved !\n",
    "ALPHASIZE = my_txtutils.ALPHASIZE\n",
    "NLAYERS = 3\n",
    "INTERNALSIZE = 512\n",
    "\n",
    "ok02 =\"checkpoints/rnn_train_1560839489-30000000\" # okrika 2019-06-18\n",
    "ok03 =\"checkpoints/rnn_train_1563601394-60000000\" # okrika 2019-07-20\n",
    "\n",
    "author=ok03\n",
    "\n",
    "ncnt = 0\n",
    "with tf.Session() as sess:\n",
    "    # new_saver = tf.train.import_meta_graph('checkpoints/rnn_train_1512567262-0.meta')\n",
    "    new_saver = tf.train.import_meta_graph(author+'.meta')\n",
    "    new_saver.restore(sess, author)\n",
    "    x = my_txtutils.convert_from_alphabet(ord(\"L\"))\n",
    "    x = np.array([[x]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "\n",
    "    # initial values\n",
    "    y = x\n",
    "    h = np.zeros([1, INTERNALSIZE * NLAYERS], dtype=np.float32)  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
    "    #for i in range(10):\n",
    "    yo, h = sess.run(['Yo:0', 'H:0'], feed_dict={'X:0': y, 'pkeep:0': 1., 'Hin:0': h, 'batchsize:0': 1})\n",
    "    print(tf.shape(yo))\n",
    "    print(yo)\n",
    "\n",
    "# Softmax layer implementation:\n",
    "# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n",
    "# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n",
    "'''\n",
    "Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\n",
    "Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\n",
    "loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\n",
    "Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\n",
    "Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[46]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(my_txtutils.convert_from_alphabet(ord(\"L\")))\n",
    "my_txtutils.encode_text(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÿþ \\x00o\\x00k\\x00u\\x00n\\x00w\\x00e\\x00n\\x00g\\x00i\\x00a\\x00p\\x00u\\x00 \\x00m\\x00i\\x00e\\x00y\\x00e\\x00 \\x00m\\x00a\\x00 \\x00 \\x00d\\x00i\\x00e\\x00p\\x00a\\x00k\\x00u\\x00m\\x00a\\x00y\\x00e\\x00 \\x00 \\x00o\\x00k\\x00u\\x00n\\x00w\\x00e\\x00n\\x00g\\x00i\\x00a\\x00p\\x00u\\x00'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[0:100]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        # If sampling is be done from the topn most likely characters, the generated text\n",
    "        # is more credible and more \"english\". If topn is not set, it defaults to the full\n",
    "        # distribution (ALPHASIZE)\n",
    "\n",
    "        # Recommended: topn = 10 for intermediate checkpoints, topn=2 or 3 for fully trained checkpoints\n",
    "\n",
    "        c = my_txtutils.sample_from_probabilities(yo, topn=2)\n",
    "        y = np.array([[c]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "        c = chr(my_txtutils.convert_to_alphabet(c))\n",
    "        print(c, end=\"\")\n",
    "\n",
    "        if c == '\\n':\n",
    "            ncnt = 0\n",
    "        else:\n",
    "            ncnt += 1\n",
    "        if ncnt == 100:\n",
    "            print(\"\")\n",
    "            ncnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
