{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import my_txtutils\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acts_new.txt', 'gal_eph_new.txt', 'heb_new.txt', 'jam_jud_new.txt', 'john_new.txt', 'jud_rev_new.txt', 'luke_8_john_new.txt', 'mark01_new.txt', 'matt02_new.txt', 'matt_new.txt', 'phil_col_new.txt', 'thes_tim_new.txt', 'tit_phl_new.txt']\n"
     ]
    }
   ],
   "source": [
    "mypath='./new_txts/'\n",
    "txts='./txts/'\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(txts) if isfile(join(txts, f))]\n",
    "print(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#large files\n",
    "def large_f():\n",
    "    #filenames = ['file1.txt', 'file2.txt', ...]\n",
    "    with open('path/to/output/file', 'w') as outfile:\n",
    "        for fname in onlyfiles:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#small files\n",
    "def small_f():\n",
    "    #filenames = ['file1.txt', 'file2.txt', ...]\n",
    "    with open(mypath+'full.txt', 'w') as outfile:\n",
    "        for fname in onlyfiles:\n",
    "            print('writing file',fname)\n",
    "            with open(txts+fname) as infile:\n",
    "                outfile.write(infile.read())\n",
    "    return open(mypath+\"full.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file acts_new.txt\n",
      "writing file gal_eph_new.txt\n",
      "writing file heb_new.txt\n",
      "writing file jam_jud_new.txt\n",
      "writing file john_new.txt\n",
      "writing file jud_rev_new.txt\n",
      "writing file luke_8_john_new.txt\n",
      "writing file mark01_new.txt\n",
      "writing file matt02_new.txt\n",
      "writing file matt_new.txt\n",
      "writing file phil_col_new.txt\n",
      "writing file thes_tim_new.txt\n",
      "writing file tit_phl_new.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "649190"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=small_f()\n",
    "words=re.sub(' +', ' ',txt.replace('\\n',' ')).split(' ')\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6507438"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/rnn_train_1563601394-60000000\n",
      " i\u0000 \u0000i\u0000 \u0000i\u0000 \u0000i\u0000i\u0000 \u0000 \u0000 \u0000 \u0000 \u0000i\u0000i\u0000 \u0000 \u0000 \u0000i\u0000 \u0000 \u0000i\u0000 \u0000 \u0000 \u0000i\u0000 \u0000 \u0000 \u0000 \u0000i\u0000i\u0000i\u0000i\u0000 \u0000 \u0000i\u0000i\u0000 \u0000 \u0000 \u0000 \u0000i\u0000 \u0000i\u0000i\u0000 \u0000 \u0000 \u0000i\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000i\u0000i\u0000i\u0000i\u0000 \u0000i\u0000 \u0000 \u0000i\u0000 \u0000 \u0000 \u0000 \u0000i\u0000  i\u0000i\u0000 \u0000i\u0000i\u0000i\u0000 \u0000 \u0000 \u0000i\u0000i\u0000 \u0000 \u0000i\u0000i\u0000i\u0000 \u0000i\u0000i\u0000 \u0000 \u0000 \u0000 \u0000i\u0000i\u0000 \u0000 \u0000i\u0000i\u0000i\u0000 \n",
      "\u0000 \u0000i\u0000i\u0000i\u0000i\u0000 \u0000 \u0000i\u0000 \u0000i\u0000i\u0000 \u0000 \u0000 \u0000i\u0000i\u0000 \u0000 \u0000 \u0000i\u0000 \u0000i\u0000i\u0000i\u0000i\u0000 \u0000i\u0000i\u0000 \u0000 \u0000 \u0000 \u0000 \u0000i\u0000 \u0000i\u0000i\u0000 \u0000 \u0000 \u0000i\u0000 \u0000i\u0000i\u0000 \u0000i\u0000i\u0000 \u0000 \u0000i\n",
      "\u0000i\u0000 \u0000i\u0000 \u0000 \u0000 \u0000 \u0000 \u0000i\u0000 \u0000 \u0000 \u0000i\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000i\u0000i\u0000 \u0000i\u0000i\u0000 \u0000 \u0000i\u0000i\u0000 \u0000i\u0000 \u0000i\u0000 \u0000i\u0000 \u0000 \u0000i\u0000i\u0000 \u0000i\u0000 \u0000i\u0000i\u0000i\u0000 \u0000 \u0000 \u0000i\n",
      "\u0000i\u0000 \u0000i\u0000 \u0000i\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000i\u0000 \u0000 \u0000i\u0000 \u0000 \u0000i\u0000 \u0000i\u0000 \u0000a\u0000 \u0000a\u0000a\u0000a\u0000a\u0000a\u0000 \u0000 \u0000a\u0000a\u0000a\u0000a\u0000 \u0000a\u0000 \u0000 \u0000a\u0000 \u0000a\u0000a\u0000 \u0000 \u0000a\u0000 \u0000a\u0000 \u0000a\u0000a\n",
      "\u0000 \u0000a\u0000 \u0000 \u0000a\u0000 \u0000 \u0000 \u0000 \u0000a\u0000 \u0000a\u0000 \u0000 \u0000 \u0000 \u0000 \u0000a\u0000a\u0000a\u0000a\u0000 \u0000 \u0000a\u0000 \u0000a\u0000a\u0000 \u0000 \u0000 \u0000 \u0000 \u0000a\u0000a\u0000a\u0000 \u0000 \u0000a\u0000a\u0000 \u0000 \u0000 \u0000 \u0000a\u0000 \u0000a\u0000a\u0000 \u0000 \u0000 \n",
      "\u0000a\u0000a\u0000 \u0000 \u0000a\u0000a\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000a\u0000 \u0000 \u0000 \u0000 \u0000a\u0000a\u0000 \u0000a\u0000a\u0000a\u0000 \u0000a\u0000a\u0000 \u0000a\u0000a\u0000 \u0000a\u0000a\u0000a\u0000a\u0000 \u0000 \u0000a\u0000a\u0000 \u0000a\u0000 \u0000a\u0000 \u0000 \u0000 \u0000 \u0000 \u0000a\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000a\u0000a\u0000a\u0000 \u0000a\u0000 \u0000a\u0000a\u0000a\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000a\u0000a\u0000a\u0000a\u0000a\u0000a\u0000a\u0000 \u0000 \u0000a\u0000a\u0000 \u0000 \u0000a\u0000a\u0000a\u0000 \u0000 \u0000a\u0000 \u0000a\u0000 \u0000 \u0000a\u0000 \u0000 \u0000 \u0000 \u0000 \u0000a\u0000 \n",
      "\u0000a\u0000 \u0000 \u0000 \u0000a\u0000 \u0000a\u0000a\u0000 \u0000 \u0000a\u0000a\u0000 \u0000a\u0000a\u0000a\u0000 \u0000a\u0000 \u0000a\u0000a\u0000a\u0000 \u0000 \u0000a\u0000 \u0000a\u0000 \u0000 \u0000 \u0000a\u0000 \u0000 \u0000 \u0000 \u0000a\u0000a\u0000 \u0000 \u0000 \u0000 \u0000a\u0000 \u0000a\u0000a\u0000 \u0000a\u0000 \u0000 \u0000a\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000a\u0000 \u0000 \u0000a\u0000a\u0000a\u0000 \u0000 \u0000a\u0000 \u0000a\u0000 \u0000 \u0000a\u0000 \u0000a\u0000 \u0000 \u0000a\u0000a\u0000 \u0000 \u0000a\u0000 \u0000 \u0000a\u0000 \u0000 \u0000 \u0000 \u0000a\u0000 \u0000a\u0000a\u0000a\u0000 \u0000a\u0000a\u0000a\u0000a\u0000 \u0000a\u0000 \u0000 \u0000 \u0000 \n"
     ]
    }
   ],
   "source": [
    "# these must match what was saved !\n",
    "ALPHASIZE = my_txtutils.ALPHASIZE\n",
    "NLAYERS = 3\n",
    "INTERNALSIZE = 512\n",
    "\n",
    "ok02 =\"checkpoints/rnn_train_1560839489-30000000\" # okrika 2019-06-18\n",
    "ok03 =\"checkpoints/rnn_train_1563601394-60000000\" # okrika 2019-07-20\n",
    "\n",
    "author=ok03\n",
    "\n",
    "ncnt = 0\n",
    "with tf.Session() as sess:\n",
    "    # new_saver = tf.train.import_meta_graph('checkpoints/rnn_train_1512567262-0.meta')\n",
    "    new_saver = tf.train.import_meta_graph(author+'.meta')\n",
    "    new_saver.restore(sess, author)\n",
    "    x = my_txtutils.convert_from_alphabet(ord(\"L\"))\n",
    "    x = np.array([[x]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "\n",
    "    # initial values\n",
    "    y = x\n",
    "    h = np.zeros([1, INTERNALSIZE * NLAYERS], dtype=np.float32)  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
    "    for i in range(1000):\n",
    "        yo, h = sess.run(['Yo:0', 'H:0'], feed_dict={'X:0': y, 'pkeep:0': 1., 'Hin:0': h, 'batchsize:0': 1})\n",
    "        #print(tf.shape(yo))\n",
    "        #print(len(yo))\n",
    "        c = my_txtutils.sample_from_probabilities(yo, topn=2)\n",
    "        #y = np.array([[c]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "        c = chr(my_txtutils.convert_to_alphabet(c))\n",
    "        print(c, end=\"\")\n",
    "        if c == '\\n':\n",
    "            ncnt = 0\n",
    "        else:\n",
    "            ncnt += 1\n",
    "        if ncnt == 100:\n",
    "            print(\"\")\n",
    "            ncnt = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pcx(txt):\n",
    "    # these must match what was saved !\n",
    "    ALPHASIZE = my_txtutils.ALPHASIZE\n",
    "    NLAYERS = 3\n",
    "    INTERNALSIZE = 512\n",
    "\n",
    "    ok02 =\"checkpoints/rnn_train_1560839489-30000000\" # okrika 2019-06-18\n",
    "    ok03 =\"checkpoints/rnn_train_1563601394-60000000\" # okrika 2019-07-20\n",
    "\n",
    "    author=ok03\n",
    "\n",
    "    ncnt = 0\n",
    "    with tf.Session() as sess:\n",
    "        # new_saver = tf.train.import_meta_graph('checkpoints/rnn_train_1512567262-0.meta')\n",
    "        new_saver = tf.train.import_meta_graph(author+'.meta')\n",
    "        new_saver.restore(sess, author)\n",
    "        x = my_txtutils.convert_from_alphabet(ord(txt[0]))\n",
    "        x = np.array([[x]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "\n",
    "        # initial values\n",
    "        y = x\n",
    "        h = np.zeros([1, INTERNALSIZE * NLAYERS], dtype=np.float32)  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
    "        arr=[]\n",
    "        for i in range(len(txt)-1):\n",
    "            yo, h = sess.run(['Yo:0', 'H:0'], feed_dict={'X:0': y, 'pkeep:0': 1., 'Hin:0': h, 'batchsize:0': 1})\n",
    "            c = my_txtutils.convert_from_alphabet(ord(txt[i+1]))\n",
    "            y = np.array([[c]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "            y1=np.reshape(yo,[ALPHASIZE,-1])\n",
    "            y1=y1.flatten()\n",
    "            #c = chr(my_txtutils.convert_to_alphabet(i))\n",
    "            print(txt[i],':',c,':',y1[c],',max=',np.max(y1),'=',chr(my_txtutils.convert_to_alphabet(np.argmax(y1))))\n",
    "            arr.append(y1[c])\n",
    "    return np.prod(arr)**(-1/len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/rnn_train_1563601394-60000000\n",
      "h : 71 : 0.017361918 ,max= 0.2139146 = \u0000\n",
      "e : 78 : 0.008416108 ,max= 0.24278364 = a\n",
      "l : 78 : 0.00042135394 ,max= 0.9912458 = \u0000\n",
      "l : 81 : 0.07286589 ,max= 0.59736776 = e\n",
      "o : 2 : 2.351578e-06 ,max= 0.99999464 = \u0000\n",
      "  : 89 : 0.006226908 ,max= 0.5184772 =  \n",
      "w : 81 : 1.9324569e-08 ,max= 0.9999999 = \u0000\n",
      "o : 84 : 0.0021418824 ,max= 0.604625 = a\n",
      "r : 78 : 8.288168e-09 ,max= 0.99999976 = \u0000\n",
      "l : 70 : 0.00027098384 ,max= 0.48159385 = i\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2978.628928604615"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcx('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/rnn_train_1563601394-60000000\n",
      "\u0000 : 0.02731734\n",
      "\t : 0.009119334\n",
      "  : 0.029877855\n",
      "! : 0.007832738\n",
      "\" : 0.00781383\n",
      "# : 0.007916751\n",
      "$ : 0.007920723\n",
      "% : 0.007974687\n",
      "& : 0.008838342\n",
      "' : 0.008898315\n",
      "( : 0.009162439\n",
      ") : 0.009861022\n",
      "* : 0.008206715\n",
      "+ : 0.008296969\n",
      ", : 0.011283324\n",
      "- : 0.010530039\n",
      ". : 0.010989632\n",
      "/ : 0.008403456\n",
      "0 : 0.007671907\n",
      "1 : 0.008130989\n",
      "2 : 0.008053313\n",
      "3 : 0.008033515\n",
      "4 : 0.007931977\n",
      "5 : 0.008146205\n",
      "6 : 0.008111852\n",
      "7 : 0.007924892\n",
      "8 : 0.00794537\n",
      "9 : 0.008214056\n",
      ": : 0.008255819\n",
      "; : 0.008002921\n",
      "< : 0.0078708455\n",
      "= : 0.007675543\n",
      "> : 0.008108395\n",
      "? : 0.0079168985\n",
      "@ : 0.008103323\n",
      "A : 0.007865107\n",
      "B : 0.008132115\n",
      "C : 0.008002383\n",
      "D : 0.0077815484\n",
      "E : 0.007962899\n",
      "F : 0.007916055\n",
      "G : 0.008172119\n",
      "H : 0.008301339\n",
      "I : 0.008191501\n",
      "J : 0.008042702\n",
      "K : 0.008129442\n",
      "L : 0.007832516\n",
      "M : 0.008023312\n",
      "N : 0.008021216\n",
      "O : 0.008252282\n",
      "P : 0.0078577\n",
      "Q : 0.00826596\n",
      "R : 0.007967731\n",
      "S : 0.007995544\n",
      "T : 0.008093961\n",
      "U : 0.008012479\n",
      "V : 0.0079518845\n",
      "W : 0.008264683\n",
      "X : 0.007903835\n",
      "Y : 0.007751875\n",
      "Z : 0.008116478\n",
      "[ : 0.008984768\n",
      "\\ : 0.008746207\n",
      "] : 0.0090804\n",
      "^ : 0.0080182925\n",
      "_ : 0.008483667\n",
      "` : 0.008282312\n",
      "a : 0.019716697\n",
      "b : 0.016895326\n",
      "c : 0.010803865\n",
      "d : 0.013215684\n",
      "e : 0.017806694\n",
      "f : 0.012086138\n",
      "g : 0.01350665\n",
      "h : 0.011121903\n",
      "i : 0.020978568\n",
      "j : 0.0120839095\n",
      "k : 0.015714558\n",
      "l : 0.013184545\n",
      "m : 0.016164854\n",
      "n : 0.016808707\n",
      "o : 0.01884054\n",
      "p : 0.014388515\n",
      "q : 0.008455491\n",
      "r : 0.014404566\n",
      "s : 0.015138127\n",
      "t : 0.0132025\n",
      "u : 0.014733665\n",
      "v : 0.009416855\n",
      "w : 0.013000052\n",
      "x : 0.008009185\n",
      "y : 0.013426269\n",
      "z : 0.010286275\n",
      "{ : 0.007910336\n",
      "| : 0.008087664\n",
      "} : 0.0077788737\n",
      "~ : 0.008005693\n",
      "\n",
      " : 0.008081588\n"
     ]
    }
   ],
   "source": [
    "# these must match what was saved !\n",
    "ALPHASIZE = my_txtutils.ALPHASIZE\n",
    "NLAYERS = 3\n",
    "INTERNALSIZE = 512\n",
    "\n",
    "ok02 =\"checkpoints/rnn_train_1560839489-30000000\" # okrika 2019-06-18\n",
    "ok03 =\"checkpoints/rnn_train_1563601394-60000000\" # okrika 2019-07-20\n",
    "\n",
    "author=ok03\n",
    "\n",
    "ncnt = 0\n",
    "with tf.Session() as sess:\n",
    "    # new_saver = tf.train.import_meta_graph('checkpoints/rnn_train_1512567262-0.meta')\n",
    "    new_saver = tf.train.import_meta_graph(author+'.meta')\n",
    "    new_saver.restore(sess, author)\n",
    "    x = my_txtutils.convert_from_alphabet(ord(\"L\"))\n",
    "    x = np.array([[x]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "\n",
    "    # initial values\n",
    "    y = x\n",
    "    h = np.zeros([1, INTERNALSIZE * NLAYERS], dtype=np.float32)  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
    "    yo, h = sess.run(['Yo:0', 'H:0'], feed_dict={'X:0': y, 'pkeep:0': 1., 'Hin:0': h, 'batchsize:0': 1})\n",
    "    y=np.reshape(yo,[98,-1])\n",
    "    y=y.flatten()\n",
    "    v=\"hello world\"\n",
    "    for i in range(ALPHASIZE):\n",
    "        #print(tf.shape(yo))\n",
    "        #print(len(yo))\n",
    "        #c = my_txtutils.sample_from_probabilities(yo, topn=2)\n",
    "        #y = np.array([[c]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "        c = chr(my_txtutils.convert_to_alphabet(i))\n",
    "        print(c,':',y[i])\n",
    "        if c == '\\n':\n",
    "            ncnt = 0\n",
    "        else:\n",
    "            ncnt += 1\n",
    "        if ncnt == 100:\n",
    "            print(\"\")\n",
    "            ncnt = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Softmax layer implementation:\n",
    "# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n",
    "# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n",
    "'''\n",
    "Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\n",
    "Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\n",
    "loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\n",
    "Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\n",
    "Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "'''\n",
    "        # If sampling is be done from the topn most likely characters, the generated text\n",
    "        # is more credible and more \"english\". If topn is not set, it defaults to the full\n",
    "        # distribution (ALPHASIZE)\n",
    "\n",
    "        # Recommended: topn = 10 for intermediate checkpoints, topn=2 or 3 for fully trained checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_txtutils.convert_from_alphabet(ord('i'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
