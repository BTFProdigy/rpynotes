{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import my_txtutils\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acts_new.txt', 'gal_eph_new.txt', 'heb_new.txt', 'jam_jud_new.txt', 'john_new.txt', 'jud_rev_new.txt', 'luke_8_john_new.txt', 'mark01_new.txt', 'matt02_new.txt', 'matt_new.txt', 'phil_col_new.txt', 'thes_tim_new.txt', 'tit_phl_new.txt']\n"
     ]
    }
   ],
   "source": [
    "mypath='./new_txts/'\n",
    "txts='./txts/'\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(txts) if isfile(join(txts, f))]\n",
    "print(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#large files\n",
    "def large_f():\n",
    "    #filenames = ['file1.txt', 'file2.txt', ...]\n",
    "    with open('path/to/output/file', 'w') as outfile:\n",
    "        for fname in onlyfiles:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#small files\n",
    "def small_f():\n",
    "    #filenames = ['file1.txt', 'file2.txt', ...]\n",
    "    with open(mypath+'full.txt', 'w') as outfile:\n",
    "        for fname in onlyfiles:\n",
    "            print('writing file',fname)\n",
    "            with open(txts+fname) as infile:\n",
    "                outfile.write(infile.read())\n",
    "    return open(mypath+\"full.txt\").read()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "        # If sampling is be done from the topn most likely characters, the generated text\n",
    "        # is more credible and more \"english\". If topn is not set, it defaults to the full\n",
    "        # distribution (ALPHASIZE)\n",
    "\n",
    "        # Recommended: topn = 10 for intermediate checkpoints, topn=2 or 3 for fully trained checkpoints\n",
    "\n",
    "        c = my_txtutils.sample_from_probabilities(yo, topn=2)\n",
    "        y = np.array([[c]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "        c = chr(my_txtutils.convert_to_alphabet(c))\n",
    "        print(c, end=\"\")\n",
    "\n",
    "        if c == '\\n':\n",
    "            ncnt = 0\n",
    "        else:\n",
    "            ncnt += 1\n",
    "        if ncnt == 100:\n",
    "            print(\"\")\n",
    "            ncnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file acts_new.txt\n",
      "writing file gal_eph_new.txt\n",
      "writing file heb_new.txt\n",
      "writing file jam_jud_new.txt\n",
      "writing file john_new.txt\n",
      "writing file jud_rev_new.txt\n",
      "writing file luke_8_john_new.txt\n",
      "writing file mark01_new.txt\n",
      "writing file matt02_new.txt\n",
      "writing file matt_new.txt\n",
      "writing file phil_col_new.txt\n",
      "writing file thes_tim_new.txt\n",
      "writing file tit_phl_new.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "649190"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=small_f()\n",
    "words=re.sub(' +', ' ',txt.replace('\\n',' ')).split(' ')\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6507438"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/rnn_train_1563601394-60000000\n",
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# these must match what was saved !\n",
    "ALPHASIZE = my_txtutils.ALPHASIZE\n",
    "NLAYERS = 3\n",
    "INTERNALSIZE = 512\n",
    "\n",
    "ok02 =\"checkpoints/rnn_train_1560839489-30000000\" # okrika 2019-06-18\n",
    "ok03 =\"checkpoints/rnn_train_1563601394-60000000\" # okrika 2019-07-20\n",
    "\n",
    "author=ok03\n",
    "\n",
    "ncnt = 0\n",
    "with tf.Session() as sess:\n",
    "    # new_saver = tf.train.import_meta_graph('checkpoints/rnn_train_1512567262-0.meta')\n",
    "    new_saver = tf.train.import_meta_graph(author+'.meta')\n",
    "    new_saver.restore(sess, author)\n",
    "    x = my_txtutils.convert_from_alphabet(ord(\"L\"))\n",
    "    x = np.array([[x]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n",
    "\n",
    "    # initial values\n",
    "    y = x\n",
    "    h = np.zeros([1, INTERNALSIZE * NLAYERS], dtype=np.float32)  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
    "    #for i in range(10):\n",
    "    yo, h = sess.run(['Yo:0', 'H:0'], feed_dict={'X:0': y, 'pkeep:0': 1., 'Hin:0': h, 'batchsize:0': 1})\n",
    "    print(tf.shape(yo))\n",
    "    print(yo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-80a5e0a7d4db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtxt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "txt[0].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÿþ \\x00o\\x00k\\x00u\\x00n\\x00w\\x00e\\x00n\\x00g\\x00i\\x00a\\x00p\\x00u\\x00 \\x00m\\x00i\\x00e\\x00y\\x00e\\x00 \\x00m\\x00a\\x00 \\x00 \\x00d\\x00i\\x00e\\x00p\\x00a\\x00k\\x00u\\x00m\\x00a\\x00y\\x00e\\x00 \\x00 \\x00o\\x00k\\x00u\\x00n\\x00w\\x00e\\x00n\\x00g\\x00i\\x00a\\x00p\\x00u\\x00'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
