{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.contrib import layers\n",
    "#from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import my_txtutils as txt\n",
    "import torch\n",
    "import json\n",
    "import datetime\n",
    "#tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model parameters\n",
    "#\n",
    "# Usage:\n",
    "#   Training only:\n",
    "#         Leave all the parameters as they are\n",
    "#         Disable validation to run a bit faster (set validation=False below)\n",
    "#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n",
    "#   Training and experimentation (default):\n",
    "#         Keep validation enabled\n",
    "#         You can now play with the parameters anf follow the effects in Tensorboard\n",
    "#         A good choice of parameters ensures that the testing and validation curves stay close\n",
    "#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n",
    "#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n",
    "#\n",
    "nb_epoch=70\n",
    "SEQLEN = 30\n",
    "BATCHSIZE = 200\n",
    "VALI_SEQLEN = SEQLEN\n",
    "ALPHASIZE = txt.ALPHASIZE\n",
    "INTERNALSIZE = 512\n",
    "NLAYERS = 3\n",
    "learning_rate = 0.001  # fixed learning rate\n",
    "dropout_pkeep = 0.8    # some dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# size of the alphabet that we work with\n",
    "ALPHASIZE = 98\n",
    "\n",
    "\n",
    "# Specification of the supported alphabet (subset of ASCII-7)\n",
    "# 10 line feed LF\n",
    "# 32-64 numbers and punctuation\n",
    "# 65-90 upper-case letters\n",
    "# 91-97 more punctuation\n",
    "# 97-122 lower-case letters\n",
    "# 123-126 more punctuation\n",
    "def convert_from_alphabet(a):\n",
    "    \"\"\"Encode a character\n",
    "    :param a: one character\n",
    "    :return: the encoded value\n",
    "    \"\"\"\n",
    "    if a == 9:\n",
    "        return 1\n",
    "    if a == 10:\n",
    "        return 127 - 30  # LF\n",
    "    elif 32 <= a <= 126:\n",
    "        return a - 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "# encoded values:\n",
    "# unknown = 0\n",
    "# tab = 1\n",
    "# space = 2\n",
    "# all chars from 32 to 126 = c-30\n",
    "# LF mapped to 127-30\n",
    "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode a code point\n",
    "    :param c: code point\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return: decoded character\n",
    "    \"\"\"\n",
    "    if c == 1:\n",
    "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
    "    if c == 127 - 30:\n",
    "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
    "    if 32 <= c + 30 <= 126:\n",
    "        return c + 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "def encode_text(s):\n",
    "    \"\"\"Encode a string.\n",
    "    :param s: a text string\n",
    "    :return: encoded list of code points\n",
    "    \"\"\"\n",
    "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
    "\n",
    "\n",
    "def decode_to_text(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode an encoded string.\n",
    "    :param c: encoded list of code points\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
    "\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(probabilities)\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
    "\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "\n",
    "\n",
    "def find_book(index, bookranges):\n",
    "    return next(\n",
    "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def find_book_index(index, bookranges):\n",
    "    return next(\n",
    "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
    "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
    "    print()\n",
    "    # epoch_size in number of batches\n",
    "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
    "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
    "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "    for k in range(batch_size):\n",
    "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
    "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
    "        bookname = find_book(index_in_epoch, bookranges)\n",
    "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
    "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
    "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
    "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
    "        print(print_string.format(decx, decy, loss_string))\n",
    "        index += sequence_len\n",
    "    # box formatting characters:\n",
    "    # │ \\u2502\n",
    "    # ─ \\u2500\n",
    "    # └ \\u2514\n",
    "    # ┘ \\u2518\n",
    "    # ┴ \\u2534\n",
    "    # ┌ \\u250C\n",
    "    # ┐ \\u2510\n",
    "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
    "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
    "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
    "    print(footer)\n",
    "    # print statistics\n",
    "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
    "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
    "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
    "    print()\n",
    "    print(\"TRAINING STATS: {}\".format(stats))\n",
    "\n",
    "\n",
    "class Progress:\n",
    "    \"\"\"Text mode progress bar.\n",
    "    Usage:\n",
    "            p = Progress(30)\n",
    "            p.step()\n",
    "            p.step()\n",
    "            p.step(start=True) # to restart form 0%\n",
    "    The progress bar displays a new header at each restart.\"\"\"\n",
    "    def __init__(self, maxi, size=100, msg=\"\"):\n",
    "        \"\"\"\n",
    "        :param maxi: the number of steps required to reach 100%\n",
    "        :param size: the number of characters taken on the screen by the progress bar\n",
    "        :param msg: the message displayed in the header of the progress bat\n",
    "        \"\"\"\n",
    "        self.maxi = maxi\n",
    "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
    "        self.header_printed = False\n",
    "        self.msg = msg\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, reset=False):\n",
    "        if reset:\n",
    "            self.__init__(self.maxi, self.size, self.msg)\n",
    "        if not self.header_printed:\n",
    "            self.__print_header()\n",
    "        next(self.p)\n",
    "\n",
    "    def __print_header(self):\n",
    "        print()\n",
    "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
    "        print(format_string.format(self.msg))\n",
    "        self.header_printed = True\n",
    "\n",
    "    def __start_progress(self, maxi):\n",
    "        def print_progress():\n",
    "            # Bresenham's algorithm. Yields the number of dots printed.\n",
    "            # This will always print 100 dots in max invocations.\n",
    "            dx = maxi\n",
    "            dy = self.size\n",
    "            d = dy - dx\n",
    "            for x in range(maxi):\n",
    "                k = 0\n",
    "                while d >= 0:\n",
    "                    print('=', end=\"\", flush=True)\n",
    "                    k += 1\n",
    "                    d -= dx\n",
    "                d += dy\n",
    "                yield k\n",
    "\n",
    "        return print_progress\n",
    "\n",
    "\n",
    "def read_data_files(directory, validation=True):\n",
    "    \"\"\"Read data files according to the specified glob pattern\n",
    "    Optionnaly set aside the last file as validation data.\n",
    "    No validation data is returned if there are 5 files or less.\n",
    "    :param directory: for example \"data/*.txt\"\n",
    "    :param validation: if True (default), sets the last file aside as validation data\n",
    "    :return: training data, validation data, list of loaded file names with ranges\n",
    "     If validation is\n",
    "    \"\"\"\n",
    "    codetext = []\n",
    "    bookranges = []\n",
    "    shakelist = glob.glob(directory, recursive=True)\n",
    "    for shakefile in shakelist:\n",
    "        shaketext = open(shakefile, \"r\")\n",
    "        print(\"Loading file \" + shakefile)\n",
    "        start = len(codetext)\n",
    "        codetext.extend(encode_text(shaketext.read()))\n",
    "        end = len(codetext)\n",
    "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
    "        shaketext.close()\n",
    "\n",
    "    if len(bookranges) == 0:\n",
    "        sys.exit(\"No training data has been found. Aborting.\")\n",
    "\n",
    "    # For validation, use roughly 90K of text,\n",
    "    # but no more than 10% of the entire text\n",
    "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "    # 10% of the text is how many files ?\n",
    "    total_len = len(codetext)\n",
    "    validation_len = 0\n",
    "    nb_books1 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books1 += 1\n",
    "        if validation_len > total_len // 10:\n",
    "            break\n",
    "\n",
    "    # 90K of text is how many books ?\n",
    "    validation_len = 0\n",
    "    nb_books2 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books2 += 1\n",
    "        if validation_len > 90*1024:\n",
    "            break\n",
    "\n",
    "    # 20% of the books is how many books ?\n",
    "    nb_books3 = len(bookranges) // 5\n",
    "\n",
    "    # pick the smallest\n",
    "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
    "\n",
    "    if nb_books == 0 or not validation:\n",
    "        cutoff = len(codetext)\n",
    "    else:\n",
    "        cutoff = bookranges[-nb_books][\"start\"]\n",
    "    valitext = codetext[cutoff:]\n",
    "    codetext = codetext[:cutoff]\n",
    "    return codetext, valitext, bookranges\n",
    "\n",
    "\n",
    "def print_data_stats(datalen, valilen, epoch_size):\n",
    "    datalen_mb = datalen/1024.0/1024.0\n",
    "    valilen_kb = valilen/1024.0\n",
    "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
    "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
    "\n",
    "\n",
    "def print_validation_header(validation_start, bookranges):\n",
    "    bookindex = find_book_index(validation_start, bookranges)\n",
    "    books = ''\n",
    "    for i in range(bookindex, len(bookranges)):\n",
    "        books += bookranges[i][\"name\"]\n",
    "        if i < len(bookranges)-1:\n",
    "            books += \", \"\n",
    "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
    "\n",
    "\n",
    "def print_validation_stats(loss, accuracy):\n",
    "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
    "                                                                                                           accuracy))\n",
    "\n",
    "\n",
    "def print_text_generation_header():\n",
    "    print()\n",
    "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
    "\n",
    "\n",
    "def print_text_generation_footer():\n",
    "    print()\n",
    "    print(\"└{:─^111}┘\".format('End of generation'))\n",
    "\n",
    "\n",
    "def frequency_limiter(n, multiple=1, modulo=0):\n",
    "    def limit(i):\n",
    "        return i % (multiple * n) == modulo*multiple\n",
    "    return limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file txts\\acts_new.txt\n",
      "Loading file txts\\gal_eph_new.txt\n",
      "Loading file txts\\heb_new.txt\n",
      "Loading file txts\\jam_jud_new.txt\n",
      "Loading file txts\\john_new.txt\n",
      "Loading file txts\\jud_rev_new.txt\n",
      "Loading file txts\\luke_8_john_new.txt\n",
      "Loading file txts\\mark01_new.txt\n",
      "Loading file txts\\matt02_new.txt\n",
      "Loading file txts\\matt_new.txt\n",
      "Loading file txts\\phil_col_new.txt\n",
      "Loading file txts\\thes_tim_new.txt\n",
      "Loading file txts\\tit_phl_new.txt\n"
     ]
    }
   ],
   "source": [
    "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
    "shakedir = \"txts/*.txt\"\n",
    "#shakedir = \"../tensorflow/**/*.py\"\n",
    "codetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text size is 2.92MB with 192.15KB set aside for validation. There will be 509 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "# display some stats on the data\n",
    "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
    "txt.print_data_stats(len(codetext), len(valitext), epoch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programs\\python38\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = [nn.GRUCell(input_size, hidden_size) for i in range(NLAYERS)]\n",
    "        self.i2o = [nn.Linear(input_size + hidden_size, output_size) for i in range(NLAYERS)]\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        self.i2h[0].to(device)\n",
    "        self.i2o[0].to(device)\n",
    "          hidden = self.i2h[0](input,hidden)\n",
    "        output = self.i2o[0](combined)\n",
    "        for i in range(1,NLAYERS):\n",
    "            combined = torch.cat((output, hidden), 1)\n",
    "            self.i2h[i].to(device)\n",
    "            self.i2o[i].to(device)\n",
    "            hidden = self.i2h[i](output,hidden)\n",
    "            output = self.i2o[i](combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(BATCHSIZE, self.hidden_size, device=device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using gpu..' if torch.cuda.is_available() else 'using cpu..')\n",
    "rnn = RNN(ALPHASIZE, INTERNALSIZE, ALPHASIZE)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# loss fn\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#from ok_seq2seq import EncoderRNN \\\n",
    "#                        ,DecoderRNN \\\n",
    "#                        ,AttnDecoderRNN \\\n",
    "#                        ,evaluateRandomly \\\n",
    "#                        ,teacher_forcing_ratio \n",
    "\n",
    "#torch.nn.GRUCell(input_size: int, hidden_size: int, bias: bool = True)\n",
    "# Parameters\n",
    "# input_size – The number of expected features in the input x\n",
    "\n",
    "# hidden_size – The number of features in the hidden state h\n",
    "\n",
    "# bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "\n",
    "# Inputs: input, hidden\n",
    "# input of shape (batch, input_size): tensor containing input features\n",
    "\n",
    "# hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\n",
    "\n",
    "# Outputs: h’\n",
    "# h’ of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training fn\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    lint = []\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "        lint.append(output)\n",
    "    input = torch.stack(lint).transpose(0,1).transpose(1,2)\n",
    "    loss = criterion(input, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return torch.stack(lint).transpose(0,1), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def one_hot(chcode):\n",
    "    tensor = torch.zeros(1, ALPHASIZE)\n",
    "    tensor[0][chcode] = 1\n",
    "    return tensor\n",
    "\n",
    "def mb2t(rows):\n",
    "    rows=rows.transpose()\n",
    "    tensor = torch.zeros(rows.shape[0], rows.shape[1], ALPHASIZE)\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, letter_code in enumerate(row):\n",
    "            tensor[i][j][letter_code] = 1\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def lin2txt(lt):\n",
    "    return ''.join([chr(txt.convert_to_alphabet(c))  if c != 0 else '' for c in lt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 of 70 (0:00:03.101995) 4.5830  okunwengiapu mieye ma  diepak / B*mmmmmmmm|fmBmmmmmBmmBBmmmf|m ✗ calculating stats..\n",
      "epoch 0 of 70 (0:00:03.101995) 4.5830 ara mi nwose omine mgba oriari / B*mmmmmmmm|fmBmmmmmBmmBBmmmf|m ✗ calculating stats..\n",
      "epoch 0 of 70 (0:10:50.176113) 4.5825 isiapu ma bereniapu ma se kpot / |mm|fmBmmBmm|mmm|fmBmmBmmBmm*| ✗ calculating stats..\n",
      "epoch 0 of 70 (0:10:50.176113) 4.5825 ow naa kirikiri o biki koro se / |mm|fmBmmBmm|mmm|fmBmmBmmBmm*| ✗ calculating stats..\n",
      "epoch 0 of 70 (0:22:22.784447) 4.5830 mun dumo bie chu-chu bo be now / mmmBmmm*BmmmBmmmmmmmBm*BmmBm|m ✗ calculating stats..\n",
      "epoch 0 of 70 (0:22:22.784447) 4.5830 loko dieapu ma na nwo chukurum / mmmBmmm*BmmmBmmmmmmmBm*BmmBm|m ✗ calculating stats..\n",
      "epoch 1 of 70 (0:36:10.302125) 4.5826 werima i nyanaye mi ma i nwo d / mm||m|B|Bmm|m|mmBmmBmmB|Bmm*Bm ✗ average batch rate per hr = 1.66,  eta = 1 day, 17:35:50\n",
      "epoch 1 of 70 (0:36:10.302125) 4.5826 mi ani iya amanyanabo ngada-e  / mm||m|B|Bmm|m|mmBmmBmmB|Bmm*Bm ✗ average batch rate per hr = 1.66,  eta = 1 day, 17:35:50\n",
      "epoch 1 of 70 (0:49:39.960792) 4.5832 u ma na stivin be na berejinea / |BmmBm|Bm||B|mBmmBm|Bmm|mmmmm| ✗ average batch rate per hr = 1.21,  eta = 2 days, 9:06:57\n",
      "epoch 1 of 70 (0:49:39.960792) 4.5832 sotoru bo enekubu bia o kelema / |BmmBm|Bm||B|mBmmBm|Bmm|mmmmm| ✗ average batch rate per hr = 1.21,  eta = 2 days, 9:06:57\n",
      "epoch 2 of 70 (1:02:45.170650) 4.5827 lame. o tamuno be torukubu ibi / ||mm|B*B||mmm|BmmB|||mmmmmBmmm ✗ average batch rate per hr = 1.91,  eta = 1 day, 11:33:35\n",
      "epoch 2 of 70 (1:02:45.170650) 4.5827 ian mi fie ye be koroma nwo ko / ||mm|B*B||mmm|BmmB|||mmmmmBmmm ✗ average batch rate per hr = 1.91,  eta = 1 day, 11:33:35\n",
      "epoch 2 of 70 (1:17:34.479202) 4.5832 am askos bie sime juapu tekewa / |mB|mm|mBmmmBmmmmBmm|fmB|mmmm| ✗ average batch rate per hr = 1.55,  eta = 1 day, 19:57:32\n",
      "epoch 2 of 70 (1:17:34.479202) 4.5832 me. antiok bie mi ini tatari k / |mB|mm|mBmmmBmmmmBmm|fmB|mmmm| ✗ average batch rate per hr = 1.55,  eta = 1 day, 19:57:32\n",
      "epoch 3 of 70 (1:31:46.409956) 4.5826 n siki ma a pulasa ye, anikani / |BmmmmBmmB|Bfm||m|BmmmB|mmm|mm ✗ average batch rate per hr = 1.96,  eta = 1 day, 10:09:36\n",
      "epoch 3 of 70 (1:31:46.410955) 4.5826 nkoro samuel be buo lame. se i / |BmmmmBmmB|Bfm||m|BmmmB|mmm|mm ✗ average batch rate per hr = 1.96,  eta = 1 day, 10:09:36\n",
      "epoch 3 of 70 (1:44:48.977238) 4.5829 eme be na bereni na bie bein b / mmmBmmBm|Bmm|mmmBm|BmmmBmmmmBm ✗ average batch rate per hr = 1.72,  eta = 1 day, 15:00:53\n",
      "epoch 3 of 70 (1:44:48.977238) 4.5829 bereniapu ma bu chua se inia k / mmmBmmBm|Bmm|mmmBm|BmmmBmmmmBm ✗ average batch rate per hr = 1.72,  eta = 1 day, 15:00:53\n",
      "epoch 4 of 70 (1:57:04.571594) 4.5831 se kura mesi na gbasi ye bu or / mmBmm||BmmmmBm|Bmm|mmBmmBmmB*| ✗ average batch rate per hr = 2.05,  eta = 1 day, 8:11:45\n",
      "epoch 4 of 70 (1:57:04.571594) 4.5831 ia kienbipi enbene-bene senime / mmBmm||BmmmmBm|Bmm|mmBmmBmmB*| ✗ average batch rate per hr = 2.05,  eta = 1 day, 8:11:45\n",
      "epoch 4 of 70 (2:09:10.656208) 4.5836 a mi bu ori na banabas be na s / |BmmBmmB*||Bm|Bm|m|m|mBmmBm|Bm ✗ average batch rate per hr = 1.86,  eta = 1 day, 11:31:25\n",
      "epoch 4 of 70 (2:09:10.656208) 4.5836 ume, pol be ini firimame na in / |BmmBmmB*||Bm|Bm|m|m|mBmmBm|Bm ✗ average batch rate per hr = 1.86,  eta = 1 day, 11:31:25\n",
      "epoch 5 of 70 (2:21:38.493459) 4.5827 ie tolu boro muari siki ini ok / |mB|||mBm*||Bmm|||BmmmmB|mmB*m ✗ average batch rate per hr = 2.12,  eta = 1 day, 6:41:20\n",
      "epoch 5 of 70 (2:21:38.493459) 4.5827 da obudukoapu o nwo orime, se  / |mB|||mBm*||Bmm|||BmmmmB|mmB*m ✗ average batch rate per hr = 2.12,  eta = 1 day, 6:41:20\n",
      "epoch 5 of 70 (2:32:46.752717) 4.5834 i tomoni ma chukuruma se amafi / |B||m*m|BmmBmmmmm|mmmBmmB|m|mm ✗ average batch rate per hr = 1.96,  eta = 1 day, 9:06:07\n",
      "epoch 5 of 70 (2:32:46.753700) 4.5834 a  wa toroko mun owoin-aru mi  / |B||m*m|BmmBmmmmm|mmmBmmB|m|mm ✗ average batch rate per hr = 1.96,  eta = 1 day, 9:06:07\n",
      "epoch 6 of 70 (2:43:45.714523) 4.5829 tibi ori be kuroma okwein bu,  / ||mmB*||BmmBmm||m|B*mmmmmBmmmB ✗ average batch rate per hr = 2.20,  eta = 1 day, 5:06:47\n",
      "epoch 6 of 70 (2:43:45.714523) 4.5829 e kuro bara se pol be olome. i / ||mmB*||BmmBmm||m|B*mmmmmBmmmB ✗ average batch rate per hr = 2.20,  eta = 1 day, 5:06:47\n",
      "epoch 6 of 70 (2:55:09.987447) 4.5830 ine obu balabalama ma, o pa du / |mmB*mmBm|||m|||m|BmmmB*Bf|Bmm ✗ average batch rate per hr = 2.06,  eta = 1 day, 7:08:26\n",
      "epoch 6 of 70 (2:55:09.987447) 4.5830 tibi mi piki gele teinme, se o / |mmB*mmBm|||m|||m|BmmmB*Bf|Bmm ✗ average batch rate per hr = 2.06,  eta = 1 day, 7:08:26\n",
      "epoch 7 of 70 (3:06:06.025713) 4.5833  bumiefiafiama mi okibia ene m / Bmmmmmmm|mm|m|BmmB*mmmm|BmmmBm ✗ average batch rate per hr = 2.26,  eta = 1 day, 3:54:54\n",
      "epoch 7 of 70 (3:06:06.025713) 4.5833  pol be okweinma mun se bo ibi / Bmmmmmmm|mm|m|BmmB*mmmm|BmmmBm ✗ average batch rate per hr = 2.26,  eta = 1 day, 3:54:54\n",
      "epoch 7 of 70 (3:17:24.329403) 4.5829 sise se dumobia omin mi now ok / mmmmBmmBmmm*mm|B*mmmBmmBm|mB*m ✗ average batch rate per hr = 2.13,  eta = 1 day, 5:36:38\n",
      "epoch 7 of 70 (3:17:24.330405) 4.5829 re mi dukome na ini inia sima  / mmmmBmmBmmm*mm|B*mmmBmmBm|mB*m ✗ average batch rate per hr = 2.13,  eta = 1 day, 5:36:38\n",
      "epoch 8 of 70 (3:28:27.688744) 4.5834 i oria ikiapu ma piri ini o bu / |B*|||B|mm|fmBmmBf|||B|mmB*Bmm ✗ average batch rate per hr = 2.30,  eta = 1 day, 2:55:34\n",
      "epoch 8 of 70 (3:28:27.689746) 4.5834 a ton werime na ini ikoliapu m / |B*|||B|mm|fmBmmBf|||B|mmB*Bmm ✗ average batch rate per hr = 2.30,  eta = 1 day, 2:55:34\n",
      "epoch 8 of 70 (3:38:41.862202) 4.5831 moni ma bie inia kiri nwo okib / m*m|BmmBmmmBmmm|Bmm||Bmm*B*mmm ✗ average batch rate per hr = 2.19,  eta = 1 day, 4:14:54\n",
      "epoch 8 of 70 (3:38:41.862202) 4.5831 zos be nwose nin beme, sikima  / m*m|BmmBmmmBmmm|Bmm||Bmm*B*mmm ✗ average batch rate per hr = 2.19,  eta = 1 day, 4:14:54\n",
      "epoch 9 of 70 (3:48:44.571543) 4.5828  join ma olo diki ma nwo fimas / Bm*|mBmmB*|*BmmmmBmmBmm*Bmmm|m ✗ average batch rate per hr = 2.36,  eta = 1 day, 1:50:22\n",
      "epoch 9 of 70 (3:48:44.571543) 4.5828 bara fiafia teme be now okisam / Bm*|mBmmB*|*BmmmmBmmBmm*Bmmm|m ✗ average batch rate per hr = 2.36,  eta = 1 day, 1:50:22\n",
      "epoch 9 of 70 (3:58:33.302767) 4.5827  bu. jon be mengi se nweni sar / Bmm|Bm*mBmmBmmmmmBmmBmmmmmBm|| ✗ average batch rate per hr = 2.26,  eta = 1 day, 2:56:52\n",
      "epoch 9 of 70 (3:58:33.302767) 4.5827 a toroko gwosa bara bu chie si / Bmm|Bm*mBmmBmmmmmBmmBmmmmmBm|| ✗ average batch rate per hr = 2.26,  eta = 1 day, 2:56:52\n",
      "epoch 10 of 70 (4:08:06.704241) 4.5837 ere nwo dukome, o min mi nwo d / m|mBmm*Bmmm|mmmB*BmmmBmmBmm*Bm ✗ average batch rate per hr = 2.42,  eta = 1 day, 0:48:40\n",
      "epoch 10 of 70 (4:08:06.704241) 4.5837  bie chu apu ma ini oputekewar / m|mBmm*Bmmm|mmmB*BmmmBmmBmm*Bm ✗ average batch rate per hr = 2.42,  eta = 1 day, 0:48:40\n",
      "epoch 10 of 70 (4:18:18.404088) 4.5828 kereme, omine tomoni ma be tib / mm|mmmmB*mmmmB||m*m|BmmBmmB||m ✗ average batch rate per hr = 2.32,  eta = 1 day, 1:49:50\n",
      "epoch 10 of 70 (4:18:18.405088) 4.5828 a sime se fi nyo mi ini inia f / mm|mmmmB*mmmmB||m*m|BmmBmmB||m ✗ average batch rate per hr = 2.32,  eta = 1 day, 1:49:50\n",
      "epoch 11 of 70 (4:29:34.998324) 4.5825 i angasiki koruabe okuma wa bk / |B|mm|mmmmBm||m|mmB*mmmmBm|Bmm ✗ average batch rate per hr = 2.45,  eta = 1 day, 0:05:56\n",
      "epoch 11 of 70 (4:29:34.999325) 4.5825 e mi nab u, ini mengi se ini s / |B|mm|mmmmBm||m|mmB*mmmmBm|Bmm ✗ average batch rate per hr = 2.45,  eta = 1 day, 0:05:56\n",
      "epoch 11 of 70 (4:39:58.805196) 4.5828 a o lekirime, se fero be piki  / |B*B|mmm||mmmBmmBmm||BmmBf|mmB ✗ average batch rate per hr = 2.36,  eta = 1 day, 1:01:42\n",
      "epoch 11 of 70 (4:39:58.805196) 4.5828  ini mgba o dikibalafame. anis / |B*B|mmm||mmmBmmBmm||BmmBf|mmB ✗ average batch rate per hr = 2.36,  eta = 1 day, 1:01:42\n",
      "epoch 12 of 70 (4:50:00.498597) 4.5827  uri ani tamuno kuro mie, op / Bmmm||B|mmB||mmm|Bmm||BmmmmB*f ✗ average batch rate per hr = 2.48,  eta = 23:21:42\n",
      "epoch 12 of 70 (4:50:00.499555) 4.5827 ro nyana apu ma o nwo gwome. t / Bmmm||B|mmB||mmm|Bmm||BmmmmB*f ✗ average batch rate per hr = 2.48,  eta = 23:21:42\n",
      "epoch 12 of 70 (4:59:46.929591) 4.5823 okuma gbori ene din bie sol be / |mmmmBmm*||BmmmBmmmBmmmBm||Bmm ✗ average batch rate per hr = 2.40,  eta = 1 day, 0:08:56\n",
      "epoch 12 of 70 (4:59:46.929591) 4.5823  pirime nyanabo be ikoli bie o / |mmmmBmm*||BmmmBmmmBmmmBm||Bmm ✗ average batch rate per hr = 2.40,  eta = 1 day, 0:08:56\n",
      "epoch 13 of 70 (5:09:27.989213) 4.5831  mamgba nyanabo be. izrel ama  / Bmmmmm|Bmm|m|m*Bmm|B|m|m|B|m|B ✗ average batch rate per hr = 2.52,  eta = 22:36:53\n",
      "epoch 13 of 70 (5:09:27.990207) 4.5831 piri se inia kubie kuromame na / Bmmmmm|Bmm|m|m*Bmm|B|m|m|B|m|B ✗ average batch rate per hr = 2.52,  eta = 22:36:53\n",
      "epoch 13 of 70 (5:19:07.221459) 4.5829 e a beme. okuma a kuroma dukom / mB|Bmmmm|B*mmmmB|Bmm||m|Bmmm|m ✗ average batch rate per hr = 2.44,  eta = 23:19:13\n",
      "epoch 13 of 70 (5:19:07.221459) 4.5829  vinpiki bobia, nyanabo be oko / mB|Bmmmm|B*mmmmB|Bmm||m|Bmmm|m ✗ average batch rate per hr = 2.44,  eta = 23:19:13\n",
      "epoch 14 of 70 (5:29:20.516557) 4.5831 boari inyosara ene mi bie ini  / m*|||B|mm|m|||BmmmBmmBmmmBmmmB ✗ average batch rate per hr = 2.55,  eta = 21:57:22\n",
      "epoch 14 of 70 (5:29:20.517557) 4.5831 ari book ma finji oku nwo ori  / m*|||B|mm|m|||BmmmBmmBmmmBmmmB ✗ average batch rate per hr = 2.55,  eta = 21:57:22\n",
      "epoch 14 of 70 (5:38:57.740007) 4.5831 apu ma nwose yechin oku mi, na / |fmBmmBmm*mmBmmmm|mB*mmBmmmBm| ✗ average batch rate per hr = 2.48,  eta = 22:35:50\n",
      "epoch 14 of 70 (5:38:57.740007) 4.5831 ios ama bie ini yi bo, ma bolo / |fmBmmBmm*mmBmmmm|mB*mmBmmmBm| ✗ average batch rate per hr = 2.48,  eta = 22:35:50\n",
      "epoch 15 of 70 (5:48:54.057057) 4.5833 mabe, isia ikoliapu ma ini pok / mmmmmBmmm|B|m||||fmBmmB|mmBf*m ✗ average batch rate per hr = 2.58,  eta = 21:19:18\n",
      "epoch 15 of 70 (5:48:54.058058) 4.5833 fisos bie na piki kalaye kenge / mmmmmBmmm|B|m||||fmBmmB|mmBf*m ✗ average batch rate per hr = 2.58,  eta = 21:19:18\n",
      "epoch 15 of 70 (5:58:40.291198) 4.5825 pu obu some, se berrenime, ini / fmB*mmBm|mmmBmmBmm||mmmmmmBmmm ✗ average batch rate per hr = 2.51,  eta = 21:55:07\n",
      "epoch 15 of 70 (5:58:40.291198) 4.5825 mine ibioku sime wa nwose ini  / fmB*mmBm|mmmBmmBmm||mmmmmmBmmm ✗ average batch rate per hr = 2.51,  eta = 21:55:07\n",
      "epoch 16 of 70 (6:08:19.225251) 4.5835 u ma, se nwose ini beme, wuap / |BmmmBmmBmm*mmBmmmBmmmmmBmmm|f ✗ average batch rate per hr = 2.61,  eta = 20:43:04\n",
      "epoch 16 of 70 (6:08:19.225251) 4.5835 ekubu mi gose, ani miese i bu  / |BmmmBmmBmm*mmBmmmBmmmmmBmmm|f ✗ average batch rate per hr = 2.61,  eta = 20:43:04\n",
      "epoch 16 of 70 (6:18:09.647403) 4.5830 pol be ori okwein so siki, ori / f*|BmmB*||B*mmmmmBm|BmmmmmB*|| ✗ average batch rate per hr = 2.54,  eta = 21:16:17\n",
      "epoch 16 of 70 (6:18:09.647403) 4.5830 vina be piri, se pol be sabama / f*|BmmB*||B*mmmmmBm|BmmmmmB*|| ✗ average batch rate per hr = 2.54,  eta = 21:16:17\n",
      "epoch 17 of 70 (6:27:59.660749) 4.5828  na sime owuapu ma enekubu mi  / Bm|BmmmmB*mm|fmBmmBmmmmmmmBmmB ✗ average batch rate per hr = 2.63,  eta = 20:09:37\n",
      "epoch 17 of 70 (6:27:59.660749) 4.5828  nwo naabia,festos be nwo peke / Bm|BmmmmB*mm|fmBmmBmmmmmmmBmmB ✗ average batch rate per hr = 2.63,  eta = 20:09:37\n",
      "epoch 17 of 70 (6:37:35.869094) 4.5831  bara mi nwo mieme. ani din, m / Bm|||BmmBmm*Bmmmmm|B|mmBmmmmBm ✗ average batch rate per hr = 2.57,  eta = 20:39:34\n",
      "epoch 17 of 70 (6:37:35.870091) 4.5831 uma bolo ka siki bu, ogono for / Bm|||BmmBmm*Bmmmmm|B|mmBmmmmBm ✗ average batch rate per hr = 2.57,  eta = 20:39:34\n",
      "epoch 18 of 70 (6:47:32.759736) 4.5829 sabamame o bime ini o se o kor / m|m|m|mmB*BmmmmBmmmB*BmmB*Bm|| ✗ average batch rate per hr = 2.65,  eta = 19:37:21\n",
      "epoch 18 of 70 (6:47:32.759736) 4.5829 omaye mi. o piki bukuroma mies / m|m|m|mmB*BmmmmBmmmB*BmmB*Bm|| ✗ average batch rate per hr = 2.65,  eta = 19:37:21\n",
      "epoch 18 of 70 (6:57:22.784590) 4.5831 piki ineda oboku siki mi se an / f|mmB|mmm|B*m*mmBmmmmBmmBmmB|m ✗ average batch rate per hr = 2.59,  eta = 20:05:45\n",
      "epoch 18 of 70 (6:57:22.784590) 4.5831 pu nab u ini nwo beme, nwo mi  / f|mmB|mmm|B*m*mmBmmmmBmmBmmB|m ✗ average batch rate per hr = 2.59,  eta = 20:05:45\n",
      "epoch 19 of 70 (7:07:31.961514) 4.5831 ania bu weri sime bara, aniati / |mm|BmmBmm||BmmmmBm|||mB|mm||| ✗ average batch rate per hr = 2.67,  eta = 19:07:35\n",
      "epoch 19 of 70 (7:07:31.961514) 4.5831 ki mi bara mgba se wa dikiari  / |mm|BmmBmm||BmmmmBm|||mB|mm||| ✗ average batch rate per hr = 2.67,  eta = 19:07:35\n",
      "epoch 19 of 70 (7:17:15.914791) 4.5834  mi bie anga juapu omie, ani b / BmmBmmmB|mm|Bmm|fmB*mmmmB|mmBm ✗ average batch rate per hr = 2.61,  eta = 19:33:42\n",
      "epoch 19 of 70 (7:17:15.914791) 4.5834 bu, josef, livai fur obo be in / BmmBmmmB|mm|Bmm|fmB*mmmmB|mmBm ✗ average batch rate per hr = 2.61,  eta = 19:33:42\n",
      "epoch 20 of 70 (7:26:58.558017) 4.5829 putekewari mi bie nweni bo be  / fm|mmmm|||BmmBmmmBmmmmmBm*BmmB ✗ average batch rate per hr = 2.68,  eta = 18:37:26\n",
      "epoch 20 of 70 (7:26:58.558017) 4.5829  bu sosa boe kobirima mi ini o / fm|mmmm|||BmmBmmmBmmmmmBm*BmmB ✗ average batch rate per hr = 2.68,  eta = 18:37:26\n",
      "epoch 20 of 70 (7:36:43.004763) 4.5833 ma nwose inimgba ogono sarame  / mmBmm*mmBmmmmmm|B*m*m|Bm|||mmB ✗ average batch rate per hr = 2.63,  eta = 19:01:47\n",
      "epoch 20 of 70 (7:36:43.004763) 4.5833 ni na ini kara rifan oru mi, o / mmBmm*mmBmmmmmm|B*m*m|Bm|||mmB ✗ average batch rate per hr = 2.63,  eta = 19:01:47\n",
      "epoch 21 of 70 (7:46:46.503920) 4.5833 uma ye mi now bereni okime bie / |mmBmmBmmBm|mBmm|mmmB*mmmmBmmm ✗ average batch rate per hr = 2.70,  eta = 18:09:08\n",
      "epoch 21 of 70 (7:46:46.503920) 4.5833 engi mi biepakabo siki, tamuno / |mmBmmBmmBm|mBmm|mmmB*mmmmBmmm ✗ average batch rate per hr = 2.70,  eta = 18:09:08\n",
      "epoch 21 of 70 (7:56:47.797838) 4.5834 piri yee  ani tamuno be obu pi / f|||BmmmBB|mmB||mmm|BmmB*mmBf| ✗ average batch rate per hr = 2.64,  eta = 18:32:31\n",
      "epoch 21 of 70 (7:56:47.798839) 4.5834  eneogboin biri bu tekebia ere / f|||BmmmBB|mmB||mmm|BmmB*mmBf| ✗ average batch rate per hr = 2.64,  eta = 18:32:31\n",
      "epoch 22 of 70 (8:06:32.613795) 4.5832 gbolu mgba se bereni me bebe   / mm*|mBmmm|BmmBmm|mmmBmmBmmmmBB ✗ average batch rate per hr = 2.71,  eta = 17:41:32\n",
      "epoch 22 of 70 (8:06:32.613795) 4.5832 aka tomoni berenime se vinpiki / mm*|mBmmm|BmmBmm|mmmBmmBmmmmBB ✗ average batch rate per hr = 2.71,  eta = 17:41:32\n",
      "epoch 22 of 70 (8:16:20.127499) 4.5831 e mama apu nwo chinme, piki gb / mBmmm|B|fmBmm*Bmm|mmmmBf|mmBmm ✗ average batch rate per hr = 2.66,  eta = 18:02:54\n",
      "epoch 22 of 70 (8:16:20.127499) 4.5831 , bara se ini dede sa saki okw / mBmmm|B|fmBmm*Bmm|mmmmBf|mmBmm ✗ average batch rate per hr = 2.66,  eta = 18:02:54\n",
      "epoch 23 of 70 (8:26:05.027176) 4.5832 uo ma gose ani ini stivin be b / ||BmmBm*mmB|mmB|mmBm||B|mBmmBm ✗ average batch rate per hr = 2.73,  eta = 17:14:10\n",
      "epoch 23 of 70 (8:26:05.027176) 4.5832 pisidia se mi bie sime antiok  / ||BmmBm*mmB|mmB|mmBm||B|mBmmBm ✗ average batch rate per hr = 2.73,  eta = 17:14:10\n",
      "epoch 23 of 70 (8:35:51.808167) 4.5829 ewari mi bie smun se kponji si / mm|||BmmBmmmBmmmmBmmBmm*mmmBmm ✗ average batch rate per hr = 2.68,  eta = 17:34:09\n",
      "epoch 23 of 70 (8:35:51.809167) 4.5829 ikoniom na bie sime bereniapu  / mm|||BmmBmmmBmmmmBmmBmm*mmmBmm ✗ average batch rate per hr = 2.68,  eta = 17:34:09\n",
      "epoch 24 of 70 (8:46:27.894026) 4.5834 oro o sobie ene seni o pirime, / |||B*Bm|mmmBmmmBmmmmB*Bf|||mmm ✗ average batch rate per hr = 2.74,  eta = 16:49:03\n",
      "epoch 24 of 70 (8:46:27.894026) 4.5834 e bipiberelapu ma na piki owua / |||B*Bm|mmmBmmmBmmmmB*Bf|||mmm ✗ average batch rate per hr = 2.74,  eta = 16:49:03\n",
      "epoch 24 of 70 (8:56:19.314195) 4.5829 e, se o chochi ma miekuromame  / mmBmmB*Bmm|mm|BmmBmmmmm||m|mmB ✗ average batch rate per hr = 2.68,  eta = 17:07:57\n",
      "epoch 24 of 70 (8:56:19.314195) 4.5829 e, na ini ibioku se ori okie o / mmBmmB*Bmm|mm|BmmBmmmmm||m|mmB ✗ average batch rate per hr = 2.68,  eta = 17:07:57\n",
      "epoch 25 of 70 (9:06:06.023351) 4.5829  mume  aninyo sime apu ma kubi / BmmmmBB|mmmm|BmmmmB|fmBmmBmmmm ✗ average batch rate per hr = 2.75,  eta = 16:22:58\n",
      "epoch 25 of 70 (9:06:06.023351) 4.5829  biki oria ogono koro se ori a / BmmmmBB|mmmm|BmmmmB|fmBmmBmmmm ✗ average batch rate per hr = 2.75,  eta = 16:22:58\n",
      "epoch 25 of 70 (9:15:49.568475) 4.5827 ila be na o duko ye naa siki,  / |||BmmBm|B*Bmmm|BmmBm||BmmmmmB ✗ average batch rate per hr = 2.70,  eta = 16:40:29\n",
      "epoch 25 of 70 (9:15:49.568475) 4.5827 rime, bumiefiafiama mi okibia  / |||BmmBm|B*Bmmm|BmmBm||BmmmmmB ✗ average batch rate per hr = 2.70,  eta = 16:40:29\n",
      "epoch 26 of 70 (9:25:38.051067) 4.5835 obirima sime ogono wari mi bie / |mm||m|BmmmmB*m*m|Bm|||BmmBmmm ✗ average batch rate per hr = 2.76,  eta = 15:57:13\n",
      "epoch 26 of 70 (9:25:38.052068) 4.5835  sadusiapu ma na saki kamsome  / |mm||m|BmmmmB*m*m|Bm|||BmmBmmm ✗ average batch rate per hr = 2.76,  eta = 15:57:13\n",
      "epoch 26 of 70 (9:35:20.494069) 4.5829 a fiye goyegoye now fi ma, pik / |BmmmmBm*mmm|mmBm|mBmmBmmmBf|m ✗ average batch rate per hr = 2.71,  eta = 16:13:39\n",
      "epoch 26 of 70 (9:35:20.494069) 4.5829 boro siki feliks be na taro dr / |BmmmmBm*mmm|mmBm|mBmmBmmmBf|m ✗ average batch rate per hr = 2.71,  eta = 16:13:39\n",
      "epoch 27 of 70 (9:45:19.461213) 4.5829  pol be ori na kobirima mi bie / Bm*|BmmB*||Bm|Bm|mm||m|BmmBmmm ✗ average batch rate per hr = 2.77,  eta = 15:32:10\n",
      "epoch 27 of 70 (9:45:19.462211) 4.5829 mi bie na, piki jerusalem mi b / Bm*|BmmB*||Bm|Bm|mm||m|BmmBmmm ✗ average batch rate per hr = 2.77,  eta = 15:32:10\n",
      "epoch 27 of 70 (9:55:12.023290) 4.5833 o, lisias be bo la siki u o to / |mB||mm|mBmmBm*B||BmmmmBmB*B|| ✗ average batch rate per hr = 2.72,  eta = 15:47:54\n",
      "epoch 27 of 70 (9:55:12.023290) 4.5833 e kuro mi dukuno mi fomu kanme / |mB||mm|mBmmBm*B||BmmmmBmB*B|| ✗ average batch rate per hr = 2.72,  eta = 15:47:54\n",
      "epoch 27 of 70 (10:04:59.631084) 4.5828 amuno be piribia, miese ini ib / |mmm|BmmBf|||mm|mBmmmmmBmmmB|m ✗ average batch rate per hr = 2.68,  eta = 16:03:30\n",
      "epoch 27 of 70 (10:04:59.631084) 4.5828 ni i gose boma se tamuno be pi / |mmm|BmmBf|||mm|mBmmmmmBmmmB|m ✗ average batch rate per hr = 2.68,  eta = 16:03:30\n",
      "epoch 28 of 70 (10:14:43.443141) 4.5832 yee, ani gbori siki mi bu, ini / |mmmB|mmBmm*||BmmmmBmmBmmmBmmm ✗ average batch rate per hr = 2.73,  eta = 15:22:05\n",
      "epoch 28 of 70 (10:14:43.443141) 4.5832 ime ye    min mi ani die-au na / |mmmB|mmBmm*||BmmmmBmmBmmmBmmm ✗ average batch rate per hr = 2.73,  eta = 15:22:05\n",
      "epoch 28 of 70 (10:24:28.375418) 4.5826 siria na salisia na nyo ma nwo / mm|||Bm|Bm|||mm|Bm|Bmm|BmmBmm* ✗ average batch rate per hr = 2.69,  eta = 15:36:42\n",
      "epoch 28 of 70 (10:24:28.375418) 4.5826 a oju ma biari ye bereton o pi / mm|||Bm|Bm|||mm|Bm|Bmm|BmmBmm* ✗ average batch rate per hr = 2.69,  eta = 15:36:42\n",
      "epoch 29 of 70 (10:34:11.912833) 4.5828 ia bo be bobia siki labia, ani / ||Bm*BmmBm*mm|BmmmmB||mm|mB|mm ✗ average batch rate per hr = 2.74,  eta = 14:56:37\n",
      "epoch 29 of 70 (10:34:11.913832) 4.5828 a sekiri na bie sime ye goyego / ||Bm*BmmBm*mm|BmmmmB||mm|mB|mm ✗ average batch rate per hr = 2.74,  eta = 14:56:37\n",
      "epoch 29 of 70 (10:44:18.843624) 4.5831 e kraist mgbesechi bere duko m / mBm|||m|Bmmmmmmmm|Bmm|mBmmm|Bm ✗ average batch rate per hr = 2.70,  eta = 15:10:55\n",
      "epoch 29 of 70 (10:44:18.844622) 4.5831 iatibi ari omine gose torusior / mBm|||m|Bmmmmmmmm|Bmm|mBmmm|Bm ✗ average batch rate per hr = 2.70,  eta = 15:10:55\n",
      "epoch 30 of 70 (10:54:26.639764) 4.5829 u diki. ani o se wa piriye mi  / |Bmmmm|B|mmB*BmmBm|Bf|||mmBmmB ✗ average batch rate per hr = 2.75,  eta = 14:32:35\n",
      "epoch 30 of 70 (10:54:26.639764) 4.5829 w some, okuma o bukuroma nyana / |Bmmmm|B|mmB*BmmBm|Bf|||mmBmmB ✗ average batch rate per hr = 2.75,  eta = 14:32:35\n",
      "epoch 30 of 70 (11:04:17.261038) 4.5828 aka yeton mi weri sime bara bu / |m|Bmm||mBmmBmm||BmmmmBm|||Bmm ✗ average batch rate per hr = 2.71,  eta = 14:45:43\n",
      "epoch 30 of 70 (11:04:17.261038) 4.5828 i. se ori aniduko peleme, na o / |m|Bmm||mBmmBmm||BmmmmBm|||Bmm ✗ average batch rate per hr = 2.71,  eta = 14:45:43\n",
      "epoch 31 of 70 (11:14:03.446333) 4.5827 , piki sise fii bu, se kraist  / mBf|mmBmmmmBmm|BmmmBmmBm|||m|B ✗ average batch rate per hr = 2.76,  eta = 14:08:00\n",
      "epoch 31 of 70 (11:14:03.446333) 4.5827 aist bee. a fieariye, ani na,  / mBf|mmBmmmmBmm|BmmmBmmBm|||m|B ✗ average batch rate per hr = 2.76,  eta = 14:08:00\n",
      "epoch 31 of 70 (11:29:19.747585) 4.5832 bia erechi. iri na oguapu ma b / mm|Bm|mmm||B|||Bm|B*mm|fmBmmBm ✗ average batch rate per hr = 2.70,  eta = 14:27:13\n",
      "epoch 31 of 70 (11:29:19.748586) 4.5832 umaye mi ani minaminaka baka f / mm|Bm|mmm||B|||Bm|B*mm|fmBmmBm ✗ average batch rate per hr = 2.70,  eta = 14:27:13\n",
      "epoch 32 of 70 (12:09:00.591451) 4.5832 bu bipiogboso nyana se ani gos / mmBmmf||mm*m|Bmm|m|BmmB|mmBm*m ✗ average batch rate per hr = 2.63,  eta = 14:25:41\n",
      "epoch 32 of 70 (12:09:00.592456) 4.5832  bu boro mine oki o bu piribia / mmBmmf||mm*m|Bmm|m|BmmB|mmBm*m ✗ average batch rate per hr = 2.63,  eta = 14:25:41\n",
      "epoch 32 of 70 (12:18:40.311206) 4.5830 upele na bupeleka na mie kirim / |fm|mBm|Bmmfm|mm|Bm|BmmmBmm||m ✗ average batch rate per hr = 2.60,  eta = 14:37:10\n",
      "epoch 32 of 70 (12:18:40.311206) 4.5830 injibia. chuku saki bu ngisi,  / |fm|mBm|Bmmfm|mm|Bm|BmmmBmm||m ✗ average batch rate per hr = 2.60,  eta = 14:37:10\n",
      "epoch 33 of 70 (12:28:20.645380) 4.5826 e, so bie bu. tomonikiri mi ma / mmBm|BmmmBmm|B||m*m|mm||BmmBmm ✗ average batch rate per hr = 2.65,  eta = 13:59:03\n",
      "epoch 33 of 70 (12:28:20.645380) 4.5826 oki pakuma enekubu mi bie bobi / mmBm|BmmmBmm|B||m*m|mm||BmmBmm ✗ average batch rate per hr = 2.65,  eta = 13:59:03\n",
      "epoch 33 of 70 (12:38:11.714097) 4.5833  bu. iri iderima tein tamunoa  / Bmm|B|||B|mm||m|B|m|mB||mmm||B ✗ average batch rate per hr = 2.61,  eta = 14:10:05\n",
      "epoch 33 of 70 (12:38:11.714097) 4.5833  siki, a dikibugerere nyanaka  / Bmm|B|||B|mm||m|B|m|mB||mmm||B ✗ average batch rate per hr = 2.61,  eta = 14:10:05\n",
      "epoch 34 of 70 (12:47:57.140175) 4.5829 a nyanabo be bubelebiaye now d / |Bmm|m|m*BmmBmmmm|mmm|mmBm|mBm ✗ average batch rate per hr = 2.66,  eta = 13:33:07\n",
      "epoch 34 of 70 (12:47:57.140175) 4.5829  bara  okoko mi na yeberenibip / |Bmm|m|m*BmmBmmmm|mmm|mmBm|mBm ✗ average batch rate per hr = 2.66,  eta = 13:33:07\n",
      "epoch 34 of 70 (12:57:49.800532) 4.5833 n mi tomonibo bara bub o i la  / |BmmB||m*m|m*Bm|||BmmmB*B|B||B ✗ average batch rate per hr = 2.62,  eta = 13:43:35\n",
      "epoch 34 of 70 (12:57:49.800532) 4.5833 kuro mi bu, minea bereni bu bo / |BmmB||m*m|m*Bm|||BmmmB*B|B||B ✗ average batch rate per hr = 2.62,  eta = 13:43:35\n",
      "epoch 35 of 70 (13:07:32.094696) 4.5829 e chin ogono sololo bo goyegoy / mBmm|mB*m*m|Bm||*|*Bm*Bm*mmm|m ✗ average batch rate per hr = 2.67,  eta = 13:07:32\n",
      "epoch 35 of 70 (13:07:32.095697) 4.5829 be aniatibi omine belemasam mi / mBmm|mB*m*m|Bm||*|*Bm*Bm*mmm|m ✗ average batch rate per hr = 2.67,  eta = 13:07:32\n",
      "epoch 35 of 70 (13:17:46.710123) 4.5829 roma oloko mi duko ye mie bu b / ||m|B*|*m|BmmBmmm|BmmBmmmBmmBm ✗ average batch rate per hr = 2.63,  eta = 13:17:46\n",
      "epoch 35 of 70 (13:17:46.710123) 4.5829 boro bereni anye ma nwo diaoki / ||m|B*|*m|BmmBmmm|BmmBmmmBmmBm ✗ average batch rate per hr = 2.63,  eta = 13:17:46\n",
      "epoch 36 of 70 (13:27:43.867883) 4.5838  apu ma seimabe na, dikibugere / B|fmBmmBmmmm|mmBm|mBmmmmmmmm|m ✗ average batch rate per hr = 2.67,  eta = 12:42:51\n",
      "epoch 36 of 70 (13:27:43.867883) 4.5838 a  aniatibi enekubu mi ani ibi / B|fmBmmBmmmm|mmBm|mBmmmmmmmm|m ✗ average batch rate per hr = 2.67,  eta = 12:42:51\n",
      "epoch 36 of 70 (13:37:29.002712) 4.5830 a mioku tamuo be o teme be bu  / |Bmm|mmB||mm|BmmB*B|mmmBmmBmmB ✗ average batch rate per hr = 2.64,  eta = 12:52:04\n",
      "epoch 36 of 70 (13:37:29.002712) 4.5830 mi go apu ma pirime na ini ber / |Bmm|mmB||mm|BmmB*B|mmmBmmBmmB ✗ average batch rate per hr = 2.64,  eta = 12:52:04\n",
      "epoch 37 of 70 (13:47:11.413647) 4.5829 ipokika apu ma ogono gbanabia  / |f*mmm|B|fmBmmB*m*m|Bmm|m|mm|B ✗ average batch rate per hr = 2.68,  eta = 12:17:45\n",
      "epoch 37 of 70 (13:47:11.413647) 4.5829 ni, \t  ini i dadiki piki i ele / |f*mmm|B|fmBmmB*m*m|Bmm|m|mm|B ✗ average batch rate per hr = 2.68,  eta = 12:17:45\n",
      "epoch 37 of 70 (13:57:03.568129) 4.5831 okoma duaboroma mi piripiri bo / |m|m|Bmm|m*||m|BmmBf|||f|||Bm* ✗ average batch rate per hr = 2.65,  eta = 12:26:33\n",
      "epoch 37 of 70 (13:57:03.569129) 4.5831 u nemime na, omine ibitein bel / |m|m|Bmm|m*||m|BmmBf|||f|||Bm* ✗ average batch rate per hr = 2.65,  eta = 12:26:33\n",
      "epoch 38 of 70 (14:06:46.347297) 4.5833 bara bu, mimgba o nemikase tam / m|||BmmmBmmmmm|B*Bmmmmm|mmB||m ✗ average batch rate per hr = 2.69,  eta = 11:53:04\n",
      "epoch 38 of 70 (14:06:46.347297) 4.5833 oroma oboku mi ibioku mi ani i / m|||BmmmBmmmmm|B*Bmmmmm|mmB||m ✗ average batch rate per hr = 2.69,  eta = 11:53:04\n",
      "epoch 38 of 70 (14:16:34.941585) 4.5830 kuma, tamuno be ani belemame.  / mmmmmB||mmm|BmmB|mmBmm|mmmmm|B ✗ average batch rate per hr = 2.66,  eta = 12:01:19\n",
      "epoch 38 of 70 (14:16:34.942585) 4.5830 ton-a nama se edeni ogono mie  / mmmmmB||mmm|BmmB|mmBmm|mmmmm|B ✗ average batch rate per hr = 2.66,  eta = 12:01:19\n",
      "epoch 39 of 70 (14:26:18.378265) 4.5829  mi-e. ori oboku ikowari mi na / Bmmmm|B*||B*m*mmBmm|m|||BmmBm| ✗ average batch rate per hr = 2.70,  eta = 11:28:36\n",
      "epoch 39 of 70 (14:26:18.378265) 4.5829 sam.    bereni mi miese ebraha / Bmmmm|B*||B*m*mmBmm|m|||BmmBm| ✗ average batch rate per hr = 2.70,  eta = 11:28:36\n",
      "epoch 39 of 70 (14:36:04.107475) 4.5831 tomoni ma be sima nwose inia b / ||m*m|BmmBmmBmmm|Bmm*mmBmmm|Bm ✗ average batch rate per hr = 2.67,  eta = 11:36:21\n",
      "epoch 39 of 70 (14:36:04.107475) 4.5831 bi, se okwein ari bo be poki d / ||m*m|BmmBmmBmmm|Bmm*mmBmmm|Bm ✗ average batch rate per hr = 2.67,  eta = 11:36:21\n",
      "epoch 40 of 70 (14:45:45.894818) 4.5834 se mi be koroma nwo konke, ini / mmBmmBmmBm|||m|Bmm*Bm|mmmmBmmm ✗ average batch rate per hr = 2.71,  eta = 11:04:19\n",
      "epoch 40 of 70 (14:45:45.894818) 4.5834 ria firinwengiapu ma nwose fin / mmBmmBmmBm|||m|Bmm*Bm|mmmmBmmm ✗ average batch rate per hr = 2.71,  eta = 11:04:19\n",
      "epoch 40 of 70 (14:56:01.212451) 4.5828 pu ma. nweni mamgba be berekon / fmBmm|BmmmmmBmmmmm|BmmBmm|mm|m ✗ average batch rate per hr = 2.68,  eta = 11:12:00\n",
      "epoch 40 of 70 (14:56:01.212451) 4.5828 ma ene mi gbolomaye ini nwo du / fmBmm|BmmmmmBmmmmm|BmmBmm|mm|m ✗ average batch rate per hr = 2.68,  eta = 11:12:00\n",
      "epoch 41 of 70 (15:06:51.405420) 4.5828  piribia  ori piki toku so i p / Bm|||mm|BB*||Bf|mmB||mmBm|B|Bf ✗ average batch rate per hr = 2.71,  eta = 10:41:26\n",
      "epoch 41 of 70 (15:06:51.405420) 4.5828 k be pirime. mi melkizedek be  / Bm|||mm|BB*||Bf|mmB||mmBm|B|Bf ✗ average batch rate per hr = 2.71,  eta = 10:41:26\n",
      "epoch 41 of 70 (15:17:10.053771) 4.5836  bara mi, ani, ani paka okue.  / Bm|||BmmmB|mmmB|mmBf|m|B*mmm|B ✗ average batch rate per hr = 2.68,  eta = 10:48:43\n",
      "epoch 41 of 70 (15:17:10.053771) 4.5836 bamieapu ma enebenebene so mun / Bm|||BmmmB|mmmB|mmBf|m|B*mmm|B ✗ average batch rate per hr = 2.68,  eta = 10:48:43\n",
      "epoch 42 of 70 (15:27:28.155785) 4.5837 ana boe, piki gele tein tamuno / |m|Bm*mmBf|mmBmm|mB|m|mB||mmm| ✗ average batch rate per hr = 2.72,  eta = 10:18:18\n",
      "epoch 42 of 70 (15:27:28.155785) 4.5837 oku mi. aninakaraka, o ye wa j / |m|Bm*mmBf|mmBmm|mB|m|mB||mmm| ✗ average batch rate per hr = 2.72,  eta = 10:18:18\n",
      "epoch 42 of 70 (15:37:54.269775) 4.5826 e mi be ogonoju cherubim nwo c / mBmmBmmB*m*m|mmBmmm|mmmmBmm*Bm ✗ average batch rate per hr = 2.69,  eta = 10:25:16\n",
      "epoch 42 of 70 (15:37:54.270785) 4.5826 umapu ma nyana bome, ini fibus / mBmmBmmB*m*m|mmBmmm|mmmmBmm*Bm ✗ average batch rate per hr = 2.69,  eta = 10:25:16\n",
      "epoch 43 of 70 (15:47:58.855415) 4.5831 use dikiariye mi nwo olo, ania / |mmBmmmm|||mmBmmBmm*B*|*mB|mm| ✗ average batch rate per hr = 2.72,  eta = 9:55:14\n",
      "epoch 43 of 70 (15:47:58.855415) 4.5831 nabo bee nwobe ye mi dukopakum / |mmBmmmm|||mmBmmBmm*B*|*mB|mm| ✗ average batch rate per hr = 2.72,  eta = 9:55:14\n",
      "epoch 43 of 70 (15:58:47.652537) 4.5830 e nwo mieme se tamuno be beren / mBmm*BmmmmmBmmB||mmm|BmmBmm|mm ✗ average batch rate per hr = 2.69,  eta = 10:02:02\n",
      "epoch 43 of 70 (15:58:47.652537) 4.5830 wo okibo oria boma mi dieokibi / mBmm*BmmmmmBmmB||mmm|BmmBmm|mm ✗ average batch rate per hr = 2.69,  eta = 10:02:02\n",
      "epoch 44 of 70 (16:09:55.802245) 4.5824 kuma ikowari mi obuju o damamu / mmmmB|m|m|||BmmB*mmmmB*Bm|m|mm ✗ average batch rate per hr = 2.72,  eta = 9:33:08\n",
      "epoch 44 of 70 (16:09:55.802245) 4.5824 e oku mise,   ori agbamiebo os / mmmmB|m|m|||BmmB*mmmmB*Bm|m|mm ✗ average batch rate per hr = 2.72,  eta = 9:33:08\n",
      "epoch 44 of 70 (16:20:23.815812) 4.5829 kala siki bu o fi tomoni mamgb / m|||BmmmmBmmB*BmmB||m*m|Bmmmmm ✗ average batch rate per hr = 2.69,  eta = 9:39:19\n",
      "epoch 44 of 70 (16:20:23.815812) 4.5829 be jinye, tamuno be yeberenime / m|||BmmmmBmmB*BmmB||m*m|Bmmmmm ✗ average batch rate per hr = 2.69,  eta = 9:39:19\n",
      "epoch 45 of 70 (16:30:16.921758) 4.5831 iki, tomoni ma be si ma, abga  / |mmmB||m*m|BmmBmmBmmBmmmB|mm|B ✗ average batch rate per hr = 2.73,  eta = 9:10:09\n",
      "epoch 45 of 70 (16:30:16.921758) 4.5831 i oloko diri mi na piki tomoni / |mmmB||m*m|BmmBmmBmmBmmmB|mm|B ✗ average batch rate per hr = 2.73,  eta = 9:10:09\n",
      "epoch 45 of 70 (16:40:02.186166) 4.5824 a, melkizedek be oru mi bu  an / |mBmm|mmmmmmmBmmB*|mBmmBmmBB|m ✗ average batch rate per hr = 2.70,  eta = 9:15:34\n",
      "epoch 45 of 70 (16:40:02.186166) 4.5824 raka ini ori oki iwo tamuno be / |mBmm|mmmmmmmBmmB*|mBmmBmmBB|m ✗ average batch rate per hr = 2.70,  eta = 9:15:34\n",
      "epoch 46 of 70 (16:49:47.245959) 4.5830  bebe o ton wari kiri okibia   / BmmmmB*B||mBm|||Bmm||B*mmmm|BB ✗ average batch rate per hr = 2.73,  eta = 8:46:50\n",
      "epoch 46 of 70 (16:49:47.246961) 4.5830 me, biebeleke  obuko, ani sime / BmmmmB*B||mBm|||Bmm||B*mmmm|BB ✗ average batch rate per hr = 2.73,  eta = 8:46:50\n",
      "epoch 46 of 70 (16:59:42.348136) 4.5829 ein ken be nengi yee  oria ber / m|mBmmmBmmBmmmmmBmmmBB*|||Bmm| ✗ average batch rate per hr = 2.71,  eta = 8:52:01\n",
      "epoch 46 of 70 (16:59:42.348136) 4.5829 ama bebe tomonikiri mi kelema  / m|mBmmmBmmBmmmmmBmmmBB*|||Bmm| ✗ average batch rate per hr = 2.71,  eta = 8:52:01\n",
      "epoch 47 of 70 (17:10:07.140701) 4.5827 naga dabo be piri se dumo gbei / ||m|Bm|m*BmmBf|||BmmBmmm*Bmmmm ✗ average batch rate per hr = 2.74,  eta = 8:24:06\n",
      "epoch 47 of 70 (17:10:07.140701) 4.5827 le mi soni simeoku-e ani boka  / ||m|Bm|m*BmmBf|||BmmBmmm*Bmmmm ✗ average batch rate per hr = 2.74,  eta = 8:24:06\n",
      "epoch 47 of 70 (17:22:53.114650) 4.5829 berejine okwein .- bereni na f / mm|mmmmmB*mmmmmB|mBmm|mmmBm|Bm ✗ average batch rate per hr = 2.70,  eta = 8:30:20\n",
      "epoch 47 of 70 (17:22:53.114650) 4.5829     diepakumaye    pita be bar / mm|mmmmmB*mmmmmB|mBmm|mmmBm|Bm ✗ average batch rate per hr = 2.70,  eta = 8:30:20\n",
      "epoch 48 of 70 (17:35:37.290654) 4.5831 o miebia oku, ani bu wa piki i / |Bmmmmm|B*mmmB|mmBmmBm|Bf|mmB| ✗ average batch rate per hr = 2.73,  eta = 8:03:49\n",
      "epoch 48 of 70 (17:35:37.291654) 4.5831 i ani boro sedabo be-o, bereto / |Bmmmmm|B*mmmB|mmBmmBm|Bf|mmB| ✗ average batch rate per hr = 2.73,  eta = 8:03:49\n",
      "epoch 48 of 70 (17:49:59.679544) 4.5832 vinpiki bome bebe, o min       / B|mf|mmBm*mmBmmmmmB*BmmmBBBBBB ✗ average batch rate per hr = 2.69,  eta = 8:10:24\n",
      "epoch 48 of 70 (17:49:59.680541) 4.5832 .  okuma o biebele na kraist b / B|mf|mmBm*mmBmmmmmB*BmmmBBBBBB ✗ average batch rate per hr = 2.69,  eta = 8:10:24\n",
      "epoch 49 of 70 (18:01:49.621489) 4.5837 e oru bere nemi apu ogboku omi / mB*|mBmm|mBmmmmB|fmB*mm*mmB*mm ✗ average batch rate per hr = 2.72,  eta = 7:43:38\n",
      "epoch 49 of 70 (18:01:49.621489) 4.5837 niabu gbaname bebe, aniatibi k / mB*|mBmm|mBmmmmB|fmB*mm*mmB*mm ✗ average batch rate per hr = 2.72,  eta = 7:43:38\n",
      "epoch 49 of 70 (18:12:57.918096) 4.5827 oro mun tamuno be labia oku ji / |||BmmmB||mmm|BmmB||mm|B*mmBmm ✗ average batch rate per hr = 2.69,  eta = 7:48:24\n",
      "epoch 49 of 70 (18:12:57.918096) 4.5827 amgba na amen.                 / |||BmmmB||mmm|BmmB||mm|B*mmBmm ✗ average batch rate per hr = 2.69,  eta = 7:48:24\n",
      "epoch 50 of 70 (18:23:52.088969) 4.5833 ubeleme  mine na minabuna min  / |mm|mmmBBmmmmBm|Bmmm|mmm|BmmmB ✗ average batch rate per hr = 2.72,  eta = 7:21:32\n",
      "epoch 50 of 70 (18:23:52.088969) 4.5833  mine ma mioku tamuno awo-e, o / |mm|mmmBBmmmmBm|Bmmm|mmm|BmmmB ✗ average batch rate per hr = 2.72,  eta = 7:21:32\n",
      "epoch 50 of 70 (18:44:15.026509) 4.5831 u nemi, miese bara layelaye mi / |BmmmmmBmmmmmBm|||B||mm||mmBmm ✗ average batch rate per hr = 2.67,  eta = 7:29:42\n",
      "epoch 50 of 70 (18:44:15.027667) 4.5831 oteinme ori piki onyechie seni / |BmmmmmBmmmmmBm|||B||mm||mmBmm ✗ average batch rate per hr = 2.67,  eta = 7:29:42\n",
      "epoch 51 of 70 (19:05:10.515449) 4.5832  na karakaraye mi mie bo goyeg / Bm|Bm|||m|||mmBmmBmmmBm*Bm*mmm ✗ average batch rate per hr = 2.67,  eta = 7:06:38\n",
      "epoch 51 of 70 (19:05:10.515449) 4.5832  tetema -  belema piri -    __ / Bm|Bm|||m|||mmBmmBmmmBm*Bm*mmm ✗ average batch rate per hr = 2.67,  eta = 7:06:38\n",
      "epoch 51 of 70 (19:26:27.406931) 4.5831 na onyechie min mi pirime na k / ||B*mmmmm|mBmmmBmmBf|||mmBm|Bm ✗ average batch rate per hr = 2.62,  eta = 7:14:33\n",
      "epoch 51 of 70 (19:26:27.406931) 4.5831 ma nwose ini be, dumo bu inia  / ||B*mmmmm|mBmmmBmmBf|||mmBm|Bm ✗ average batch rate per hr = 2.62,  eta = 7:14:33\n",
      "epoch 52 of 70 (19:48:18.954453) 4.5830  mi kien-apu ma kubiekuromame  / BmmBmmmmm|fmBmmBmmmmmmm||m|mmB ✗ average batch rate per hr = 2.63,  eta = 6:51:20\n",
      "epoch 52 of 70 (19:48:18.955449) 4.5830 ene ma soni nwo lame. omine so / BmmBmmmmm|fmBmmBmmmmmmm||m|mmB ✗ average batch rate per hr = 2.63,  eta = 6:51:20\n",
      "epoch 52 of 70 (20:08:34.282150) 4.5827 tomonibo nwo be na o bereni ny / ||m*m|m*Bmm*BmmBm|B*Bmm|mmmBmm ✗ average batch rate per hr = 2.58,  eta = 6:58:21\n",
      "epoch 52 of 70 (20:08:34.282150) 4.5827  piki duaboroma nyanabia oku.  / ||m*m|m*Bmm*BmmBm|B*Bmm|mmmBmm ✗ average batch rate per hr = 2.58,  eta = 6:58:21\n",
      "epoch 53 of 70 (1 day, 2:32:42.933655) 4.5834 ki o basam, o piki omine be ku / mmB*Bm|m|mmB*Bf|mmB*mmmmBmmBmm ✗ average batch rate per hr = 2.00,  eta = 8:30:52\n",
      "epoch 53 of 70 (1 day, 2:32:42.934659) 4.5834 uno be bereton tomonikiri bie  / mmB*Bm|m|mmB*Bf|mmB*mmmmBmmBmm ✗ average batch rate per hr = 2.00,  eta = 8:30:52\n",
      "epoch 53 of 70 (1 day, 2:45:11.972765) 4.5830 iki fiafia se mi na ani gose s / |mmBmm|mm|BmmBmmBm|B|mmBm*mmBm ✗ average batch rate per hr = 1.98,  eta = 8:34:52\n",
      "epoch 53 of 70 (1 day, 2:45:11.973764) 4.5830 ist be bu o dawonemisa ye ma b / |mmBmm|mm|BmmBmmBm|B|mmBm*mmBm ✗ average batch rate per hr = 1.98,  eta = 8:34:52\n",
      "epoch 54 of 70 (1 day, 2:57:56.810493) 4.5825  ma na piki kuro mamgba piriar / BmmBm|Bf|mmBmm||Bmmmmm|Bf||||| ✗ average batch rate per hr = 2.00,  eta = 7:59:23\n",
      "epoch 54 of 70 (1 day, 2:57:56.811438) 4.5825 e aye ma ari doki fabia, piki  / BmmBm|Bf|mmBmm||Bmmmmm|Bf||||| ✗ average batch rate per hr = 2.00,  eta = 7:59:23\n",
      "epoch 54 of 70 (1 day, 3:09:59.879654) 4.5824 kirima nyana mi bie tamuno kik / mm||m|Bmm|m|BmmBmmmB||mmm|Bmmm ✗ average batch rate per hr = 1.99,  eta = 8:02:57\n",
      "epoch 54 of 70 (1 day, 3:09:59.880652) 4.5824 nwo be ye goseke aninakaraka,  / mm||m|Bmm|m|BmmBmmmB||mmm|Bmmm ✗ average batch rate per hr = 1.99,  eta = 8:02:57\n",
      "epoch 55 of 70 (1 day, 3:21:08.301609) 4.5829 , o bereni ye mie bu, tonapu o / mB*Bmm|mmmBmmBmmmBmmmB||m|fmB* ✗ average batch rate per hr = 2.01,  eta = 7:27:34\n",
      "epoch 55 of 70 (1 day, 3:21:08.301609) 4.5829 me na o tamuno be belemame, ok / mB*Bmm|mmmBmmBmmmBmmmB||m|fmB* ✗ average batch rate per hr = 2.01,  eta = 7:27:34\n",
      "epoch 55 of 70 (1 day, 3:31:46.463487) 4.5826 , ini mine na simesam. okuma i / mBmmmBmmmmBm|Bmmmmm|m|B*mmmmB| ✗ average batch rate per hr = 2.00,  eta = 7:30:29\n",
      "epoch 55 of 70 (1 day, 3:31:46.464484) 4.5826  ye mi nwo da. ibiye mie bo go / mBmmmBmmmmBm|Bmmmmm|m|B*mmmmB| ✗ average batch rate per hr = 2.00,  eta = 7:30:29\n"
     ]
    }
   ],
   "source": [
    "print_every = 250\n",
    "plot_every = 100\n",
    "\n",
    "vloss = []\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "iter=0\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=nb_epoch):\n",
    "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    category =  [lin2txt(l) for l in y_]\n",
    "    lines = [lin2txt(l) for l in x]\n",
    "    category_tensor=mb2t(y_)\n",
    "    line_tensor=mb2t(x)\n",
    "    output, loss = train(torch.tensor(y_, device=device, dtype=torch.long), line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess = [lin2txt([ch.argmax(dim=0) for ch in line]) for line in output]\n",
    "        for i in range(2):\n",
    "            elapsed_time = time.time() - start\n",
    "            tss = str(datetime.timedelta(seconds=elapsed_time)) # time since start string\n",
    "            if epoch > 0:\n",
    "                speed = epoch/elapsed_time\n",
    "                eta = (nb_epoch-epoch)/speed\n",
    "                sspeed = speed*60*60\n",
    "                seta = str(datetime.timedelta(seconds=int(eta)))\n",
    "                stats = f'average batch rate per hr = %3.2f,  eta = {seta}'%(sspeed)\n",
    "            else:\n",
    "                stats ='calculating stats..'\n",
    "            correct = '✓' if guess[i] == category[i] else '✗ %s' % stats \n",
    "            print('epoch %d of %d (%s) %.4f %s / %s %s' % (epoch, nb_epoch, tss, loss, lines[i], guess[0], correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0 and len(valitext) > 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, BATCHSIZE, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        line_tensor = mb2t(vali_x)\n",
    "        output, loss = train(torch.tensor(vali_y, device=device, dtype=torch.long), line_tensor)\n",
    "        vloss.append(loss)\n",
    "        with open('vloss.json', 'w') as f:\n",
    "            loss_data={'vloss':vloss, 'tloss':all_losses}\n",
    "            json.dump(loss_data, f)\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(vloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is=torch.Size([1, 98]), hs=torch.Size([1, 128])\n",
      "torch.Size([1, 98])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "PATH = './slgru_epoch36.model'\n",
    "torch.save(rnn.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
