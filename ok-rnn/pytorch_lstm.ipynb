{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.contrib import layers\n",
    "#from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import my_txtutils as txt\n",
    "#tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# model parameters\n",
    "#\n",
    "# Usage:\n",
    "#   Training only:\n",
    "#         Leave all the parameters as they are\n",
    "#         Disable validation to run a bit faster (set validation=False below)\n",
    "#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n",
    "#   Training and experimentation (default):\n",
    "#         Keep validation enabled\n",
    "#         You can now play with the parameters anf follow the effects in Tensorboard\n",
    "#         A good choice of parameters ensures that the testing and validation curves stay close\n",
    "#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n",
    "#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n",
    "#\n",
    "nb_epoch=10\n",
    "SEQLEN = 30\n",
    "BATCHSIZE = 200\n",
    "ALPHASIZE = txt.ALPHASIZE\n",
    "INTERNALSIZE = 256\n",
    "NLAYERS = 3\n",
    "learning_rate = 0.001  # fixed learning rate\n",
    "dropout_pkeep = 0.8    # some dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# size of the alphabet that we work with\n",
    "ALPHASIZE = 98\n",
    "\n",
    "\n",
    "# Specification of the supported alphabet (subset of ASCII-7)\n",
    "# 10 line feed LF\n",
    "# 32-64 numbers and punctuation\n",
    "# 65-90 upper-case letters\n",
    "# 91-97 more punctuation\n",
    "# 97-122 lower-case letters\n",
    "# 123-126 more punctuation\n",
    "def convert_from_alphabet(a):\n",
    "    \"\"\"Encode a character\n",
    "    :param a: one character\n",
    "    :return: the encoded value\n",
    "    \"\"\"\n",
    "    if a == 9:\n",
    "        return 1\n",
    "    if a == 10:\n",
    "        return 127 - 30  # LF\n",
    "    elif 32 <= a <= 126:\n",
    "        return a - 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "# encoded values:\n",
    "# unknown = 0\n",
    "# tab = 1\n",
    "# space = 2\n",
    "# all chars from 32 to 126 = c-30\n",
    "# LF mapped to 127-30\n",
    "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode a code point\n",
    "    :param c: code point\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return: decoded character\n",
    "    \"\"\"\n",
    "    if c == 1:\n",
    "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
    "    if c == 127 - 30:\n",
    "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
    "    if 32 <= c + 30 <= 126:\n",
    "        return c + 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "def encode_text(s):\n",
    "    \"\"\"Encode a string.\n",
    "    :param s: a text string\n",
    "    :return: encoded list of code points\n",
    "    \"\"\"\n",
    "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
    "\n",
    "\n",
    "def decode_to_text(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode an encoded string.\n",
    "    :param c: encoded list of code points\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
    "\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(probabilities)\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
    "\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "\n",
    "\n",
    "def find_book(index, bookranges):\n",
    "    return next(\n",
    "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def find_book_index(index, bookranges):\n",
    "    return next(\n",
    "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
    "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
    "    print()\n",
    "    # epoch_size in number of batches\n",
    "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
    "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
    "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "    for k in range(batch_size):\n",
    "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
    "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
    "        bookname = find_book(index_in_epoch, bookranges)\n",
    "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
    "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
    "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
    "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
    "        print(print_string.format(decx, decy, loss_string))\n",
    "        index += sequence_len\n",
    "    # box formatting characters:\n",
    "    # │ \\u2502\n",
    "    # ─ \\u2500\n",
    "    # └ \\u2514\n",
    "    # ┘ \\u2518\n",
    "    # ┴ \\u2534\n",
    "    # ┌ \\u250C\n",
    "    # ┐ \\u2510\n",
    "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
    "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
    "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
    "    print(footer)\n",
    "    # print statistics\n",
    "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
    "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
    "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
    "    print()\n",
    "    print(\"TRAINING STATS: {}\".format(stats))\n",
    "\n",
    "\n",
    "class Progress:\n",
    "    \"\"\"Text mode progress bar.\n",
    "    Usage:\n",
    "            p = Progress(30)\n",
    "            p.step()\n",
    "            p.step()\n",
    "            p.step(start=True) # to restart form 0%\n",
    "    The progress bar displays a new header at each restart.\"\"\"\n",
    "    def __init__(self, maxi, size=100, msg=\"\"):\n",
    "        \"\"\"\n",
    "        :param maxi: the number of steps required to reach 100%\n",
    "        :param size: the number of characters taken on the screen by the progress bar\n",
    "        :param msg: the message displayed in the header of the progress bat\n",
    "        \"\"\"\n",
    "        self.maxi = maxi\n",
    "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
    "        self.header_printed = False\n",
    "        self.msg = msg\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, reset=False):\n",
    "        if reset:\n",
    "            self.__init__(self.maxi, self.size, self.msg)\n",
    "        if not self.header_printed:\n",
    "            self.__print_header()\n",
    "        next(self.p)\n",
    "\n",
    "    def __print_header(self):\n",
    "        print()\n",
    "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
    "        print(format_string.format(self.msg))\n",
    "        self.header_printed = True\n",
    "\n",
    "    def __start_progress(self, maxi):\n",
    "        def print_progress():\n",
    "            # Bresenham's algorithm. Yields the number of dots printed.\n",
    "            # This will always print 100 dots in max invocations.\n",
    "            dx = maxi\n",
    "            dy = self.size\n",
    "            d = dy - dx\n",
    "            for x in range(maxi):\n",
    "                k = 0\n",
    "                while d >= 0:\n",
    "                    print('=', end=\"\", flush=True)\n",
    "                    k += 1\n",
    "                    d -= dx\n",
    "                d += dy\n",
    "                yield k\n",
    "\n",
    "        return print_progress\n",
    "\n",
    "\n",
    "def read_data_files(directory, validation=True):\n",
    "    \"\"\"Read data files according to the specified glob pattern\n",
    "    Optionnaly set aside the last file as validation data.\n",
    "    No validation data is returned if there are 5 files or less.\n",
    "    :param directory: for example \"data/*.txt\"\n",
    "    :param validation: if True (default), sets the last file aside as validation data\n",
    "    :return: training data, validation data, list of loaded file names with ranges\n",
    "     If validation is\n",
    "    \"\"\"\n",
    "    codetext = []\n",
    "    bookranges = []\n",
    "    shakelist = glob.glob(directory, recursive=True)\n",
    "    for shakefile in shakelist:\n",
    "        shaketext = open(shakefile, \"r\")\n",
    "        print(\"Loading file \" + shakefile)\n",
    "        start = len(codetext)\n",
    "        codetext.extend(encode_text(shaketext.read()))\n",
    "        end = len(codetext)\n",
    "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
    "        shaketext.close()\n",
    "\n",
    "    if len(bookranges) == 0:\n",
    "        sys.exit(\"No training data has been found. Aborting.\")\n",
    "\n",
    "    # For validation, use roughly 90K of text,\n",
    "    # but no more than 10% of the entire text\n",
    "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "    # 10% of the text is how many files ?\n",
    "    total_len = len(codetext)\n",
    "    validation_len = 0\n",
    "    nb_books1 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books1 += 1\n",
    "        if validation_len > total_len // 10:\n",
    "            break\n",
    "\n",
    "    # 90K of text is how many books ?\n",
    "    validation_len = 0\n",
    "    nb_books2 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books2 += 1\n",
    "        if validation_len > 90*1024:\n",
    "            break\n",
    "\n",
    "    # 20% of the books is how many books ?\n",
    "    nb_books3 = len(bookranges) // 5\n",
    "\n",
    "    # pick the smallest\n",
    "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
    "\n",
    "    if nb_books == 0 or not validation:\n",
    "        cutoff = len(codetext)\n",
    "    else:\n",
    "        cutoff = bookranges[-nb_books][\"start\"]\n",
    "    valitext = codetext[cutoff:]\n",
    "    codetext = codetext[:cutoff]\n",
    "    return codetext, valitext, bookranges\n",
    "\n",
    "\n",
    "def print_data_stats(datalen, valilen, epoch_size):\n",
    "    datalen_mb = datalen/1024.0/1024.0\n",
    "    valilen_kb = valilen/1024.0\n",
    "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
    "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
    "\n",
    "\n",
    "def print_validation_header(validation_start, bookranges):\n",
    "    bookindex = find_book_index(validation_start, bookranges)\n",
    "    books = ''\n",
    "    for i in range(bookindex, len(bookranges)):\n",
    "        books += bookranges[i][\"name\"]\n",
    "        if i < len(bookranges)-1:\n",
    "            books += \", \"\n",
    "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
    "\n",
    "\n",
    "def print_validation_stats(loss, accuracy):\n",
    "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
    "                                                                                                           accuracy))\n",
    "\n",
    "\n",
    "def print_text_generation_header():\n",
    "    print()\n",
    "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
    "\n",
    "\n",
    "def print_text_generation_footer():\n",
    "    print()\n",
    "    print(\"└{:─^111}┘\".format('End of generation'))\n",
    "\n",
    "\n",
    "def frequency_limiter(n, multiple=1, modulo=0):\n",
    "    def limit(i):\n",
    "        return i % (multiple * n) == modulo*multiple\n",
    "    return limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file txts\\acts_new.txt\n",
      "Loading file txts\\gal_eph_new.txt\n",
      "Loading file txts\\heb_new.txt\n",
      "Loading file txts\\jam_jud_new.txt\n",
      "Loading file txts\\john_new.txt\n",
      "Loading file txts\\jud_rev_new.txt\n",
      "Loading file txts\\luke_8_john_new.txt\n",
      "Loading file txts\\mark01_new.txt\n",
      "Loading file txts\\matt02_new.txt\n",
      "Loading file txts\\matt_new.txt\n",
      "Loading file txts\\phil_col_new.txt\n",
      "Loading file txts\\thes_tim_new.txt\n",
      "Loading file txts\\tit_phl_new.txt\n"
     ]
    }
   ],
   "source": [
    "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
    "shakedir = \"txts/*.txt\"\n",
    "#shakedir = \"../tensorflow/**/*.py\"\n",
    "codetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text size is 5.83MB with 384.30KB set aside for validation. There will be 1018 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "# display some stats on the data\n",
    "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
    "txt.print_data_stats(len(codetext), len(valitext), epoch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layers):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # combined = torch.cat((input, hidden), 1)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(NLAYERS, BATCHSIZE, self.hidden_size, device=device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gru = GRU(ALPHASIZE, INTERNALSIZE, NLAYERS)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Softmax layer implementation:\n",
    "# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n",
    "# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n",
    "\n",
    "Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\n",
    "Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\n",
    "loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\n",
    "Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\n",
    "Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nParameters\\ninput_size – The number of expected features in the input x\\nhidden_size – The number of features in the hidden state h\\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, \\n    with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\\nbias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\\nbatch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\\ndropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\\nbidirectional – If True, becomes a bidirectional GRU. Default: False\\n\\nrnn = nn.GRU(10, 20, 2)\\ninput = torch.randn(5, 3, 10)\\nh0 = torch.randn(2, 3, 20)\\noutput, hn = rnn(input, h0)\\n\\ninput of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. \\n    The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details.\\n\\nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. \\n    Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1\\n    \\noutput of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. \\n    If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. \\n    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\\n    Similarly, the directions can be separated in the packed case.\\n\\nh_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\\n    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).\\n    \\nShape:\\nInput: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K)(N,C,d \\nTarget: (N)(N) where each value is 0 \\\\leq \\text{targets}[i] \\\\leq C-10≤targets[i]≤C−1 , or (N, d_1, d_2, ..., d_K)(N,d \\nOutput: scalar. If reduction is 'none', then the same size as the target: (N)(N) , or (N, d_1, d_2, ..., d_K)(N,d \\n\""
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss fn\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#from ok_seq2seq import EncoderRNN \\\n",
    "#                        ,DecoderRNN \\\n",
    "#                        ,AttnDecoderRNN \\\n",
    "#                        ,evaluateRandomly \\\n",
    "#                        ,teacher_forcing_ratio \n",
    "'''\n",
    "Parameters\n",
    "input_size – The number of expected features in the input x\n",
    "hidden_size – The number of features in the hidden state h\n",
    "num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, \n",
    "    with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\n",
    "bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "bidirectional – If True, becomes a bidirectional GRU. Default: False\n",
    "\n",
    "rnn = nn.GRU(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)\n",
    "\n",
    "input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. \n",
    "    The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details.\n",
    "\n",
    "h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. \n",
    "    Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1\n",
    "    \n",
    "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. \n",
    "    If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. \n",
    "    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\n",
    "    Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
    "    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).\n",
    "    \n",
    "Shape:\n",
    "Input: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K)(N,C,d \n",
    "Target: (N)(N) where each value is 0 \\leq \\text{targets}[i] \\leq C-10≤targets[i]≤C−1 , or (N, d_1, d_2, ..., d_K)(N,d \n",
    "Output: scalar. If reduction is 'none', then the same size as the target: (N)(N) , or (N, d_1, d_2, ..., d_K)(N,d \n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# stats for display\n",
    "seqloss = tf.reduce_mean(loss, 1)\n",
    "batchloss = tf.reduce_mean(seqloss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\n",
    "loss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\n",
    "acc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\n",
    "summaries = tf.summary.merge([loss_summary, acc_summary])\n",
    "\n",
    "# Init Tensorboard stuff. This will save Tensorboard information into a different\n",
    "# folder at each run named 'log/<timestamp>/'. Two sets of data are saved so that\n",
    "# you can compare training and validation curves visually in Tensorboard.\n",
    "timestamp = str(math.trunc(time.time()))\n",
    "summary_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-training\")\n",
    "validation_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-validation\")\n",
    "\n",
    "# Init for saving models. They will be saved into a directory named 'checkpoints'.\n",
    "# Only the last checkpoint is kept.\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")\n",
    "saver = tf.train.Saver(max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# training fn\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = gru.initHidden()\n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    fc_layer = nn.Linear(INTERNALSIZE, ALPHASIZE)        \n",
    "    gru.zero_grad()\n",
    "    \n",
    "    output, hidden = gru(line_tensor, hidden)\n",
    "    #print(f'gos={output.size()}, is={line_tensor.size()}')\n",
    "    output = fc_layer(output)\n",
    "    #print(f'fc={output.size()}, ls={line_tensor.size()}')\n",
    "    output = softmax(output)\n",
    "    #print(f'sm={output.size()}, cs[2:]={category_tensor.size()}')\n",
    "    input=output.transpose(0,1).transpose(1,2)\n",
    "    loss = criterion(input, category_tensor) # N (batch),C\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in gru.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output.transpose(0,1), loss.item()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# for display: init the progress bar\n",
    "DISPLAY_FREQ = 50\n",
    "_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n",
    "progress = txt.Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n",
    "\n",
    "# init\n",
    "istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "a=txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mb2t(rows):\n",
    "    rows=rows.transpose()\n",
    "    tensor = torch.zeros(rows.shape[0], rows.shape[1], ALPHASIZE, device=device)\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, letter_code in enumerate(row):\n",
    "            tensor[i][j][letter_code] = 1\n",
    "    return tensor\n",
    "\n",
    "def lin2txt(lt):\n",
    "    return ''.join([chr(txt.convert_to_alphabet(c))  if c != 0 else '' for c in lt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init train\n",
    "\n",
    "import time\n",
    "import math\n",
    "  # product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
    "\n",
    "\n",
    "print_every = 250\n",
    "plot_every = 500\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "iter=0\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 of  10 (0m 1s) 2659.7578  okunwengiapu (14) / iAqf\\\\n()Neee==C\"\"\"\"IIIIICCCCv(30) ✗  okunwengiapu m\n",
      "epoch 0 of  10 (0m 1s) 2659.7578 bara mi nwose o(15) / vvvvx&xB\n",
      "|ssZZQQQ7==kkYYY\"YYYY(30) ✗ ara mi nwose om\n",
      "epoch 0 of  10 (5m 31s) 1324.5800 wo sobie mun ok(15) / BBBGGGGGGGGGGn\\\\--[![tNNNN000J(30) ✗ o sobie mun oku\n",
      "epoch 0 of  10 (5m 31s) 1324.5800 o goyegoye nemi(15) / GGGG$5555_???`555_????00]]]]&&(30) ✗  goyegoye nemim\n",
      "epoch 0 of  10 (10m 56s) 2649.1596  isiapu ma bere(15) / dd!nnnnnn3ccIA&&T___FFF3888Abt(30) ✗ isiapu ma beren\n",
      "epoch 0 of  10 (10m 56s) 2649.1596 now naa kirikir(15) / ff@@@Sq[kk###aFFFFFnn~nnnnnnnn(30) ✗ ow naa kirikiri\n",
      "epoch 0 of  10 (15m 11s) 1324.5790 aist be ere bu (15) / OOOO???MFz999nntnn000000008888(30) ✗ ist be ere bu o\n",
      "epoch 0 of  10 (15m 11s) 1324.5790 ini vinpiki se (15) / L6XXXX++mmmmmmm3))p\"\"6+++zCCzz(30) ✗ ni vinpiki se s\n",
      "epoch 0 of  10 (19m 38s) 2649.1579  mun dumo bie c(15) / KK2^ooll;;BB,,oooSZPP:]]YYY2ww(30) ✗ mun dumo bie ch\n",
      "epoch 0 of  10 (19m 38s) 2649.1579 oloko dieapu ma(15) / PPPQQQm=mPPPPP??\"Y9TEE''''o^uu(30) ✗ loko dieapu ma \n",
      "epoch 1 of  10 (24m 9s) 1324.5798  ofori-e, aniat(15) / N5rP;;;jaazz<<<ff%iBBBBe//^^FF(30) ✗ ofori-e, aniati\n",
      "epoch 1 of  10 (24m 9s) 1324.5798 be bo kura mesi(15) / rrbb;n!~22!!+{11EEEFFFFF;;SSII(30) ✗ e bo kura mesi \n",
      "epoch 1 of  10 (28m 30s) 2649.1604  werima i nyana(15) / ``3333EEEqqq//hhffffJJJRII#..#(30) ✗ werima i nyanay\n",
      "epoch 1 of  10 (28m 30s) 2649.1604  mi ani iya ama(15) / ```c=qqff;ggJJJJJJDt%%%v######(30) ✗ mi ani iya aman\n",
      "epoch 1 of  10 (32m 24s) 1324.5787 reme, tamuno be(15) / c[00jjjj\n",
      "\n",
      "x`U=rrr33#--OONTTN:)(30) ✗ eme, tamuno be \n",
      "epoch 1 of  10 (32m 24s) 1324.5787 i. ani ori igbi(15) / ??kkFFC\tWWa@,,=n[[[[X`@oooo<<X(30) ✗ . ani ori igbik\n",
      "epoch 1 of  10 (36m 41s) 2649.1581 pu ma na stivin(15) / ?*J''''''nnnppnnnn\t\t++##%%%Mff(30) ✗ u ma na stivin \n",
      "epoch 1 of  10 (36m 41s) 2649.1581  sotoru bo enek(15) / WWWWW)ggP0gggggg}}}}ggiiiX^^^^(30) ✗ sotoru bo eneku\n",
      "epoch 2 of  10 (41m 2s) 1324.5797 lu ini nwose wa(15) / ##BBTTHH##aajjjjXXXz^^^!!\"vttt(30) ✗ u ini nwose war\n",
      "epoch 2 of  10 (41m 2s) 1324.5797 ie mgba deinma (15) / aaaaKKYYY!]]999hhH(-YYaa>>1]1{(30) ✗ e mgba deinma s\n",
      "epoch 2 of  10 (45m 10s) 2649.1601  lame. o tamuno(15) / yyy//(YYY776jjo.vOTT66YYYYYYpK(30) ✗ lame. o tamuno \n",
      "epoch 2 of  10 (45m 10s) 2649.1601 kian mi fie ye (15) / GGGs\t\t\tDDyRRZUUUOqqaaaMMM!MMMM(30) ✗ ian mi fie ye b\n",
      "epoch 2 of  10 (50m 10s) 1324.5815 os be ere bu ke(15) / iiiF\\\\cD``DDDDrrrrrrzD``$$OO''(30) ✗ s be ere bu ken\n",
      "epoch 2 of  10 (50m 10s) 1324.5815 sin inibo pirib(15) / UbbsddHHHH55___D\n",
      "\n",
      "\n",
      "U555555(26) ✗ in inibo piribi\n",
      "epoch 2 of  10 (54m 44s) 2649.1600 dam askos bie s(15) / wwwwXXXXXAXooooNaa]]nnnF222r-L(30) ✗ am askos bie si\n",
      "epoch 2 of  10 (54m 44s) 2649.1600 eme. antiok bie(15) / eeE11j555-55ssst===@&&ddND==22(30) ✗ me. antiok bie \n",
      "epoch 3 of  10 (59m 29s) 1324.5785 piki grik okwei(15) / BBTddTTTUUGG~&&~~d\"\"\"\"\"\"\"\",,++(30) ✗ iki grik okwein\n",
      "epoch 3 of  10 (59m 29s) 1324.5785 e na o barachua(15) / ,P++J;!!!>>>@@@>''i1''^^!!!|2r(30) ✗  na o barachua \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-435-0496fd814472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlin2txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mline_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmb2t\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-412-f59522c76120>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(category_tensor, line_tensor)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# N (batch),C\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Add parameters' gradients to their values, multiplied by learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programs\\python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programs\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "plt.figure()\n",
    "vloss=[]\n",
    "VALI_SEQLEN = 30\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=nb_epoch):\n",
    "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    category =  [lin2txt(l) for l in y_]\n",
    "    lines = [lin2txt(l) for l in x]\n",
    "    line_tensor=mb2t(x)\n",
    "    output, loss = train(torch.tensor(y_, device=device, dtype=torch.long), line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess = [lin2txt([ch.argmax(dim=0) for ch in line]) for line in output]\n",
    "        for i in range(2):\n",
    "            correct = '✓' if guess[i] == category[i] else '✗ %s' % category[i] \n",
    "            closs = '%.4f' % current_loss\n",
    "            print(f'epoch {epoch} of  {nb_epoch} ({timeSince(start)}) {closs} {lines[i]}({len(lines[i])}) / {guess[i]}({len(guess[i])}) {correct}' )\n",
    " \n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0 and len(valitext) > 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        plt.plot(all_losses)\n",
    "        current_loss = 0\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, BATCHSIZE, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        line_tensor = mb2t(vali_x)\n",
    "        output, loss = train(torch.tensor(vali_y, device=device, dtype=torch.long), line_tensor)\n",
    "        vloss.append(loss)\n",
    "        plt.plot(vloss)      \n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(vloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# training loop\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10):\n",
    "\n",
    "    # train on one minibatch\n",
    "    # feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n",
    "    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n",
    "\n",
    "    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n",
    "    if step % _50_BATCHES == 0:\n",
    "        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n",
    "        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n",
    "        summary_writer.add_summary(smm, step)\n",
    "\n",
    "    # run a validation step every 50 batches\n",
    "    # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n",
    "    # so we cut it up and batch the pieces (slightly inaccurate)\n",
    "    # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n",
    "    if step % _50_BATCHES == 0 and len(valitext) > 0:\n",
    "        VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n",
    "        bsize = len(valitext) // VALI_SEQLEN\n",
    "        txt.print_validation_header(len(codetext), bookranges)\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n",
    "        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n",
    "                     batchsize: bsize}\n",
    "        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_validation_stats(ls, acc)\n",
    "        # save validation data for Tensorboard\n",
    "        validation_writer.add_summary(smm, step)\n",
    "\n",
    "    # display a short text generated with the current weights and biases (every 150 batches)\n",
    "    if step // 3 % _50_BATCHES == 0:\n",
    "        txt.print_text_generation_header()\n",
    "        ry = np.array([[txt.convert_from_alphabet(ord(\"K\"))]])\n",
    "        rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n",
    "        for k in range(1000):\n",
    "            ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n",
    "            rc = txt.sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n",
    "            print(chr(txt.convert_to_alphabet(rc)), end=\"\")\n",
    "            ry = np.array([[rc]])\n",
    "        txt.print_text_generation_footer()\n",
    "\n",
    "    # save a checkpoint (every 500 batches)\n",
    "    if step // 10 % _50_BATCHES == 0:\n",
    "        saved_file = saver.save(sess, 'checkpoints/rnn_train_' + timestamp, global_step=step)\n",
    "        print(\"Saved file: \" + saved_file)\n",
    "\n",
    "    # display progress bar\n",
    "    progress.step(reset=step % _50_BATCHES == 0)\n",
    "\n",
    "    # loop state around\n",
    "    istate = ostate\n",
    "    step += BATCHSIZE * SEQLEN"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
