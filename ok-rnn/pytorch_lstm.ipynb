{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.contrib import layers\n",
    "#from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import my_txtutils as txt\n",
    "#tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# model parameters\n",
    "#\n",
    "# Usage:\n",
    "#   Training only:\n",
    "#         Leave all the parameters as they are\n",
    "#         Disable validation to run a bit faster (set validation=False below)\n",
    "#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n",
    "#   Training and experimentation (default):\n",
    "#         Keep validation enabled\n",
    "#         You can now play with the parameters anf follow the effects in Tensorboard\n",
    "#         A good choice of parameters ensures that the testing and validation curves stay close\n",
    "#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n",
    "#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n",
    "#\n",
    "nb_epoch=10\n",
    "SEQLEN = 30\n",
    "BATCHSIZE = 200\n",
    "ALPHASIZE = txt.ALPHASIZE\n",
    "INTERNALSIZE = 256\n",
    "NLAYERS = 3\n",
    "learning_rate = 0.001  # fixed learning rate\n",
    "dropout_pkeep = 0.8    # some dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# size of the alphabet that we work with\n",
    "ALPHASIZE = 98\n",
    "\n",
    "\n",
    "# Specification of the supported alphabet (subset of ASCII-7)\n",
    "# 10 line feed LF\n",
    "# 32-64 numbers and punctuation\n",
    "# 65-90 upper-case letters\n",
    "# 91-97 more punctuation\n",
    "# 97-122 lower-case letters\n",
    "# 123-126 more punctuation\n",
    "def convert_from_alphabet(a):\n",
    "    \"\"\"Encode a character\n",
    "    :param a: one character\n",
    "    :return: the encoded value\n",
    "    \"\"\"\n",
    "    if a == 9:\n",
    "        return 1\n",
    "    if a == 10:\n",
    "        return 127 - 30  # LF\n",
    "    elif 32 <= a <= 126:\n",
    "        return a - 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "# encoded values:\n",
    "# unknown = 0\n",
    "# tab = 1\n",
    "# space = 2\n",
    "# all chars from 32 to 126 = c-30\n",
    "# LF mapped to 127-30\n",
    "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode a code point\n",
    "    :param c: code point\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return: decoded character\n",
    "    \"\"\"\n",
    "    if c == 1:\n",
    "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
    "    if c == 127 - 30:\n",
    "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
    "    if 32 <= c + 30 <= 126:\n",
    "        return c + 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "def encode_text(s):\n",
    "    \"\"\"Encode a string.\n",
    "    :param s: a text string\n",
    "    :return: encoded list of code points\n",
    "    \"\"\"\n",
    "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
    "\n",
    "\n",
    "def decode_to_text(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode an encoded string.\n",
    "    :param c: encoded list of code points\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
    "\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(probabilities)\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
    "\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "\n",
    "\n",
    "def find_book(index, bookranges):\n",
    "    return next(\n",
    "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def find_book_index(index, bookranges):\n",
    "    return next(\n",
    "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
    "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
    "    print()\n",
    "    # epoch_size in number of batches\n",
    "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
    "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
    "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "    for k in range(batch_size):\n",
    "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
    "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
    "        bookname = find_book(index_in_epoch, bookranges)\n",
    "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
    "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
    "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
    "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
    "        print(print_string.format(decx, decy, loss_string))\n",
    "        index += sequence_len\n",
    "    # box formatting characters:\n",
    "    # │ \\u2502\n",
    "    # ─ \\u2500\n",
    "    # └ \\u2514\n",
    "    # ┘ \\u2518\n",
    "    # ┴ \\u2534\n",
    "    # ┌ \\u250C\n",
    "    # ┐ \\u2510\n",
    "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
    "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
    "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
    "    print(footer)\n",
    "    # print statistics\n",
    "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
    "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
    "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
    "    print()\n",
    "    print(\"TRAINING STATS: {}\".format(stats))\n",
    "\n",
    "\n",
    "class Progress:\n",
    "    \"\"\"Text mode progress bar.\n",
    "    Usage:\n",
    "            p = Progress(30)\n",
    "            p.step()\n",
    "            p.step()\n",
    "            p.step(start=True) # to restart form 0%\n",
    "    The progress bar displays a new header at each restart.\"\"\"\n",
    "    def __init__(self, maxi, size=100, msg=\"\"):\n",
    "        \"\"\"\n",
    "        :param maxi: the number of steps required to reach 100%\n",
    "        :param size: the number of characters taken on the screen by the progress bar\n",
    "        :param msg: the message displayed in the header of the progress bat\n",
    "        \"\"\"\n",
    "        self.maxi = maxi\n",
    "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
    "        self.header_printed = False\n",
    "        self.msg = msg\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, reset=False):\n",
    "        if reset:\n",
    "            self.__init__(self.maxi, self.size, self.msg)\n",
    "        if not self.header_printed:\n",
    "            self.__print_header()\n",
    "        next(self.p)\n",
    "\n",
    "    def __print_header(self):\n",
    "        print()\n",
    "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
    "        print(format_string.format(self.msg))\n",
    "        self.header_printed = True\n",
    "\n",
    "    def __start_progress(self, maxi):\n",
    "        def print_progress():\n",
    "            # Bresenham's algorithm. Yields the number of dots printed.\n",
    "            # This will always print 100 dots in max invocations.\n",
    "            dx = maxi\n",
    "            dy = self.size\n",
    "            d = dy - dx\n",
    "            for x in range(maxi):\n",
    "                k = 0\n",
    "                while d >= 0:\n",
    "                    print('=', end=\"\", flush=True)\n",
    "                    k += 1\n",
    "                    d -= dx\n",
    "                d += dy\n",
    "                yield k\n",
    "\n",
    "        return print_progress\n",
    "\n",
    "\n",
    "def read_data_files(directory, validation=True):\n",
    "    \"\"\"Read data files according to the specified glob pattern\n",
    "    Optionnaly set aside the last file as validation data.\n",
    "    No validation data is returned if there are 5 files or less.\n",
    "    :param directory: for example \"data/*.txt\"\n",
    "    :param validation: if True (default), sets the last file aside as validation data\n",
    "    :return: training data, validation data, list of loaded file names with ranges\n",
    "     If validation is\n",
    "    \"\"\"\n",
    "    codetext = []\n",
    "    bookranges = []\n",
    "    shakelist = glob.glob(directory, recursive=True)\n",
    "    for shakefile in shakelist:\n",
    "        shaketext = open(shakefile, \"r\")\n",
    "        print(\"Loading file \" + shakefile)\n",
    "        start = len(codetext)\n",
    "        codetext.extend(encode_text(shaketext.read()))\n",
    "        end = len(codetext)\n",
    "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
    "        shaketext.close()\n",
    "\n",
    "    if len(bookranges) == 0:\n",
    "        sys.exit(\"No training data has been found. Aborting.\")\n",
    "\n",
    "    # For validation, use roughly 90K of text,\n",
    "    # but no more than 10% of the entire text\n",
    "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "    # 10% of the text is how many files ?\n",
    "    total_len = len(codetext)\n",
    "    validation_len = 0\n",
    "    nb_books1 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books1 += 1\n",
    "        if validation_len > total_len // 10:\n",
    "            break\n",
    "\n",
    "    # 90K of text is how many books ?\n",
    "    validation_len = 0\n",
    "    nb_books2 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books2 += 1\n",
    "        if validation_len > 90*1024:\n",
    "            break\n",
    "\n",
    "    # 20% of the books is how many books ?\n",
    "    nb_books3 = len(bookranges) // 5\n",
    "\n",
    "    # pick the smallest\n",
    "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
    "\n",
    "    if nb_books == 0 or not validation:\n",
    "        cutoff = len(codetext)\n",
    "    else:\n",
    "        cutoff = bookranges[-nb_books][\"start\"]\n",
    "    valitext = codetext[cutoff:]\n",
    "    codetext = codetext[:cutoff]\n",
    "    return codetext, valitext, bookranges\n",
    "\n",
    "\n",
    "def print_data_stats(datalen, valilen, epoch_size):\n",
    "    datalen_mb = datalen/1024.0/1024.0\n",
    "    valilen_kb = valilen/1024.0\n",
    "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
    "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
    "\n",
    "\n",
    "def print_validation_header(validation_start, bookranges):\n",
    "    bookindex = find_book_index(validation_start, bookranges)\n",
    "    books = ''\n",
    "    for i in range(bookindex, len(bookranges)):\n",
    "        books += bookranges[i][\"name\"]\n",
    "        if i < len(bookranges)-1:\n",
    "            books += \", \"\n",
    "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
    "\n",
    "\n",
    "def print_validation_stats(loss, accuracy):\n",
    "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
    "                                                                                                           accuracy))\n",
    "\n",
    "\n",
    "def print_text_generation_header():\n",
    "    print()\n",
    "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
    "\n",
    "\n",
    "def print_text_generation_footer():\n",
    "    print()\n",
    "    print(\"└{:─^111}┘\".format('End of generation'))\n",
    "\n",
    "\n",
    "def frequency_limiter(n, multiple=1, modulo=0):\n",
    "    def limit(i):\n",
    "        return i % (multiple * n) == modulo*multiple\n",
    "    return limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file txts\\acts_new.txt\n",
      "Loading file txts\\gal_eph_new.txt\n",
      "Loading file txts\\heb_new.txt\n",
      "Loading file txts\\jam_jud_new.txt\n",
      "Loading file txts\\john_new.txt\n",
      "Loading file txts\\jud_rev_new.txt\n",
      "Loading file txts\\luke_8_john_new.txt\n",
      "Loading file txts\\mark01_new.txt\n",
      "Loading file txts\\matt02_new.txt\n",
      "Loading file txts\\matt_new.txt\n",
      "Loading file txts\\phil_col_new.txt\n",
      "Loading file txts\\thes_tim_new.txt\n",
      "Loading file txts\\tit_phl_new.txt\n"
     ]
    }
   ],
   "source": [
    "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
    "shakedir = \"txts/*.txt\"\n",
    "#shakedir = \"../tensorflow/**/*.py\"\n",
    "codetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text size is 5.83MB with 384.30KB set aside for validation. There will be 1018 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "# display some stats on the data\n",
    "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
    "txt.print_data_stats(len(codetext), len(valitext), epoch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#\n",
    "# the model (see FAQ in README.md)\n",
    "#\n",
    "lr = tf.placeholder(tf.float32, name='lr')  # learning rate\n",
    "pkeep = tf.placeholder(tf.float32, name='pkeep')  # dropout parameter\n",
    "batchsize = tf.placeholder(tf.int32, name='batchsize')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# inputs\n",
    "X = tf.placeholder(tf.uint8, [None, None], name='X')    # [ BATCHSIZE, SEQLEN ]\n",
    "Xo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n",
    "# expected outputs = same sequence shifted by 1 since we are trying to predict the next character\n",
    "Y_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  # [ BATCHSIZE, SEQLEN ]\n",
    "Yo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n",
    "# input state\n",
    "Hin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
    "\n",
    "# using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times\n",
    "# dynamic_rnn infers SEQLEN from the size of the inputs Xo\n",
    "\n",
    "# How to properly apply dropout in RNNs: see README.md\n",
    "cells = [rnn.GRUCell(INTERNALSIZE) for _ in range(NLAYERS)]\n",
    "# \"naive dropout\" implementation\n",
    "dropcells = [rnn.DropoutWrapper(cell,input_keep_prob=pkeep) for cell in cells]\n",
    "multicell = rnn.MultiRNNCell(dropcells, state_is_tuple=False)\n",
    "multicell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)  # dropout for the softmax layer\n",
    "\n",
    "Yr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)\n",
    "# Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]\n",
    "# H:  [ BATCHSIZE, INTERNALSIZE*NLAYERS ] # this is the last state in the sequence\n",
    "\n",
    "H = tf.identity(H, name='H')  # just to give it a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layers):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # combined = torch.cat((input, hidden), 1)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(NLAYERS, BATCHSIZE, self.hidden_size, device=device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gru = GRU(ALPHASIZE, INTERNALSIZE, NLAYERS)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Softmax layer implementation:\n",
    "# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n",
    "# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n",
    "\n",
    "Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\n",
    "Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\n",
    "loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\n",
    "Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\n",
    "Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nParameters\\ninput_size – The number of expected features in the input x\\nhidden_size – The number of features in the hidden state h\\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, \\n    with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\\nbias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\\nbatch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\\ndropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\\nbidirectional – If True, becomes a bidirectional GRU. Default: False\\n\\nrnn = nn.GRU(10, 20, 2)\\ninput = torch.randn(5, 3, 10)\\nh0 = torch.randn(2, 3, 20)\\noutput, hn = rnn(input, h0)\\n\\ninput of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. \\n    The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details.\\n\\nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. \\n    Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1\\n    \\noutput of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. \\n    If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. \\n    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\\n    Similarly, the directions can be separated in the packed case.\\n\\nh_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\\n    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).\\n    \\nShape:\\nInput: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K)(N,C,d \\nTarget: (N)(N) where each value is 0 \\\\leq \\text{targets}[i] \\\\leq C-10≤targets[i]≤C−1 , or (N, d_1, d_2, ..., d_K)(N,d \\nOutput: scalar. If reduction is 'none', then the same size as the target: (N)(N) , or (N, d_1, d_2, ..., d_K)(N,d \\n\""
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss fn\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#from ok_seq2seq import EncoderRNN \\\n",
    "#                        ,DecoderRNN \\\n",
    "#                        ,AttnDecoderRNN \\\n",
    "#                        ,evaluateRandomly \\\n",
    "#                        ,teacher_forcing_ratio \n",
    "'''\n",
    "Parameters\n",
    "input_size – The number of expected features in the input x\n",
    "hidden_size – The number of features in the hidden state h\n",
    "num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, \n",
    "    with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\n",
    "bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "bidirectional – If True, becomes a bidirectional GRU. Default: False\n",
    "\n",
    "rnn = nn.GRU(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)\n",
    "\n",
    "input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. \n",
    "    The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details.\n",
    "\n",
    "h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. \n",
    "    Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1\n",
    "    \n",
    "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. \n",
    "    If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. \n",
    "    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\n",
    "    Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
    "    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).\n",
    "    \n",
    "Shape:\n",
    "Input: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K)(N,C,d \n",
    "Target: (N)(N) where each value is 0 \\leq \\text{targets}[i] \\leq C-10≤targets[i]≤C−1 , or (N, d_1, d_2, ..., d_K)(N,d \n",
    "Output: scalar. If reduction is 'none', then the same size as the target: (N)(N) , or (N, d_1, d_2, ..., d_K)(N,d \n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# stats for display\n",
    "seqloss = tf.reduce_mean(loss, 1)\n",
    "batchloss = tf.reduce_mean(seqloss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\n",
    "loss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\n",
    "acc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\n",
    "summaries = tf.summary.merge([loss_summary, acc_summary])\n",
    "\n",
    "# Init Tensorboard stuff. This will save Tensorboard information into a different\n",
    "# folder at each run named 'log/<timestamp>/'. Two sets of data are saved so that\n",
    "# you can compare training and validation curves visually in Tensorboard.\n",
    "timestamp = str(math.trunc(time.time()))\n",
    "summary_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-training\")\n",
    "validation_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-validation\")\n",
    "\n",
    "# Init for saving models. They will be saved into a directory named 'checkpoints'.\n",
    "# Only the last checkpoint is kept.\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")\n",
    "saver = tf.train.Saver(max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# training fn\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = gru.initHidden()\n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    fc_layer = nn.Linear(INTERNALSIZE, ALPHASIZE)        \n",
    "    gru.zero_grad()\n",
    "    \n",
    "    output, hidden = gru(line_tensor, hidden)\n",
    "    #print(f'gos={output.size()}, is={line_tensor.size()}')\n",
    "    output = fc_layer(output)\n",
    "    #print(f'fc={output.size()}, ls={line_tensor.size()}')\n",
    "    output = softmax(output)\n",
    "    #print(f'sm={output.size()}, cs[2:]={category_tensor.size()}')\n",
    "    input=output.transpose(0,1).transpose(1,2)\n",
    "    loss = criterion(input, category_tensor) # N (batch),C\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in gru.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output.transpose(0,1), loss.item()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# for display: init the progress bar\n",
    "DISPLAY_FREQ = 50\n",
    "_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n",
    "progress = txt.Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n",
    "\n",
    "# init\n",
    "istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "a=txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mb2t(rows):\n",
    "    rows=rows.transpose()\n",
    "    tensor = torch.zeros(rows.shape[0], rows.shape[1], ALPHASIZE, device=device)\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, letter_code in enumerate(row):\n",
    "            tensor[i][j][letter_code] = 1\n",
    "    return tensor\n",
    "\n",
    "def lin2txt(lt):\n",
    "    return ''.join([chr(txt.convert_to_alphabet(c))  if c != 0 else '' for c in lt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init train\n",
    "\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
    "\n",
    "\n",
    "print_every = 250\n",
    "plot_every = 500\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "iter=0\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 of  10 (0m 5s) 5.298402786254883  okunwengiapu (14) / aaakkaN||||ppp\\\\W\"\"A[[[[\"\"\\\\\\.(30) ✗  okunwengiapu m\n",
      "epoch 0 of  10 (0m 5s) 5.298402786254883 bara mi nwose o(15) / pppd^ZZZZZZOHHHHHyjjjj\t\taa]]t8(30) ✗ ara mi nwose om\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "plt.figure()\n",
    "vloss=[]\n",
    "VALI_SEQLEN = 30\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=nb_epoch):\n",
    "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    category =  [lin2txt(l) for l in y_]\n",
    "    lines = [lin2txt(l) for l in x]\n",
    "    line_tensor=mb2t(x)\n",
    "    output, loss = train(torch.tensor(y_, device=device, dtype=torch.long), line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess = [lin2txt([ch.argmax(dim=0) for ch in line]) for line in output]\n",
    "        for i in range(2):\n",
    "            correct = '✓' if guess[i] == category[i] else '✗ %s' % category[i] \n",
    "            print(f'epoch {epoch} of  {nb_epoch} ({timeSince(start)}) {loss} {lines[i]}({len(lines[i])}) / {guess[i]}({len(guess[i])}) {correct}' )\n",
    " \n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0 and len(valitext) > 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        plt.plot(all_losses)\n",
    "        current_loss = 0\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, BATCHSIZE, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        line_tensor = mb2t(vali_x)\n",
    "        output, loss = train(torch.tensor(vali_y, device=device, dtype=torch.long), line_tensor)\n",
    "        vloss.append(loss)\n",
    "        plt.plot(vloss)      \n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(vloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# training loop\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10):\n",
    "\n",
    "    # train on one minibatch\n",
    "    # feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n",
    "    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n",
    "\n",
    "    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n",
    "    if step % _50_BATCHES == 0:\n",
    "        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n",
    "        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n",
    "        summary_writer.add_summary(smm, step)\n",
    "\n",
    "    # run a validation step every 50 batches\n",
    "    # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n",
    "    # so we cut it up and batch the pieces (slightly inaccurate)\n",
    "    # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n",
    "    if step % _50_BATCHES == 0 and len(valitext) > 0:\n",
    "        VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n",
    "        bsize = len(valitext) // VALI_SEQLEN\n",
    "        txt.print_validation_header(len(codetext), bookranges)\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n",
    "        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n",
    "                     batchsize: bsize}\n",
    "        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_validation_stats(ls, acc)\n",
    "        # save validation data for Tensorboard\n",
    "        validation_writer.add_summary(smm, step)\n",
    "\n",
    "    # display a short text generated with the current weights and biases (every 150 batches)\n",
    "    if step // 3 % _50_BATCHES == 0:\n",
    "        txt.print_text_generation_header()\n",
    "        ry = np.array([[txt.convert_from_alphabet(ord(\"K\"))]])\n",
    "        rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n",
    "        for k in range(1000):\n",
    "            ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n",
    "            rc = txt.sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n",
    "            print(chr(txt.convert_to_alphabet(rc)), end=\"\")\n",
    "            ry = np.array([[rc]])\n",
    "        txt.print_text_generation_footer()\n",
    "\n",
    "    # save a checkpoint (every 500 batches)\n",
    "    if step // 10 % _50_BATCHES == 0:\n",
    "        saved_file = saver.save(sess, 'checkpoints/rnn_train_' + timestamp, global_step=step)\n",
    "        print(\"Saved file: \" + saved_file)\n",
    "\n",
    "    # display progress bar\n",
    "    progress.step(reset=step % _50_BATCHES == 0)\n",
    "\n",
    "    # loop state around\n",
    "    istate = ostate\n",
    "    step += BATCHSIZE * SEQLEN"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
