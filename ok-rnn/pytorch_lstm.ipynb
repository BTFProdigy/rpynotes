{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.contrib import layers\n",
    "#from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import my_txtutils as txt\n",
    "import datetime\n",
    "import torch\n",
    "import json\n",
    "#tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# model parameters\n",
    "#\n",
    "# Usage:\n",
    "#   Training only:\n",
    "#         Leave all the parameters as they are\n",
    "#         Disable validation to run a bit faster (set validation=False below)\n",
    "#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n",
    "#   Training and experimentation (default):\n",
    "#         Keep validation enabled\n",
    "#         You can now play with the parameters anf follow the effects in Tensorboard\n",
    "#         A good choice of parameters ensures that the testing and validation curves stay close\n",
    "#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n",
    "#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n",
    "#\n",
    "nb_epoch=10\n",
    "SEQLEN = 30\n",
    "BATCHSIZE = 200\n",
    "ALPHASIZE = txt.ALPHASIZE\n",
    "INTERNALSIZE = 256\n",
    "NLAYERS = 3\n",
    "learning_rate = 0.001  # fixed learning rate\n",
    "dropout_pkeep = 0.8    # some dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# size of the alphabet that we work with\n",
    "ALPHASIZE = 98\n",
    "\n",
    "\n",
    "# Specification of the supported alphabet (subset of ASCII-7)\n",
    "# 10 line feed LF\n",
    "# 32-64 numbers and punctuation\n",
    "# 65-90 upper-case letters\n",
    "# 91-97 more punctuation\n",
    "# 97-122 lower-case letters\n",
    "# 123-126 more punctuation\n",
    "def convert_from_alphabet(a):\n",
    "    \"\"\"Encode a character\n",
    "    :param a: one character\n",
    "    :return: the encoded value\n",
    "    \"\"\"\n",
    "    if a == 9:\n",
    "        return 1\n",
    "    if a == 10:\n",
    "        return 127 - 30  # LF\n",
    "    elif 32 <= a <= 126:\n",
    "        return a - 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "# encoded values:\n",
    "# unknown = 0\n",
    "# tab = 1\n",
    "# space = 2\n",
    "# all chars from 32 to 126 = c-30\n",
    "# LF mapped to 127-30\n",
    "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode a code point\n",
    "    :param c: code point\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return: decoded character\n",
    "    \"\"\"\n",
    "    if c == 1:\n",
    "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
    "    if c == 127 - 30:\n",
    "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
    "    if 32 <= c + 30 <= 126:\n",
    "        return c + 30\n",
    "    else:\n",
    "        return 0  # unknown\n",
    "\n",
    "\n",
    "def encode_text(s):\n",
    "    \"\"\"Encode a string.\n",
    "    :param s: a text string\n",
    "    :return: encoded list of code points\n",
    "    \"\"\"\n",
    "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
    "\n",
    "\n",
    "def decode_to_text(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode an encoded string.\n",
    "    :param c: encoded list of code points\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n",
    "\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=ALPHASIZE):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(probabilities)\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n",
    "\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "\n",
    "\n",
    "def find_book(index, bookranges):\n",
    "    return next(\n",
    "        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def find_book_index(index, bookranges):\n",
    "    return next(\n",
    "        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n",
    "\n",
    "\n",
    "def print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n",
    "    \"\"\"Display utility for printing learning statistics\"\"\"\n",
    "    print()\n",
    "    # epoch_size in number of batches\n",
    "    batch_size = X.shape[0]  # batch_size in number of sequences\n",
    "    sequence_len = X.shape[1]  # sequence_len in number of characters\n",
    "    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "    for k in range(batch_size):\n",
    "        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n",
    "        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n",
    "        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n",
    "        bookname = find_book(index_in_epoch, bookranges)\n",
    "        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n",
    "        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n",
    "        loss_string = \"loss: {:.5f}\".format(losses[k])\n",
    "        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n",
    "        print(print_string.format(decx, decy, loss_string))\n",
    "        index += sequence_len\n",
    "    # box formatting characters:\n",
    "    # │ \\u2502\n",
    "    # ─ \\u2500\n",
    "    # └ \\u2514\n",
    "    # ┘ \\u2518\n",
    "    # ┴ \\u2534\n",
    "    # ┌ \\u250C\n",
    "    # ┐ \\u2510\n",
    "    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n",
    "    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n",
    "    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n",
    "    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n",
    "    print(footer)\n",
    "    # print statistics\n",
    "    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n",
    "    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n",
    "    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n",
    "    print()\n",
    "    print(\"TRAINING STATS: {}\".format(stats))\n",
    "\n",
    "\n",
    "class Progress:\n",
    "    \"\"\"Text mode progress bar.\n",
    "    Usage:\n",
    "            p = Progress(30)\n",
    "            p.step()\n",
    "            p.step()\n",
    "            p.step(start=True) # to restart form 0%\n",
    "    The progress bar displays a new header at each restart.\"\"\"\n",
    "    def __init__(self, maxi, size=100, msg=\"\"):\n",
    "        \"\"\"\n",
    "        :param maxi: the number of steps required to reach 100%\n",
    "        :param size: the number of characters taken on the screen by the progress bar\n",
    "        :param msg: the message displayed in the header of the progress bat\n",
    "        \"\"\"\n",
    "        self.maxi = maxi\n",
    "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
    "        self.header_printed = False\n",
    "        self.msg = msg\n",
    "        self.size = size\n",
    "\n",
    "    def step(self, reset=False):\n",
    "        if reset:\n",
    "            self.__init__(self.maxi, self.size, self.msg)\n",
    "        if not self.header_printed:\n",
    "            self.__print_header()\n",
    "        next(self.p)\n",
    "\n",
    "    def __print_header(self):\n",
    "        print()\n",
    "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
    "        print(format_string.format(self.msg))\n",
    "        self.header_printed = True\n",
    "\n",
    "    def __start_progress(self, maxi):\n",
    "        def print_progress():\n",
    "            # Bresenham's algorithm. Yields the number of dots printed.\n",
    "            # This will always print 100 dots in max invocations.\n",
    "            dx = maxi\n",
    "            dy = self.size\n",
    "            d = dy - dx\n",
    "            for x in range(maxi):\n",
    "                k = 0\n",
    "                while d >= 0:\n",
    "                    print('=', end=\"\", flush=True)\n",
    "                    k += 1\n",
    "                    d -= dx\n",
    "                d += dy\n",
    "                yield k\n",
    "\n",
    "        return print_progress\n",
    "\n",
    "\n",
    "def read_data_files(directory, validation=True):\n",
    "    \"\"\"Read data files according to the specified glob pattern\n",
    "    Optionnaly set aside the last file as validation data.\n",
    "    No validation data is returned if there are 5 files or less.\n",
    "    :param directory: for example \"data/*.txt\"\n",
    "    :param validation: if True (default), sets the last file aside as validation data\n",
    "    :return: training data, validation data, list of loaded file names with ranges\n",
    "     If validation is\n",
    "    \"\"\"\n",
    "    codetext = []\n",
    "    bookranges = []\n",
    "    shakelist = glob.glob(directory, recursive=True)\n",
    "    for shakefile in shakelist:\n",
    "        shaketext = open(shakefile, \"r\")\n",
    "        print(\"Loading file \" + shakefile)\n",
    "        start = len(codetext)\n",
    "        codetext.extend(encode_text(shaketext.read()))\n",
    "        end = len(codetext)\n",
    "        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n",
    "        shaketext.close()\n",
    "\n",
    "    if len(bookranges) == 0:\n",
    "        sys.exit(\"No training data has been found. Aborting.\")\n",
    "\n",
    "    # For validation, use roughly 90K of text,\n",
    "    # but no more than 10% of the entire text\n",
    "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "    # 10% of the text is how many files ?\n",
    "    total_len = len(codetext)\n",
    "    validation_len = 0\n",
    "    nb_books1 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books1 += 1\n",
    "        if validation_len > total_len // 10:\n",
    "            break\n",
    "\n",
    "    # 90K of text is how many books ?\n",
    "    validation_len = 0\n",
    "    nb_books2 = 0\n",
    "    for book in reversed(bookranges):\n",
    "        validation_len += book[\"end\"]-book[\"start\"]\n",
    "        nb_books2 += 1\n",
    "        if validation_len > 90*1024:\n",
    "            break\n",
    "\n",
    "    # 20% of the books is how many books ?\n",
    "    nb_books3 = len(bookranges) // 5\n",
    "\n",
    "    # pick the smallest\n",
    "    nb_books = min(nb_books1, nb_books2, nb_books3)\n",
    "\n",
    "    if nb_books == 0 or not validation:\n",
    "        cutoff = len(codetext)\n",
    "    else:\n",
    "        cutoff = bookranges[-nb_books][\"start\"]\n",
    "    valitext = codetext[cutoff:]\n",
    "    codetext = codetext[:cutoff]\n",
    "    return codetext, valitext, bookranges\n",
    "\n",
    "\n",
    "def print_data_stats(datalen, valilen, epoch_size):\n",
    "    datalen_mb = datalen/1024.0/1024.0\n",
    "    valilen_kb = valilen/1024.0\n",
    "    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n",
    "          + \" There will be {} batches per epoch\".format(epoch_size))\n",
    "\n",
    "\n",
    "def print_validation_header(validation_start, bookranges):\n",
    "    bookindex = find_book_index(validation_start, bookranges)\n",
    "    books = ''\n",
    "    for i in range(bookindex, len(bookranges)):\n",
    "        books += bookranges[i][\"name\"]\n",
    "        if i < len(bookranges)-1:\n",
    "            books += \", \"\n",
    "    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n",
    "\n",
    "\n",
    "def print_validation_stats(loss, accuracy):\n",
    "    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n",
    "                                                                                                           accuracy))\n",
    "\n",
    "\n",
    "def print_text_generation_header():\n",
    "    print()\n",
    "    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n",
    "\n",
    "\n",
    "def print_text_generation_footer():\n",
    "    print()\n",
    "    print(\"└{:─^111}┘\".format('End of generation'))\n",
    "\n",
    "\n",
    "def frequency_limiter(n, multiple=1, modulo=0):\n",
    "    def limit(i):\n",
    "        return i % (multiple * n) == modulo*multiple\n",
    "    return limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file txts\\acts_new.txt\n",
      "Loading file txts\\gal_eph_new.txt\n",
      "Loading file txts\\heb_new.txt\n",
      "Loading file txts\\jam_jud_new.txt\n",
      "Loading file txts\\john_new.txt\n",
      "Loading file txts\\jud_rev_new.txt\n",
      "Loading file txts\\luke_8_john_new.txt\n",
      "Loading file txts\\mark01_new.txt\n",
      "Loading file txts\\matt02_new.txt\n",
      "Loading file txts\\matt_new.txt\n",
      "Loading file txts\\phil_col_new.txt\n",
      "Loading file txts\\thes_tim_new.txt\n",
      "Loading file txts\\tit_phl_new.txt\n"
     ]
    }
   ],
   "source": [
    "# load data, either shakespeare, or the Python source of Tensorflow itself\n",
    "shakedir = \"txts/*.txt\"\n",
    "#shakedir = \"../tensorflow/**/*.py\"\n",
    "codetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text size is 2.92MB with 192.15KB set aside for validation. There will be 509 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "# display some stats on the data\n",
    "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
    "txt.print_data_stats(len(codetext), len(valitext), epoch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\programs\\python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\programs\\python38\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\programs\\Python38\\Scripts\\pip.exe\\__main__.py\", line 4, in <module>\n",
      "ModuleNotFoundError: No module named 'pip'\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu..\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layers):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # combined = torch.cat((input, hidden), 1)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(NLAYERS, BATCHSIZE, self.hidden_size, device=device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using gpu..' if torch.cuda.is_available() else 'using cpu..')\n",
    "gru = GRU(ALPHASIZE, INTERNALSIZE, NLAYERS)\n",
    "print(len(list(gru.parameters())))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Softmax layer implementation:\n",
    "# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n",
    "# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n",
    "\n",
    "Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\n",
    "Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\n",
    "loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\n",
    "Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\n",
    "Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nParameters\\ninput_size – The number of expected features in the input x\\nhidden_size – The number of features in the hidden state h\\nnum_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, \\n    with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\\nbias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\\nbatch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\\ndropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\\nbidirectional – If True, becomes a bidirectional GRU. Default: False\\n\\nrnn = nn.GRU(10, 20, 2)\\ninput = torch.randn(5, 3, 10)\\nh0 = torch.randn(2, 3, 20)\\noutput, hn = rnn(input, h0)\\n\\ninput of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. \\n    The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details.\\n\\nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. \\n    Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1\\n    \\noutput of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. \\n    If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. \\n    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\\n    Similarly, the directions can be separated in the packed case.\\n\\nh_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\\n    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).\\n    \\nShape:\\nInput: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K)(N,C,d \\nTarget: (N)(N) where each value is 0 \\\\leq \\text{targets}[i] \\\\leq C-10≤targets[i]≤C−1 , or (N, d_1, d_2, ..., d_K)(N,d \\nOutput: scalar. If reduction is 'none', then the same size as the target: (N)(N) , or (N, d_1, d_2, ..., d_K)(N,d \\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss fn\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#from ok_seq2seq import EncoderRNN \\\n",
    "#                        ,DecoderRNN \\\n",
    "#                        ,AttnDecoderRNN \\\n",
    "#                        ,evaluateRandomly \\\n",
    "#                        ,teacher_forcing_ratio \n",
    "'''\n",
    "Parameters\n",
    "input_size – The number of expected features in the input x\n",
    "hidden_size – The number of features in the hidden state h\n",
    "num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, \n",
    "    with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\n",
    "bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "bidirectional – If True, becomes a bidirectional GRU. Default: False\n",
    "\n",
    "rnn = nn.GRU(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)\n",
    "\n",
    "input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. \n",
    "    The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details.\n",
    "\n",
    "h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. \n",
    "    Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1\n",
    "    \n",
    "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. \n",
    "    If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. \n",
    "    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\n",
    "    Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
    "    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).\n",
    "    \n",
    "Shape:\n",
    "Input: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K)(N,C,d \n",
    "Target: (N)(N) where each value is 0 \\leq \\text{targets}[i] \\leq C-10≤targets[i]≤C−1 , or (N, d_1, d_2, ..., d_K)(N,d \n",
    "Output: scalar. If reduction is 'none', then the same size as the target: (N)(N) , or (N, d_1, d_2, ..., d_K)(N,d \n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# stats for display\n",
    "seqloss = tf.reduce_mean(loss, 1)\n",
    "batchloss = tf.reduce_mean(seqloss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\n",
    "loss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\n",
    "acc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\n",
    "summaries = tf.summary.merge([loss_summary, acc_summary])\n",
    "\n",
    "# Init Tensorboard stuff. This will save Tensorboard information into a different\n",
    "# folder at each run named 'log/<timestamp>/'. Two sets of data are saved so that\n",
    "# you can compare training and validation curves visually in Tensorboard.\n",
    "timestamp = str(math.trunc(time.time()))\n",
    "summary_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-training\")\n",
    "validation_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-validation\")\n",
    "\n",
    "# Init for saving models. They will be saved into a directory named 'checkpoints'.\n",
    "# Only the last checkpoint is kept.\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")\n",
    "saver = tf.train.Saver(max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training fn\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = gru.initHidden()\n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    fc_layer = nn.Linear(INTERNALSIZE, ALPHASIZE)        \n",
    "\n",
    "    gru.zero_grad()\n",
    "    fc_layer.zero_grad()\n",
    "    \n",
    "    output, hidden = gru(line_tensor, hidden)\n",
    "    #print(f'gos={output.size()}, is={line_tensor.size()}')\n",
    "    output = fc_layer(output)\n",
    "    #print(f'fc={output.size()}, ls={line_tensor.size()}')\n",
    "    output = softmax(output)\n",
    "    #print(f'sm={output.size()}, cs[2:]={category_tensor.size()}')\n",
    "    input=output.transpose(0,1).transpose(1,2)\n",
    "    loss = criterion(input, category_tensor) # N (batch),C\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in gru.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    for p in fc_layer.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    \n",
    "    return output.transpose(0,1), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mb2t(rows):\n",
    "    rows=rows.transpose()\n",
    "    tensor = torch.zeros(rows.shape[0], rows.shape[1], ALPHASIZE, device=device)\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, letter_code in enumerate(row):\n",
    "            tensor[i][j][letter_code] = 1\n",
    "    return tensor\n",
    "\n",
    "def lin2txt(lt):\n",
    "    return ''.join([chr(txt.convert_to_alphabet(c))  if c != 0 else '' for c in lt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init train\n",
    "\n",
    "import time\n",
    "import math\n",
    "  # product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
    "\n",
    "\n",
    "print_every = 250\n",
    "plot_every = 500\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "iter=0\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 of 10 (0:00:01.732032) 5.2983  okunwengiapu mieye ma  diepak / $RKLllllqqqqMMM999``jrr_!!ee## ✗ calculating stats..\n",
      "epoch 1 of 10 (0:00:01.732032) 5.2983 ara mi nwose omine mgba oriari / $RKLllllqqqqMMM999``jrr_!!ee## ✗ calculating stats..\n",
      "epoch 1 of 10 (0:03:51.135076) 5.2983 isiapu ma bereniapu ma se kpot / *zzz3]]]]+9vVVV'||$$]]n+[[{{{/ ✗ calculating stats..\n",
      "epoch 1 of 10 (0:03:51.135076) 5.2983 ow naa kirikiri o biki koro se / *zzz3]]]]+9vVVV'||$$]]n+[[{{{/ ✗ calculating stats..\n",
      "epoch 1 of 10 (0:07:53.295222) 5.2983 mun dumo bie chu-chu bo be now / ff$((($fffffb44f$$$$$WWWffP}}m ✗ calculating stats..\n",
      "epoch 1 of 10 (0:07:53.295222) 5.2983 loko dieapu ma na nwo chukurum / ff$((($fffffb44f$$$$$WWWffP}}m ✗ calculating stats..\n",
      "epoch 2 of 10 (0:11:55.261455) 5.2984 werima i nyanaye mi ma i nwo d / ==0yyy<8oo..RRRhhh18nnnnoo ggg ✗ average batch rate per hr = 5.03,  eta = 1:47:17\n",
      "epoch 2 of 10 (0:11:55.261455) 5.2984 mi ani iya amanyanabo ngada-e  / ==0yyy<8oo..RRRhhh18nnnnoo ggg ✗ average batch rate per hr = 5.03,  eta = 1:47:17\n",
      "epoch 2 of 10 (0:15:41.733662) 5.2983 u ma na stivin be na berejinea / N^3AssAisvvvvvvOgg~55 \"HHHHHH ✗ average batch rate per hr = 3.82,  eta = 2:21:15\n",
      "epoch 2 of 10 (0:15:41.733662) 5.2983 sotoru bo enekubu bia o kelema / N^3AssAisvvvvvvOgg~55 \"HHHHHH ✗ average batch rate per hr = 3.82,  eta = 2:21:15\n",
      "epoch 3 of 10 (0:19:35.076564) 5.2981 lame. o tamuno be torukubu ibi / jjj\",,ZZZZ\"\"9//G?RNN}}}33wwww. ✗ average batch rate per hr = 6.13,  eta = 1:18:20\n",
      "epoch 3 of 10 (0:19:35.076564) 5.2981 ian mi fie ye be koroma nwo ko / jjj\",,ZZZZ\"\"9//G?RNN}}}33wwww. ✗ average batch rate per hr = 6.13,  eta = 1:18:20\n",
      "epoch 3 of 10 (0:23:10.606445) 5.2982 am askos bie sime juapu tekewa / }\\QQQ+#]]KKKKKKKKKPPPPPPc} ✗ average batch rate per hr = 5.18,  eta = 1:32:42\n",
      "epoch 3 of 10 (0:23:10.606445) 5.2982 me. antiok bie mi ini tatari k / }\\QQQ+#]]KKKKKKKKKPPPPPPc} ✗ average batch rate per hr = 5.18,  eta = 1:32:42\n",
      "epoch 4 of 10 (0:26:49.533854) 5.2983 n siki ma a pulasa ye, anikani / N$$oooTTTT\n",
      "\n",
      "OOAYtt?OO?!!Yg3333 ✗ average batch rate per hr = 6.71,  eta = 1:02:35\n",
      "epoch 4 of 10 (0:26:49.533854) 5.2983 nkoro samuel be buo lame. se i / N$$oooTTTT\n",
      "\n",
      "OOAYtt?OO?!!Yg3333 ✗ average batch rate per hr = 6.71,  eta = 1:02:35\n",
      "epoch 4 of 10 (0:30:29.790182) 5.2984 eme be na bereni na bie bein b / 822!!v\t\tQQQQQppwwwwwQQ}DDDDD\t\t ✗ average batch rate per hr = 5.90,  eta = 1:11:09\n",
      "epoch 4 of 10 (0:30:29.790182) 5.2984 bereniapu ma bu chua se inia k / 822!!v\t\tQQQQQppwwwwwQQ}DDDDD\t\t ✗ average batch rate per hr = 5.90,  eta = 1:11:09\n",
      "epoch 5 of 10 (0:34:10.239204) 5.2983 se kura mesi na gbasi ye bu or / V]V]TT\"\"\"~]]VVVk\\\\hhUqq%((IIIZ ✗ average batch rate per hr = 7.02,  eta = 0:51:15\n",
      "epoch 5 of 10 (0:34:10.239204) 5.2983 ia kienbipi enbene-bene senime / V]V]TT\"\"\"~]]VVVk\\\\hhUqq%((IIIZ ✗ average batch rate per hr = 7.02,  eta = 0:51:15\n",
      "epoch 5 of 10 (0:37:50.358832) 5.2982 a mi bu ori na banabas be na s / DD!!!}wwww\t\t\n",
      "jjgzzDzzzzzeeyyyY ✗ average batch rate per hr = 6.34,  eta = 0:56:45\n",
      "epoch 5 of 10 (0:37:50.358832) 5.2982 ume, pol be ini firimame na in / DD!!!}wwww\t\t\n",
      "jjgzzDzzzzzeeyyyY ✗ average batch rate per hr = 6.34,  eta = 0:56:45\n",
      "epoch 6 of 10 (0:41:32.551256) 5.2983 ie tolu boro muari siki ini ok / BYcg%%5(5//,,,,3,****ccccccc ✗ average batch rate per hr = 7.22,  eta = 0:41:32\n",
      "epoch 6 of 10 (0:41:32.551256) 5.2983 da obudukoapu o nwo orime, se  / BYcg%%5(5//,,,,3,****ccccccc ✗ average batch rate per hr = 7.22,  eta = 0:41:32\n",
      "epoch 6 of 10 (0:45:11.799523) 5.2982 i tomoni ma chukuruma se amafi / 77Il0LL00000\\\\[xxxDD>XXjEE<}}< ✗ average batch rate per hr = 6.64,  eta = 0:45:11\n",
      "epoch 6 of 10 (0:45:11.799523) 5.2982 a  wa toroko mun owoin-aru mi  / 77Il0LL00000\\\\[xxxDD>XXjEE<}}< ✗ average batch rate per hr = 6.64,  eta = 0:45:11\n",
      "epoch 7 of 10 (0:48:55.545988) 5.2984 tibi ori be kuroma okwein bu,  / <W###JJJJJKK$'''H|SSSc;::33wHH ✗ average batch rate per hr = 7.36,  eta = 0:32:37\n",
      "epoch 7 of 10 (0:48:55.545988) 5.2984 e kuro bara se pol be olome. i / <W###JJJJJKK$'''H|SSSc;::33wHH ✗ average batch rate per hr = 7.36,  eta = 0:32:37\n",
      "epoch 7 of 10 (0:52:35.724352) 5.2982 ine obu balabalama ma, o pa du / 4\\4ViiItZ++u  HHHHHHHHfff?:::u ✗ average batch rate per hr = 6.84,  eta = 0:35:03\n",
      "epoch 7 of 10 (0:52:35.724352) 5.2982 tibi mi piki gele teinme, se o / 4\\4ViiItZ++u  HHHHHHHHfff?:::u ✗ average batch rate per hr = 6.84,  eta = 0:35:03\n",
      "epoch 8 of 10 (0:56:16.845955) 5.2983  bumiefiafiama mi okibia ene m / \t:999j{{AAAAAAAh''888Kyeee\td.g ✗ average batch rate per hr = 7.46,  eta = 0:24:07\n",
      "epoch 8 of 10 (0:56:16.845955) 5.2983  pol be okweinma mun se bo ibi / \t:999j{{AAAAAAAh''888Kyeee\td.g ✗ average batch rate per hr = 7.46,  eta = 0:24:07\n",
      "epoch 8 of 10 (0:59:58.427315) 5.2983 sise se dumobia omin mi now ok / WUWWcrccr9999{[[>>[[2CCCKFddd5 ✗ average batch rate per hr = 7.00,  eta = 0:25:42\n",
      "epoch 8 of 10 (0:59:58.427315) 5.2983 re mi dukome na ini inia sima  / WUWWcrccr9999{[[>>[[2CCCKFddd5 ✗ average batch rate per hr = 7.00,  eta = 0:25:42\n",
      "epoch 9 of 10 (1:03:37.753189) 5.2983 i oria ikiapu ma piri ini o bu / iq7``lvviii***y*oy~~~.iiiM7HHZ ✗ average batch rate per hr = 7.54,  eta = 0:15:54\n",
      "epoch 9 of 10 (1:03:37.753189) 5.2983 a ton werime na ini ikoliapu m / iq7``lvviii***y*oy~~~.iiiM7HHZ ✗ average batch rate per hr = 7.54,  eta = 0:15:54\n",
      "epoch 9 of 10 (1:07:17.474634) 5.2982 moni ma bie inia kiri nwo okib / ltnFFnllG~~s~~~~&&&|vv|yyyyyy[ ✗ average batch rate per hr = 7.13,  eta = 0:16:49\n",
      "epoch 9 of 10 (1:07:17.474634) 5.2982 zos be nwose nin beme, sikima  / ltnFFnllG~~s~~~~&&&|vv|yyyyyy[ ✗ average batch rate per hr = 7.13,  eta = 0:16:49\n",
      "epoch 10 of 10 (1:10:59.161585) 5.2983  join ma olo diki ma nwo fimas / ATT***XT++KKK\t\t\t\t**+++CppppFFM ✗ average batch rate per hr = 7.61,  eta = 0:07:53\n",
      "epoch 10 of 10 (1:10:59.161585) 5.2983 bara fiafia teme be now okisam / ATT***XT++KKK\t\t\t\t**+++CppppFFM ✗ average batch rate per hr = 7.61,  eta = 0:07:53\n",
      "epoch 10 of 10 (1:14:39.433244) 5.2984  bu. jon be mengi se nweni sar / 880008WxxxxxGGGGQQ}N22NQQQQNNN ✗ average batch rate per hr = 7.23,  eta = 0:08:17\n",
      "epoch 10 of 10 (1:14:39.433244) 5.2984 a toroko gwosa bara bu chie si / 880008WxxxxxGGGGQQ}N22NQQQQNNN ✗ average batch rate per hr = 7.23,  eta = 0:08:17\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vloss=[]\n",
    "VALI_SEQLEN = 30\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=nb_epoch):\n",
    "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    category =  [lin2txt(l) for l in y_]\n",
    "    lines = [lin2txt(l) for l in x]\n",
    "    line_tensor=mb2t(x)\n",
    "    output, loss = train(torch.tensor(y_, device=device, dtype=torch.long), line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess = [lin2txt([ch.argmax(dim=0) for ch in line]) for line in output]\n",
    "        for i in range(2):\n",
    "            elapsed_time = time.time() - start\n",
    "            tss = str(datetime.timedelta(seconds=elapsed_time)) # time since start string\n",
    "            if epoch > 0:\n",
    "                speed = epoch/elapsed_time\n",
    "                eta = (nb_epoch-epoch)/speed\n",
    "                sspeed = speed*60*60\n",
    "                seta = str(datetime.timedelta(seconds=int(eta)))\n",
    "                stats = f'average batch rate per hr = %3.2f,  eta = {seta}'%(sspeed)\n",
    "            else:\n",
    "                stats ='initialising stats..'\n",
    "            correct = '✓' if guess[i] == category[i] else '✗ %s' % stats \n",
    "            print('epoch %d of %d (%s) %.4f %s / %s %s' % (epoch+1, nb_epoch, tss, loss, lines[i], guess[0], correct))\n",
    " \n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0 and len(valitext) > 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, BATCHSIZE, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        line_tensor = mb2t(vali_x)\n",
    "        output, loss = train(torch.tensor(vali_y, device=device, dtype=torch.long), line_tensor)\n",
    "        vloss.append(loss)\n",
    "        with open('vloss.json', 'w') as f:\n",
    "            loss_data={'vloss':vloss, 'tloss':all_losses}\n",
    "            json.dump(loss_data, f)\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(vloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# training loop\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10):\n",
    "\n",
    "    # train on one minibatch\n",
    "    # feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n",
    "    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n",
    "\n",
    "    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n",
    "    if step % _50_BATCHES == 0:\n",
    "        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n",
    "        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n",
    "        summary_writer.add_summary(smm, step)\n",
    "\n",
    "    # run a validation step every 50 batches\n",
    "    # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n",
    "    # so we cut it up and batch the pieces (slightly inaccurate)\n",
    "    # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n",
    "    if step % _50_BATCHES == 0 and len(valitext) > 0:\n",
    "        VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n",
    "        bsize = len(valitext) // VALI_SEQLEN\n",
    "        txt.print_validation_header(len(codetext), bookranges)\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n",
    "        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n",
    "                     batchsize: bsize}\n",
    "        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_validation_stats(ls, acc)\n",
    "        # save validation data for Tensorboard\n",
    "        validation_writer.add_summary(smm, step)\n",
    "\n",
    "    # display a short text generated with the current weights and biases (every 150 batches)\n",
    "    if step // 3 % _50_BATCHES == 0:\n",
    "        txt.print_text_generation_header()\n",
    "        ry = np.array([[txt.convert_from_alphabet(ord(\"K\"))]])\n",
    "        rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n",
    "        for k in range(1000):\n",
    "            ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n",
    "            rc = txt.sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n",
    "            print(chr(txt.convert_to_alphabet(rc)), end=\"\")\n",
    "            ry = np.array([[rc]])\n",
    "        txt.print_text_generation_footer()\n",
    "\n",
    "    # save a checkpoint (every 500 batches)\n",
    "    if step // 10 % _50_BATCHES == 0:\n",
    "        saved_file = saver.save(sess, 'checkpoints/rnn_train_' + timestamp, global_step=step)\n",
    "        print(\"Saved file: \" + saved_file)\n",
    "\n",
    "    # display progress bar\n",
    "    progress.step(reset=step % _50_BATCHES == 0)\n",
    "\n",
    "    # loop state around\n",
    "    istate = ostate\n",
    "    step += BATCHSIZE * SEQLEN"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
